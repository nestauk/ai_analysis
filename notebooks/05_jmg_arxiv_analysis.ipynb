{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv analysis\n",
    "\n",
    "We use the arXiv processed data and a topic model trained on a subset of the data to explore various questions about the trajectory of AI research:\n",
    "\n",
    "* Do we find any topical differences between the research undertaken in countries with different levels of political freedom / civil liberty?\n",
    "* Do we find any topical differences between the research undertaken in teams with / without women involved?\n",
    "* Do we find any topical differences between the research undertaken in public / private research organisations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Main data loads\n",
    "\n",
    "* Arxiv enriched dataset\n",
    "* Topic model\n",
    "* Freedom data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ArXiv\n",
    "\n",
    "We load the enriched arXiv dataset, which includes paper metadata, gender indicators and field predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx = pd.read_csv('../data/processed/1_8_2019_arxiv_enriched.csv',compression='zip',\n",
    "                 dtype={'id':str,'article_id':str,'paper_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add years to arXiv - TODO - do this in the load mag notebook\n",
    "arx['year'] = [x.split('-')[0] for x in arx['created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are unique papers so don't include the information about location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the arXiv-mag-grid matched dataset, which has information about the institutions and locations for papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load information about locatio\n",
    "grid_matched = pd.read_csv('../data/external/17_8_2019_papers_institution_ucl_cleaned.csv',compression='zip',dtype={'article_id':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic models\n",
    "\n",
    "We trained a topSBM topic model on 25K AI papers. We will use that for our semantic comparison between papers with and without female co-authors, between countries with different levels of political liberty, and between different types of institutions. This will require working with some auxiliaries datasets such as the press freedom indices, and GRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../models/2_8_2019_arxiv_sbm.p','rb') as infile:\n",
    "    topic_model = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the model\n",
    "model = topic_model[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freedom data\n",
    "\n",
    "This data has information about political and civil liberties in various countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freed = pd.read_csv('../data/processed/19_7_2019_freedom_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID roles\n",
    "\n",
    "This has GRID roles (what an organisation 'does'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_csv('../data/external/1_8_2019_grid_org_roles.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine thd Grid matches and metadata before we focus on the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_merged = pd.merge(grid_matched,grid[['grid_id','type']],left_on='institute_id',right_on='grid_id')\n",
    "\n",
    "grid_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We don't care about the location of multinationals since we can't match those.\n",
    "grid_merged['institute_name'] = [gr['institute_name'].split(' (')[0].strip() if gr['is_multinational']!=0 else gr['institute_name'] for p,gr in grid_merged.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step takes ages - could it be refactored?\n",
    "grid_merged['institute_country'] = ['multinational' if gr['is_multinational']!=0 else gr['institute_country'] for p,gr in grid_merged.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This creates a list of names, countries and types of institutions for each paper.\n",
    "#We can loop through those later to create dummies for comparisons and regressions\n",
    "\n",
    "grid_grouped = pd.concat(\n",
    "    [grid_merged.dropna(axis=0, subset=['institute_id']).groupby('article_id')[var].apply(lambda x: list(x)) for var in ['institute_name','institute_country',\n",
    "                                                                                                                         'type']],axis=1).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change some names\n",
    "grid_grouped.rename(columns={'institute_country':'country_list','institute_name':'institute_list','type':'type_list'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data processing\n",
    "\n",
    "We are going to enrich the topic modelled data with metadata about gender / country / institutional affiliations etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_document_topic_df(model,level,n_words):\n",
    "    '''\n",
    "    \n",
    "    We extract a document-topic df from the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: model object\n",
    "        level (int): level of the model at which we want to extract the topics\n",
    "        n_words: number of words we want to use to label the columns in the document-topic df\n",
    "        \n",
    "    Outputs:\n",
    "        A document topic df where every row is a paper (with its id) and each column is the the weight for a topic. The columns are labelled with the topic names\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the topic mix\n",
    "    d_t_df = pd.concat([pd.DataFrame(model.topicdist(n,l=level),columns=['topic',pid]).set_index('topic') for \n",
    "                      n,pid in enumerate(model.documents)],axis=1).T\n",
    "    \n",
    "    #Create the columns\n",
    "    topic_names = ['-'.join([x[0] for x in topic_comp][:n_words]) for topic_comp in model.topics(l=level).values()]\n",
    "    \n",
    "    d_t_df.columns = topic_names\n",
    "    \n",
    "    #We name the index to simplify merging later\n",
    "    d_t_df.index.name = 'paper_id'\n",
    "    \n",
    "    return(d_t_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the topic mix\n",
    "\n",
    "doc_topic_l0 = make_document_topic_df(model,0,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We store the topic names as we will use them later when working with the metadata\n",
    "topic_names = list(doc_topic_l0.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_document_topic_df(doc_top_df,metadata_df,variables,merge_id):\n",
    "    '''\n",
    "    \n",
    "    We append metadata to the document. This will be useful for crosstabbing and for regression analysis later\n",
    "    \n",
    "    Arguments:\n",
    "        doc_top_df: document topic df where we want to append metadata\n",
    "        metadata_df: df with the metadata we want to append.\n",
    "        variables: variablew we want to append\n",
    "        merge_var: id in the metadata df that we will use for merging. \n",
    "        \n",
    "    Outputs:\n",
    "        A document-topic df with additional columns capturing the metadata.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Subset the metadata df with the variables we are interested in\n",
    "    meta_df_selected = metadata_df[variables]\n",
    "    \n",
    "    #Reset index in the dtf for merging\n",
    "    doc_top_df_temp = doc_top_df.reset_index(drop=False)\n",
    "    \n",
    "    #Merge. Note that we also set the index again.\n",
    "    doc_top_enr = pd.merge(doc_top_df_temp,meta_df_selected,left_on='paper_id',right_on=merge_id,how='left').set_index('paper_id').drop('article_id',axis=1)\n",
    "    \n",
    "    return(doc_top_enr)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_variables = ['paper_id','article_id','title','abstract','year','top_field','has_female','citation_count']\n",
    "\n",
    "\n",
    "doc_topic_l0_exp = expand_document_topic_df(doc_topic_l0,arx,variables=meta_variables,merge_id='paper_id')\n",
    "\n",
    "# Note that there will be missing values for female authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will expand with the location and org type data. This requires some work with those datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_l0_exp_2 = expand_document_topic_df(doc_topic_l0_exp,grid_grouped,variables=['article_id','country_list','institute_list','type_list'],\n",
    "                                             merge_id='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create discipline fixed effects\n",
    "\n",
    "field_dummies = pd.get_dummies(doc_topic_l0_exp_2['top_field'])\n",
    "\n",
    "#Put the names here to use as controls later\n",
    "field_names =field_dummies.columns\n",
    "\n",
    "#Create the analysis DF\n",
    "analysis_df = pd.concat([doc_topic_l0_exp_2,field_dummies],axis=1)\n",
    "\n",
    "#Cast analysis df as integer\n",
    "analysis_df['year'] = analysis_df['year'].astype(int)\n",
    "\n",
    "#And log\n",
    "analysis_df['year_log'] = np.log(analysis_df['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrich the data with the political information\n",
    "\n",
    "Here we match the freedom data with the AI research data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from fuzzywuzzy import fuzz,process\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return([x for el in a_list for x in el])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the countries in the AI data\n",
    "countries = set(flatten_list(analysis_df['country_list'].dropna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we fuzzy match them with the Freedom data\n",
    "results = []\n",
    "\n",
    "for c in list(countries):\n",
    "    \n",
    "    #\n",
    "    \n",
    "    out = process.extract(c,list(set(freed['Country'])))\n",
    "\n",
    "    results.append([c,[x[0] for x in out if x[1]==100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a lookup between AI country names and freedom country names\n",
    "ai_country_freed_country_lu = {x[0]:x[1][0] for x in results if len(x[1])>0}\n",
    "\n",
    "\n",
    "#Create a lookup between country names and freedom\n",
    "\n",
    "country_status_lookup = {x['Country']:x['Status'] for c,x in freed.loc[freed['year']==2018].iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['freedom_list'] = [[country_status_lookup[ai_country_freed_country_lu[c]] for c in \n",
    "                                 c_list if c in ai_country_freed_country_lu.keys()] if type(c_list)==list else np.nan for\n",
    "                                 c_list in analysis_df['country_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community detection\n",
    "\n",
    "We are going to implement some community detection with two objectives:\n",
    "\n",
    "First, we want to explore rules to remove topics with high centrality since they are less informative about the purpose of a paper\n",
    "\n",
    "Second, we want to visualise the structure of the AI field and clusters its disciplines into communities. \n",
    "\n",
    "We will write a quick function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, product, chain\n",
    "import networkx as nx\n",
    "import community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_network_from_doc_term_matrix(mat,threshold,id_var):\n",
    "    '''\n",
    "    Create a network from a document term matrix.\n",
    "    \n",
    "    Args\n",
    "        Document term matrix where the rows are documents and the columns are topics\n",
    "        threshold is the threshold to consider that a topic is present in a matrix.\n",
    "        \n",
    "    Returns: \n",
    "        A network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Melt the topic mix and remove empty entries\n",
    "    cd = pd.melt(mat.reset_index(drop=False),id_vars=[id_var])\n",
    "\n",
    "    cd = cd.loc[cd['value']>threshold]\n",
    "\n",
    "    #This gives us the topic co-occurrence matrix\n",
    "    co_occurrence = cd.groupby(id_var)['variable'].apply(lambda x: list(x))\n",
    "    \n",
    "    #Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "    #Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "    sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in co_occurrence])\n",
    "    sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "    #Turn the sector combs into an edgelist\n",
    "    edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "    edge_list['weight']=1\n",
    "\n",
    "    #Group over edge pairs to aggregate weights\n",
    "    edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "    edge_list_weighted.sort_values('weight',ascending=False).head(n=10)\n",
    "    \n",
    "    #Create network and extract communities\n",
    "    net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "    \n",
    "    return(net)\n",
    "\n",
    "def extract_community(net,resolution,verbose=False):\n",
    "    '''\n",
    "    \n",
    "    Extracts communities from a network.\n",
    "    \n",
    "    Args:\n",
    "        net: a networkx object\n",
    "        resolution: level of granularity in the number of communities that are extracted\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    comms = community.best_partition(net,resolution=resolution,weight='weight')\n",
    "    \n",
    "    #return(comms)\n",
    "    \n",
    "    if verbose !=False:\n",
    "        \n",
    "        #What does this look like?\n",
    "        comm_strings = pd.DataFrame(comms,index=['comm']).T.groupby('comm')\n",
    "\n",
    "        #This is just to show the participation in communities\n",
    "        for n,x in enumerate(comm_strings.groups.keys()):\n",
    "            print(n)\n",
    "            print('====')\n",
    "            print('\\t'.join(list(comm_strings.groups[x])))\n",
    "            #print(', '.join(list(x.index())))\n",
    "            \n",
    "    return(comms)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_net = make_network_from_doc_term_matrix(doc_topic_l0,0.05,'paper_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the degree distribution of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is looking at the degree distribution\n",
    "degree_distr = pd.DataFrame(list(topic_net.degree)).sort_values(1,ascending=False).set_index(0)\n",
    "\n",
    "degree_distr.columns = ['degree']\n",
    "\n",
    "degree_distr['share'] = degree_distr['degree']/len(degree_distr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_distr[:50]['share'].plot.barh(figsize=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We drop topics that appear in more than 75% of the papers - they are not very informative.\n",
    "topic_drop = degree_distr.loc[degree_distr['share']>0.7].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set random seed\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_net_2 = make_network_from_doc_term_matrix(doc_topic_l0[[x for x in doc_topic_l0 if x not in topic_drop]],0.05,'paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'../models/13_8_2019_topic_communities.json') as infile:\n",
    "#     comms = json.load(infile)\n",
    "\n",
    "comms = extract_community(topic_net_2,resolution=0.3,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create topic lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = {\n",
    "  0:'algorithms',\n",
    "  1:'robotics_agents',\n",
    "  2:'social',\n",
    "  3:'statistics',\n",
    "  4:'security',\n",
    "  5:'clustering',\n",
    "  6:'technology',\n",
    "  7:'symbolic',\n",
    "  8:'optimisation',\n",
    "  9:'classification',\n",
    "  10:'graphs',\n",
    "  11:'question_answering_systems',\n",
    "  12:'health',\n",
    "  13:'computer_vision',\n",
    "  14:'mixed',\n",
    "  15:'communication',\n",
    "  16:'mathematics',\n",
    "  17:'generative_transfer',\n",
    "  18:'language',\n",
    "  19:'mixed',\n",
    "  20:'finance',\n",
    "  21:'mixed',\n",
    "  22:'mixed',\n",
    "  23:'neuroscience',\n",
    "  24:'mixed',\n",
    "  25:'mixed',\n",
    "  26:'physics',\n",
    "  27:'genomics',\n",
    "  28:'deep_learning',\n",
    "  29:'recommendations',\n",
    "  30:'deep_learning_sound',\n",
    "  31:'computer_vision',\n",
    "  32:'imaging_materials',\n",
    "  33:'generative_transfer',\n",
    "  34:'mixed',\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the communities\n",
    "\n",
    "#json.dumps(f'../models/{today_str}_topic_communities.json')\n",
    "\n",
    "with open(f'../models/{today_str}_topic_communities.json','w') as outfile:\n",
    "    json.dump(comms, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc_topic_l0_filtered = doc_topic_l0[[x for x in doc_topic_l0.columns if x not in topic_drop]]\n",
    "topics_filtered = [x for x in topic_names if x not in topic_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exog(df,value_container,value,make_dummy=True):\n",
    "    '''\n",
    "    This creates exogenous variables for modelling later.\n",
    "    \n",
    "    Argument:\n",
    "        -df contains the variable where we want to find a value\n",
    "        -variable_container is the column where we want to look for the value\n",
    "        -value is the value we are looking for\n",
    "        -make_dummy: if true it just counts if the value is present. If false, it counts how many times it happens. \n",
    "        \n",
    "    Output\n",
    "        -A df with the new column (named)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Create a tidy variable name\n",
    "    column_name = re.sub(' ','_',value.lower())\n",
    "    \n",
    "    #If we want to create a dummy...\n",
    "    if make_dummy == True:\n",
    "        \n",
    "        #We just look for it in the value container\n",
    "        #There are some missing values so we have some control flow to manage that. \n",
    "        df_2[column_name] = [value in x if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Otherwise, we count how many times it occurs\n",
    "        #We deal with missing values ('non lists') as before\n",
    "        df_2[column_name] = [x.count(value) if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "        \n",
    "    return(df_2)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_regression(df,target_list,exog,controls,model,binarise=False,standardise=True,cov='HC1'):\n",
    "    '''\n",
    "    \n",
    "    This function regresses topic weights (or their binarisation) on predictors.\n",
    "    \n",
    "    Arguments:\n",
    "        -Df with the variables\n",
    "        -target_list: target variables. This is a list we loop over. \n",
    "        -exog: exogenous variable\n",
    "        -controls\n",
    "        -model type. OLS? Logit? TODO fix the logit\n",
    "        -Binarise in case we are using logit. If not False, the value is the threshold \n",
    "            TODO when we binarise the highly detailed models, some of them become all zeros. This will work better\n",
    "            with the mopre aggregate topics\n",
    "        -Standardise if we standardise and log the topic weights\n",
    "    \n",
    "    Returns\n",
    "        -A list of statsmodels summaries\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Drop rows with missing values - sm doesn't like them\n",
    "    df_2 = df[target_list+exog+controls].dropna(axis=0)\n",
    "    \n",
    "    #Standardise targets?\n",
    "    if standardise==True:\n",
    "        df_2[target_list] = (np.log(df_2[target_list]+0.00000001)).apply(zscore).astype(float)\n",
    "    \n",
    "    #Binarise targets if we are doing a logit\n",
    "    if binarise!=False:\n",
    "        df_2[target_list] = df_2[target_list].applymap(lambda x: x>binarise).astype(float)\n",
    "    \n",
    "    \n",
    "    #Extract the exogenous and controls, add constant and cast as float\n",
    "    exog_controls = add_constant(df_2[exog+controls]).astype(float)\n",
    "    \n",
    "\n",
    "    #Container output\n",
    "    out = []\n",
    "    coeffs = []\n",
    "    \n",
    "    #One regression for each target\n",
    "    for t in list(target_list):\n",
    "        \n",
    "        #There we gp. \n",
    "        reg = model(endog=df_2[t],exog=exog_controls).fit(cov_type=cov,disp=0)\n",
    "        \n",
    "        out.append(reg.summary())\n",
    "        \n",
    "        #coeffs.append(reg)\n",
    "        if model == OLS:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.rsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','r_square']\n",
    "    \n",
    "        else:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.prsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','pr_square']\n",
    " \n",
    "    \n",
    "    return([out,reg_coeff.sort_values('coefficient',ascending=False)])\n",
    "        \n",
    "        \n",
    "def topic_comparison(df,target_list,exog,concept_lookup,quantiles=np.arange(0,1.1,0.2),thres=0):\n",
    "    '''\n",
    "    This function compares the distribution of activity in various topics depending on an exogenous variable of interest. \n",
    "    \n",
    "    Args:\n",
    "        Df with the topic mix and metadata\n",
    "        target_list are the topics to consider\n",
    "        exog is the variable to crosstab topics against\n",
    "        concept_lookup is a df with the median proximity of each topic to the concepts\n",
    "        quantiles is how we discretise the concept lookup (default value is quintiles)\n",
    "        thres: =limit for considering a topic as present\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Copy df\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Discretise the concept lookup\n",
    "    \n",
    "    conc_discr = concept_lookup.apply(lambda x: pd.qcut(x,q=quantiles,labels=False,duplicates='drop'))\n",
    "\n",
    "    \n",
    "    #Calculate levels of activity per topic based on the exog variable\n",
    "    \n",
    "    topic_distr = pd.concat([pd.crosstab(df_2[exog],df_2[t]>thres)[True] for t in target_list],axis=1).T\n",
    "    topic_distr.index = target_list\n",
    "    \n",
    "    \n",
    "    #Merge the count with the concept lookup\n",
    "    disc = pd.melt(pd.concat([topic_distr,conc_discr],axis=1).reset_index(drop=False),id_vars=['index']+list(conc_discr.columns))\n",
    "    \n",
    "    #This is the list where we store the results\n",
    "    store={}\n",
    "    \n",
    "    for c in concept_lookup.columns:\n",
    "        \n",
    "        out = pd.pivot_table(disc.groupby([c,'variable'])['value'].sum().reset_index(drop=False),index=c,columns='variable',values='value')\n",
    "        #out.apply(lambda x: x/x.sum()).plot.bar()\n",
    "        \n",
    "        store[c] = out\n",
    "                                      \n",
    "    #Output dfs with the comparisons\n",
    "    return(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import OLS, Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables of interest\n",
    "interesting_cuts = [['freedom_list','NF'],\n",
    "                    ['country_list','China'],['country_list','Russia'],['country_list','Turkey'],\n",
    "                    ['type_list','Company'],['type_list','Government'],['type_list','Education'],\n",
    "                    ['institute_list','Google'],['institute_list','Facebook'],['institute_list','IBM'],['institute_list','Microsoft']]\n",
    "\n",
    "#Create the expanded df\n",
    "analysis_df_expanded = analysis_df.copy()\n",
    "\n",
    "#For each interesting variable we expand the df\n",
    "for detect in interesting_cuts:\n",
    "    \n",
    "    analysis_df_expanded = make_exog(analysis_df_expanded,value_container=detect[0],value=detect[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf = topic_comparison(analysis_df_2,topics_filtered,'has_female',mean_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf['health'].apply(lambda x: x/x.sum(),axis=0).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This doesn't work very well**\n",
    "\n",
    "There are several reasons for this:\n",
    "\n",
    "* The documents I am using to measure ethics, surveillance etc are not very good\n",
    "* The topics are too aggregated to pick up similarity with a concept\n",
    "* Topics co-occur with each other. Their relation with the concepts aren't linear.\n",
    "* Let's park this for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trend analysis\n",
    "\n",
    "We will create a function for this that creates a df with activity per year and topic. \n",
    "\n",
    "Another function to plot results.\n",
    "\n",
    "My idea is to highlight trends of interest for different categories - papers with female authors, papers with companies, papers with non-free countries etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rc('xtick', labelsize=12) \n",
    "matplotlib.rc('ytick', labelsize=12)\n",
    "matplotlib.rc('axes',labelsize='large')\n",
    "matplotlib.rc('legend',fontsize='large')\n",
    "matplotlib.rc('font',size=12)\n",
    "matplotlib.rc('legend',**{'fontsize':12})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_topic_mix(topic_mix,communities,community_lookup,function='sum'):\n",
    "    '''\n",
    "    Converts the topic mix into communities\n",
    "    \n",
    "    Args:\n",
    "        Topic mix\n",
    "        communities is the community lookup\n",
    "        function to aggregate topics\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Melt, apply, pivot\n",
    "    topic_long = topic_mix.reset_index(drop=False)\n",
    "    \n",
    "    topic_long_2 = pd.melt(topic_long,id_vars=['paper_id'])\n",
    "    \n",
    "    #print(set(topic_long_2['variable']))\n",
    "    \n",
    "    topic_long_2['comm'] = [community_lookup[communities[top]] for top in topic_long_2['variable']]\n",
    "    \n",
    "    #print(topic_long_2.head())\n",
    "    \n",
    "    #Pivot\n",
    "    regrouped = pd.pivot_table(topic_long_2,index='paper_id',columns='comm',values='value',aggfunc=function)\n",
    "    \n",
    "    return(regrouped[[x for x in regrouped.columns if x!='mixed']])\n",
    "    #return(topic_long_2)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_analysis(topic_mix,topics,year_var='year',year_lim = [2000,2019],thres=0.1):\n",
    "    '''\n",
    "    Takes a df and analyses topics trends\n",
    "    \n",
    "    Args:\n",
    "        -The topic mix where the rows are papers and the columns are topics\n",
    "        -The topics to visualise\n",
    "        -The year variable to consider\n",
    "        -Threshold for topic occurrence.\n",
    "        -comms = community lookup (or false, if we are not using communities)\n",
    "    \n",
    "    Returns:\n",
    "        -A table with levels of activity per topic and year\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #Topic count per year\n",
    "    \n",
    "    topic_count = pd.concat([pd.crosstab(topic_mix[year_var],topic_mix[t]>thres)[True] for t in topics],axis=1).fillna(0)\n",
    "    topic_count.columns = topics\n",
    "    \n",
    "\n",
    "        #Count papers per topic\n",
    "        #topic_count = pd.concat([pd.crosstab(topic_mix[year_var],topic_mix[t]>0)[True] for t in topics],axis=1).fillna(0)\n",
    "        \n",
    "        #Add columns\n",
    "        \n",
    "        \n",
    "    #Normalise years\n",
    "    topic_count = topic_count.loc[np.arange(year_lim[0],year_lim[1])].fillna(0)\n",
    "        \n",
    "    return(topic_count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_trend_of_interest(trend_df,topics,ax,wind=3,norm=False,**kwargs):\n",
    "    '''\n",
    "    Plots a trend of interest.\n",
    "    \n",
    "    Args: \n",
    "        trend_df: the df where rows = years and column = topics\n",
    "        topic: topic or topics of interest\n",
    "        wind: rolling mean normalisation\n",
    "        norm: if 2 = normalise for year (importance of a topic in the period) if 1 = normalise for topic (share of year activity in the topic). If False = don't normalise\n",
    "        \n",
    "    Returns the plot\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Normalise or not?\n",
    "    \n",
    "    if norm==False:\n",
    "        trend_df[topics].rolling(window=wind).mean().dropna().plot(ax=ax,**kwargs)\n",
    "        \n",
    "    else:\n",
    "        trend_norm = trend_df.apply(lambda x: x/x.sum(),norm-1).fillna(0)\n",
    "        \n",
    "        #print(trend_norm)\n",
    "    \n",
    "        trend_norm[topics].rolling(window=wind).mean().dropna().plot(ax=ax,**kwargs)\n",
    "    \n",
    "\n",
    "def trend_comparison(topic_mix,topics,var,ax,year_var='year',year_lim = [2000,2019],thres=0,norm=2):\n",
    "    '''\n",
    "    Compares two groups in a trend of interest\n",
    "    \n",
    "    Args:\n",
    "        -topic_mix = topic mix\n",
    "        -topics: topics of interest\n",
    "        -var: variable we want to compare\n",
    "        -ax will generaly be a matplotlib axis with two rows \n",
    "        -The year variable to consider\n",
    "        -Threshold for topic occurrence.\n",
    "        -comms = community lookup (or false, if we are not using communities)\n",
    "    \n",
    "    Returns the plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    outputs = [trend_analysis(topic_mix.loc[topic_mix[var]==val],topics) for val in [False,True]]\n",
    "    \n",
    "    for n,out in enumerate(topics):\n",
    "        \n",
    "        #print(out)\n",
    "        plot_trend_of_interest(out,topics,norm=norm,ax=ax[n])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(name,path='../reports/figures/slide_deck/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(arx['year'],arx['is_ai'],normalize=1)[1].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activity by field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_to_plot = [x for x in field_names if not any(num in x for num in ['1','2'])]\n",
    "\n",
    "arx['year'] = [int(x) for x in arx['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_in_fields = pd.concat([pd.crosstab(arx.loc[arx[t]>0.5]['year'],\n",
    "                                     arx.loc[arx[t]>0.5]['is_ai'],normalize=0)[1] for t in fields_to_plot],axis=1).fillna(0)\n",
    "\n",
    "ai_in_fields.columns = fields_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ai_fields = ai_in_fields.loc[2018].sort_values().index[::-1][:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (100*ai_in_fields.loc[np.arange(2000,2019),top_ai_fields].rolling(window=3).mean()).dropna().plot(figsize=(10,6),cmap='tab10',linewidth=3)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1),title='Scientific field')\n",
    "\n",
    "ax.set_title('Share of AI activity by scientific field')\n",
    "\n",
    "save_fig('field_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the topic variable\n",
    "topic_comms = convert_topic_mix(analysis_df[topics_filtered],comms,comm_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a df with all the information\n",
    "analysis_fin = pd.concat([topic_comms,analysis_df_expanded],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "community_names = topic_comms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_for_plot = ['computer_vision','machine_learning','symbolic','health','robotics','language',\n",
    "                   #'adversarial',\n",
    "                   'statistics','deep_learning',\n",
    "                  'robotics_agents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_highlight_plot(trends,vars_interest,ax,cmap,alpha=0.3):\n",
    "    '''\n",
    "    Creates a df where we select the topics to focus on\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        Trend is a trend df\n",
    "        vars_interest are the topics or variables we eanrt to focus on\n",
    "        ax the axis\n",
    "        cmap is the color map we want to use\n",
    "    \n",
    "    Returns a plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create a lookup with numbers for values\n",
    "    topic_lookup = {name:val for val,name in enumerate(vars_interest)}\n",
    "\n",
    "    #Color map\n",
    "    cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "    #Create a vector of colors\n",
    "    cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in trends.columns]\n",
    "    lw = [1 if v not in topic_lookup.keys() else 3 for v in trends.columns]\n",
    "    \n",
    "    #Plot\n",
    "    (100*trends.rolling(window=4).mean()).dropna().plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "    #Fix the legend to focus on key topics\n",
    "    hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in vars_interest],\n",
    "              labels=[x[1][:50] for x in zip(hand,labs) if x[1] in vars_interest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to normalise the years\n",
    "comm_trends = trend_analysis(analysis_fin,community_names,thres=0.05)\n",
    "all_years = analysis_fin['year'].value_counts()\n",
    "comm_norm = comm_trends.apply(lambda x: x/all_years).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "make_highlight_plot(comm_norm,topics_for_plot,cmap='tab10_r',ax=ax,alpha=0.15)\n",
    "\n",
    "#ax.legend(bbox_to_anchor=(1,1),title='Research area')\n",
    "\n",
    "ax.set_title('Share of AI activity by research area')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('community_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_topics = [\n",
    "    #'face-faces-identity-face_recognition-facial','person-surveillance-persons-pedestrian-pedestrians',\n",
    "    #'attacks-attack-adversary-vulnerable-threat',\n",
    "    #'emotions-emotion-neutral-emotional-spontaneous',\n",
    "    'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "    'cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "    'training-trained-deep_learning-deep-train',\n",
    "    'generator-gan-discriminator-generative_adversarial_networks_gans-gans',\n",
    "    'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "    'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_trends = trend_analysis(analysis_fin,topics_filtered,thres=0.05)\n",
    "all_years = analysis_fin['year'].value_counts()\n",
    "topic_trends_norm = topic_trends.apply(lambda x: x/all_years).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "make_highlight_plot(topic_trends_norm.loc[np.arange(2005,2019)],notable_topics,cmap='Dark2',ax=ax,alpha=0.1)\n",
    "\n",
    "ax.set_title('Share of AI activity by detailed topic')\n",
    "\n",
    "ax.set_ylabel('Share of AI papers with topic')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('trending_topics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional analyses that identifies growing areas in recent years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trend_plot(df,topics_to_consider,top_n,ax,top_year=2018,thres=0.05,period=[2005,2019],alpha=0.3):\n",
    "    '''\n",
    "    Generates a similar plot to those above but with automatic identification of the top trends\n",
    "    \n",
    "    Args:\n",
    "        Df is the topic mix (we will often have subsetted this to focus on a particular type of organisation)\n",
    "        top_n is the top number of entities to label and display\n",
    "        threshold for considering that a topic is present in a paper\n",
    "        period is a list with the period we are considering\n",
    "        \n",
    "    Returns a similar plot to above but visualising the top n trends \n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Check for topics with no activity:\n",
    "    total_presence = (df[topics_to_consider]>thres).sum()\n",
    "    no_values = total_presence.index[total_presence==0]\n",
    "    \n",
    "    topics_to_consider = [x for x in topics_to_consider if x not in no_values]\n",
    "    \n",
    "    \n",
    "    #Calculate topic trends\n",
    "    topic_trends = trend_analysis(df,topics_to_consider,thres=thres,year_lim=period)\n",
    "    \n",
    "    #Calculate all papers, for normalisation\n",
    "    all_years = df['year'].value_counts()\n",
    "    \n",
    "    #Normalise\n",
    "    topic_trends_norm = topic_trends.apply(lambda x: x/all_years).dropna()\n",
    "    \n",
    "    top_topics = topic_trends_norm.T.sort_values(top_year,ascending=False).index[:top_n]\n",
    "    \n",
    "    \n",
    "    make_highlight_plot(topic_trends_norm,top_topics,cmap='Dark2',ax=ax,alpha=alpha)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_plot(df,var_subset,topics_to_consider=topics_filtered,n_tops=8):\n",
    "    '''\n",
    "    Creates trend plots based on different categories.\n",
    "    \n",
    "    Args:\n",
    "        df with papers and topics\n",
    "        var_subset is the variable we want to consider (will generally be a boolean)\n",
    "        n_tops: number of top institutions to visualise\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "    my_df = df.loc[df[var_subset]==True]\n",
    "    \n",
    "    make_trend_plot(my_df,topics_filtered,n_tops,ax=ax,top_year=2018,alpha=0.2)\n",
    "    \n",
    "    ax.set_title(var_subset)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# make_trend_plot(analysis_fin,topics_filtered,8,ax=ax,top_year=2018,alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'nf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'has_female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'education')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'government')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick_plot(analysis_fin,'microsoft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chinese_govt = analysis_fin.loc[(analysis_fin['china']==True)&(analysis_fin['government']==True)]\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(10,8))\n",
    "    \n",
    "# make_trend_plot(chinese_govt,topics_filtered,top_n=8,ax=ax,top_year=2018,alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network analysis\n",
    "\n",
    "Strategy:\n",
    "\n",
    "* We need to visualise the network - which is quite dense. How do we do this?\n",
    "  * \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make the size of the nodes comparable between years\n",
    "size_lookup = pd.concat([(analysis_fin.loc[[x in year_set for x in analysis_fin['year']]][topics_filtered]>0.05).sum() for \n",
    "                         year_set in [\n",
    "                             set(np.arange(1990,2019)),\n",
    "                             set(np.arange(1990,2012)),\n",
    "                             set(np.arange(2012,2015)),\n",
    "                             set(np.arange(2015,2019))]],axis=1)\n",
    "\n",
    "size_lookup.columns = ['all','pre','mid','late']\n",
    "\n",
    "size_lookup_dict = size_lookup.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_lookup = {\n",
    "#     2:'magenta',\n",
    "#     1: 'cornflowerblue',\n",
    "#     4:'cornflowerblue',\n",
    "#     28:'cornflowerblue',\n",
    "#     7:'red',\n",
    "#     26:'yellow',\n",
    "#     27:'orange',\n",
    "#     14:'aqua',\n",
    "#     28:'aqua',\n",
    "#     #17:'plum',\n",
    "#     13:'lime'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "patches = [mpatches.Patch(facecolor=c, label=l,edgecolor='black') for l,c in color_lookup.items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_network(net,top_edge_share,label,loc,color_lookup=color_lookup,norm=1000,norm_2=1.2,layout=nx.kamada_kawai_layout,size_lookup=size_lookup):\n",
    "    '''\n",
    "    Plots a network visualisation of the topic network.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    new_net = net.copy()\n",
    "    \n",
    "    #We drop the \n",
    "    #drop_bad_edges = [e for e in new_net.edges(data=True) if not any(x in topic_drop for x in e[:2])]\n",
    "\n",
    "    #new_net_2 = nx.Graph(drop_bad_edges)\n",
    "\n",
    "    net_weight = sorted(new_net.edges(data=True),key=lambda x: x[2]['weight'],reverse=True)\n",
    "\n",
    "    length = int(top_edge_share*len(net_weight))\n",
    "    #\n",
    "    print(length)\n",
    "    \n",
    "    top_edges = net_weight[:length]\n",
    "\n",
    "    new_net_2 = nx.Graph(top_edges)\n",
    "    \n",
    "    pos = layout(new_net_2,\n",
    "                 #weight='weight',\n",
    "                 center=(0.5,0.5)\n",
    "                )\n",
    "    \n",
    "    #Get positions\n",
    "    x,y = [[v[val] for v in pos.values()] for val in [0,1]]    \n",
    "    \n",
    "    nx.draw_networkx_nodes(new_net_2,pos,\n",
    "                       node_size=list([size_lookup[x]**norm_2 for x in dict(new_net_2.degree).keys()]),\n",
    "                       node_color = [color_lookup[comm_names[comms[x]]] if comm_names[comms[x]] in color_lookup.keys() else 'white' for x in dict(new_net_2.nodes).keys()],\n",
    "                       cmap='tab20c',\n",
    "                       alpha=0.9,edgecolors='darkgrey')\n",
    "    \n",
    "    ax.annotate(label,xy=(np.min(x)+0.02,np.max(y)-0.02),size=24,color='white',fontweight='bold')\n",
    "\n",
    "    nx.draw_networkx_edges(new_net_2,pos,width=[e[2]['weight']/norm for e in new_net_2.edges(data=True)],edge_color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_facecolor('black')\n",
    "\n",
    "show_network(topic_net_2,0.05,norm=100,norm_2=0.9,layout=nx.kamada_kawai_layout,size_lookup=size_lookup['all'],label='All years',loc=(-0.5,1.48))\n",
    "\n",
    "ax.legend(handles=patches,facecolor='white',loc='upper right',title='Area')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('network_all_years.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_period = analysis_fin.loc[analysis_fin['year']<2011][topics_filtered]\n",
    "\n",
    "\n",
    "top_net_old= make_network_from_doc_term_matrix(old_period,0.025,'paper_id')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "show_network(top_net_old,0.02,norm=900,norm_2=0.9,layout=nx.kamada_kawai_layout,size_lookup=size_lookup['pre'],label='Before 2012',loc=(-0.29,1.1))\n",
    "\n",
    "ax.legend(handles=patches,facecolor='white',loc='lower left',title='Area')\n",
    "ax.set_facecolor('black')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('network_early.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_period = analysis_fin.loc[(analysis_fin['year']>=2011)&(analysis_fin['year']<2016)][topics_filtered]\n",
    "\n",
    "\n",
    "top_net_mid= make_network_from_doc_term_matrix(mid_period,0.025,'paper_id')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "show_network(top_net_mid,0.02,norm=700,norm_2=0.9,layout=nx.kamada_kawai_layout,size_lookup=size_lookup['mid'],label='Between 2012 and 2015',loc=(-0.36,1.3))\n",
    "\n",
    "ax.legend(handles=patches,facecolor='white',loc='lower left',title='Area')\n",
    "ax.set_facecolor('black')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('network_mid.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_period = analysis_fin.loc[(analysis_fin['year']>2016)][topics_filtered]\n",
    "\n",
    "\n",
    "top_net_late= make_network_from_doc_term_matrix(late_period,0.025,'paper_id')\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "show_network(top_net_late,0.02,norm=700,norm_2=0.9,layout=nx.fruchterman_reingold_layout,size_lookup=size_lookup['late'],label='After 2015',loc=(-0.52,1.4))\n",
    "\n",
    "ax.legend(handles=patches,facecolor='white',loc='lower left',title='Area')\n",
    "ax.set_facecolor('black')\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('network_late.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_lookup_2 = {\n",
    "    'deep_learning':'blue',\n",
    "    #'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    #'health':'lime',\n",
    "    #'social_biological':'forestgreen',\n",
    "    #'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    #'language':'yellow'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_centrality(network,measure,cl,ax,plot_name):\n",
    "    '''\n",
    "    This is to plot the centrality of different topics inside the topic network.\n",
    "    \n",
    "    Args:\n",
    "        -network is the network whose centralities we want to plot\n",
    "        -measure is the measure we want to plot\n",
    "        -colour lookup is to colour the bars in the network\n",
    "        -ax is the axis\n",
    "    \n",
    "    Returns a plot of the distributions of centrality\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Calculate the centrality measure and normalise it\n",
    "    c = pd.Series(measure(network,weight='weight'))\n",
    "    \n",
    "    c_norm =  pd.Series(zscore(c),index=c.index)\n",
    "    \n",
    "    #Plot\n",
    "    c_sorted = c_norm.sort_values(ascending=False)\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in c_sorted.index]\n",
    "    \n",
    "    c_sorted.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=3)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('Normalised centrality')\n",
    "    ax.set_title(plot_name)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [mpatches.Patch(facecolor=c, label=l,edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,figsize=(20,8))\n",
    "\n",
    "plot_centrality(top_net_old,nx.eigenvector_centrality,cl=color_lookup,ax=ax[0],plot_name='Before 2012')\n",
    "#plot_centrality(top_net_mid,nx.eigenvector_centrality,cl=color_lookup,ax=ax[1],plot_name='Between 2011 and 2015')\n",
    "plot_centrality(top_net_late,nx.eigenvector_centrality,cl=color_lookup,ax=ax[1],plot_name='After 2015')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('network_centrality_change.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider disruption\n",
    "\n",
    "Our final descriptive analysis considers disruption over time: what have been the changes in the composition of AI since the 2000s?\n",
    "\n",
    "We create a matrix that compares the topic vector for every year (a normalised sum) across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to measure distances between activity profiles in years\n",
    "period=np.arange(2000,2019)\n",
    "\n",
    "#We create a vector with counts of papers with activity in a year\n",
    "year_topics = pd.concat([(analysis_fin.loc[analysis_fin['year']==y,topics_filtered]>0.05).sum() for y in period],axis=1)\n",
    "\n",
    "year_topics.columns = period\n",
    "\n",
    "#We normalise the results (we want to consider the relative importance of topics, not absolute)\n",
    "topics_years_norm = year_topics.T.apply(lambda x: zscore(x)).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate distances between years\n",
    "\n",
    "year_sims = pd.DataFrame(1-pairwise_distances(topics_years_norm,metric='cosine'),index=period,columns=period)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We also calculate rolling intra-year distances. We focus on the diagonal for visualisation\n",
    "mean_sims = pd.Series(np.diag(np.matrix(year_sims.rolling(window=3).mean())))\n",
    "mean_sims.index = period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the results, which show quite starkly the disruption in AI research before and after 2012.\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,8),nrows=2,gridspec_kw={'height_ratios':[3,1.2]})\n",
    "\n",
    "ax[0].imshow(year_sims,cmap='seismic',aspect='auto')\n",
    "\n",
    "#Some formatting of labels etc\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticks(np.arange(0,len(period)))\n",
    "ax[0].set_yticklabels(period)\n",
    "ax[0].set_title('Year on year topic similarity',size=14)\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('Year-on-year \\n similarity \\n (rolling mean)')\n",
    "\n",
    "mean_sims.plot(ax=ax[1])\n",
    "\n",
    "plt.subplots_adjust(hspace=0.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "save_fig('disruption_measure.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we calculate the half life of similarity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_five_year_disruption(table,year,span):\n",
    "#     '''\n",
    "#     This calculates the rate at which a year becomes more dissimilar from other years\n",
    "    \n",
    "#     Args:\n",
    "#         Table with similarities\n",
    "#         Year is the year of interest\n",
    "#         span is how many years to consider in the analysis\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     #This extracts the five years before the year and extracts their similarities\n",
    "#     out = pd.Series(make_growth_rate(table.loc[year,(year-span):year+1][::-1])).mean()\n",
    "\n",
    "#     return(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = []\n",
    "# for y in np.arange(2005,2019):\n",
    "    \n",
    "#     dist = make_five_year_disruption(year_sims,y,4)\n",
    "    \n",
    "#     out.append(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series(out,index=np.arange(2005,2019)).rolling(window=3).mean().dropna().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Women in AI\n",
    "\n",
    "Our prior is that papers with women tend to be more focused on fields such as health and social. We explore this here.\n",
    "\n",
    "We will constrain our analysis to two issues.\n",
    "\n",
    "a. Distribution of topics over 'communities'\n",
    "b. Analysis of diversity in topics: are female papers more interdisciplinary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_comp(df,variable,topics,threshold):\n",
    "    '''\n",
    "    This function compares activity by topics between categories.\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe we are using (generally analysis_fin, with rows = papers and columns = variables and metadata)\n",
    "        variable is the variable we are using for the comparison\n",
    "        topics is the topics where we want to compare (generally the community names)\n",
    "        threshold is the threshold we want to use to determine if a paper is in a topic or not\n",
    "    \n",
    "    Returns a df with the shares of papers in each topic sorted by their distances\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the counts df.\n",
    "    \n",
    "    #We are extracting, for each topics, the % of papers with at least one female author when the topic is present, and when it isn't.\n",
    "    group_counts = pd.concat([pd.crosstab(df[variable],df[t]>threshold,normalize=1).loc[True,:] for t in topics],axis=1)\n",
    "    \n",
    "    #Name\n",
    "    group_counts.columns = topics\n",
    "    \n",
    "    #Transpose\n",
    "    group_counts = group_counts.T\n",
    "    \n",
    "    #Rename variables\n",
    "    group_counts.columns = [variable+f'_{value}' for value in ['false','true']]\n",
    "    \n",
    "    #Create a measure of difference\n",
    "    group_counts['difference'] = (group_counts.iloc[:,1]/group_counts.iloc[:,0])-1\n",
    "    \n",
    "    #Output\n",
    "    out = group_counts.sort_values('difference',ascending=False)\n",
    "    \n",
    "    return(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression_coefficients(var,cov='HC1',size=(8,6)):\n",
    "    '''\n",
    "    Plots regression coefficients.\n",
    "    \n",
    "    Arg:\n",
    "        variable we use as predictor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reg = topic_regression(analysis_fin,topics_filtered,[var],controls,OLS,cov='HC1')\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plot_topic_bar(reg[1]['coefficient'],cl=color_lookup,ax=ax)\n",
    "\n",
    "    ax.set_title(f'Regression coefficient using {var} as predictor')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_average = analysis_fin['has_female'].value_counts(normalize=True)[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_community_comp = cross_sectional_comp(analysis_fin,'has_female',community_names,threshold=0.1)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,6),ncols=2,sharey=True)\n",
    "\n",
    "(100*woman_community_comp.iloc[:,1][::-1]).plot.barh(ax=ax[0])\n",
    "\n",
    "(100*woman_community_comp['difference'][::-1]).plot.barh(ax=ax[1])\n",
    "\n",
    "ax[0].vlines(x=100*woman_average,ymin=-0.5,ymax=len(woman_community_comp),linestyle=':',color='red')\n",
    "ax[0].set_xlabel('Papers with at least one female author as \\n share of the total')\n",
    "\n",
    "\n",
    "ax[1].vlines(x=0,ymin=-0.5,ymax=len(woman_community_comp),linestyle=':',color='red')\n",
    "ax[1].set_xlabel('Representation of papers \\n with at least one female author')\n",
    "\n",
    "\n",
    "ax[0].set_ylabel('')\n",
    "\n",
    "fig.suptitle('              Representation of topics for papers with one female author',y=1.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/slide_deck/{today_str}_women_representation.pdf',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison by topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_topic_comp = cross_sectional_comp(analysis_fin,'has_female',topics_filtered,threshold=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_topic_bar(table,cl,ax):\n",
    "    '''\n",
    "    Simple function to plot topic bars which includes colours based on the topic-label lookup\n",
    "    \n",
    "    Args:\n",
    "        table has topics in the index and a value to plot in the columns\n",
    "        cl is the colour lookup between communities and topics\n",
    "        ax is the plotting axe\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in table.index]\n",
    "    \n",
    "    table.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plot_topic_bar(woman_topic_comp['difference'],cl=color_lookup,ax=ax)\n",
    "\n",
    "ax.set_title('Representation of papers with female topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Report regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = ['year']+list(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('has_female',size=(8,6))\n",
    "\n",
    "save_fig('woman_regression.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare paper multidisciplinarity between female and male\n",
    "\n",
    "We conclude our analysis of differences between papers with different genders with a look at the interdisciplinarity of different paper types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_field_comp = arx.loc[arx['is_ai']==True,:].dropna(axis=0,subset=['has_female'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(df,categories,category):\n",
    "    '''\n",
    "    We calculate entropy inside a paper using a distribution over semantic variables (eg discipline, community or topic). These have to be normalised\n",
    "    \n",
    "    arguments:\n",
    "        df is the analysis df with relevant topics and metadata\n",
    "        categories are the topics we want to compare\n",
    "        \n",
    "    outputs\n",
    "        A df with entropy measures by paper\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #Normalise\n",
    "    norm = df[categories].apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    ent = pd.DataFrame((norm.apply(lambda x: entropy(x),axis=1)),columns=['entropy'])\n",
    "    \n",
    "    ent['cat']=category\n",
    "    \n",
    "    return(ent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compare the entropies between disciplines\n",
    "# gender_field_entropy = pd.concat([calculate_entropy(\n",
    "#     arx_field_comp.loc[(arx_field_comp['has_female']==value)],field_names,category) for \n",
    "#                            value,category in zip([False,True],['no_female','has_female'])],axis=0)\n",
    "\n",
    "# gender_field_entropy.groupby('cat')['entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare the entropies between topics\n",
    "gender_ent = pd.concat([calculate_entropy(\n",
    "    analysis_fin.loc[(analysis_fin['has_female']==value)],topics_filtered,category) for \n",
    "                           value,category in zip([False,True],['no_female','has_female'])],axis=0)\n",
    "\n",
    "gender_ent.groupby('cat')['entropy'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(2,5))\n",
    "\n",
    "#ax.violinplot([list(gender_ent.loc[gender_ent['cat']==val,'entropy']) for val in ['has_female','no_female']])\n",
    "gender_ent.boxplot(column='entropy',by='cat',ax=ax)\n",
    "\n",
    "#ax.set_title('')\n",
    "#ax.set_title('Entropy by \\n female participation in AI paper')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Very preliminary gender analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the regression df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_reg = analysis_fin.copy()\n",
    "\n",
    "#Add entropy\n",
    "div_reg['entropy'] = calculate_entropy(div_reg,topics_filtered,category='drop')['entropy']\n",
    "\n",
    "#Drop missing values\n",
    "div_reg.dropna(inplace=True)\n",
    "\n",
    "#Creare endogenous variable\n",
    "endog = div_reg['entropy'].astype(float)\n",
    "\n",
    "#Create predictors\n",
    "exog = add_constant(div_reg[['has_female','year'] + list(field_names)].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = OLS(endog=endog,exog=exog).fit(cov_type='HC2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some preliminary evidence suggesting that papers involving women tend, on average, to have more diverse combinations of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Company analysis\n",
    "\n",
    "What are we going to do?\n",
    "\n",
    "* Measure the distribution over terms as before\n",
    "* Study trends (share of DL / Reinforcement learning / Computer vision accounted by companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many companies?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(analysis_fin['company'])/len(analysis_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*pd.Series(flatten_list([list(set([inst for inst in x if type(inst)==str])) for x in analysis_fin['institute_list'].dropna()])).value_counts(normalize=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = pd.concat([pd.crosstab(analysis_fin['year'],analysis_fin[var],normalize=0)[True] for var in ['company','google','facebook','microsoft','ibm']],axis=1)\n",
    "comps.columns = ['company','google','facebook','microsoft','ibm']\n",
    "comps['other companies'] = comps['company']-comps.iloc[:,1:].sum(axis=1)\n",
    "\n",
    "comps_data = 100*comps.loc[np.arange(2000,2019)].iloc[:,1:].rolling(window=3).mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "pal = sns.color_palette('Accent')\n",
    "\n",
    "ax.stackplot(comps_data.index,comps_data.T,cmap='Dark2',labels=[x.capitalize() for x in comps_data.columns],colors=pal,edgecolor='grey')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.35,1))\n",
    "\n",
    "ax.set_ylabel('% of all AI papers')\n",
    "ax.set_title('Corporate participation in AI research')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('stacked_chart.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_topic_comp = cross_sectional_comp(analysis_fin,'company',topics_filtered,threshold=0.05)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plot_topic_bar(company_topic_comp['difference'],cl=color_lookup,ax=ax)\n",
    "\n",
    "ax.set_title('Representation of papers involving companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_topic_comp = cross_sectional_comp(analysis_fin,'google',topics_filtered,threshold=0.05)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "plot_topic_bar(google_topic_comp['difference'],cl=color_lookup,ax=ax)\n",
    "\n",
    "ax.set_title('Representation of papers involving Google')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('company',size=(8,6))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('company_regression.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('education',size=(8,6))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('education_regression.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('google',size=(8,6))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('google_regression.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series analysis\n",
    "\n",
    "I want to study the level of activity in a topic accounted by different types of organisations. \n",
    "\n",
    "The target chart contains share of all papers in a topic accounted by different types of organisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['no_education'] = analysis_fin['education']==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_trend(df,cat,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    Extracts evolution of a share of a category in a topic of interest\n",
    "    \n",
    "    Args:\n",
    "        df: the usual dataframe\n",
    "        cat: the category we are interested in\n",
    "        year_lims: first and last year to consider\n",
    "\n",
    "    '''\n",
    "    #rel_df = df.loc[df[cat]==True]\n",
    "    \n",
    "    out = pd.crosstab(df['year'],df[cat],normalize=0)\n",
    "    \n",
    "    return(out.loc[np.arange(year_lims[0],year_lims[1])])\n",
    "\n",
    "def plot_topic_trend(df,cat,topics,ax,cmap,year_lims=[2000,2019],threshold=0.05,focus_topics=False,alpha=0.2):\n",
    "    '''\n",
    "    Plots topic trends (shares of a category in a topic)\n",
    "    \n",
    "    Args:\n",
    "        df the usual dataframe\n",
    "        topics: topics we want to display\n",
    "        cat: the category of interest\n",
    "        year_lims: first and last year to consider\n",
    "    \n",
    "    '''\n",
    "    activity = []\n",
    "    names = []\n",
    "    \n",
    "    #Use a loop to deal with cases where a category has no activity in a topic\n",
    "    for t in topics:\n",
    "        try:\n",
    "            levels = extract_topic_trend(df.loc[df[t]>threshold],cat,year_lims)\n",
    "            activity.append(levels[True])\n",
    "            names.append(t)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    topic_trends = pd.concat(activity,axis=1).fillna(0)\n",
    "    topic_trends.columns = names\n",
    "    \n",
    "    if focus_topics !=False:\n",
    "        \n",
    "        topic_lookup = {name:val for val,name in enumerate(focus_topics)}\n",
    "\n",
    "        #Color map\n",
    "        cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "        #Create a vector of colors\n",
    "        cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in topic_trends.columns]\n",
    "\n",
    "        #Plot\n",
    "        (100*topic_trends.rolling(window=4).mean().dropna()).plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "        #Fix the legend to focus on key topics\n",
    "        hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in focus_topics],\n",
    "                  labels=[x[1][:50] for x in zip(hand,labs) if x[1] in focus_topics])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        topic_trends.rolling(window=4).mean().dropna().plot(ax=ax)\n",
    "        ax.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_ai_topics = ['cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "                  'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks',\n",
    "                 'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "                 'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "                  'latent-generative_model-generative-generative_models-latent_variables',\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "plot_topic_trend(analysis_fin,'company',cmap='Dark2',topics=topics_filtered,ax=ax,threshold=0.01,focus_topics=core_ai_topics,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "ax.set_title('Share of all papers with company presence')\n",
    "ax.set_ylabel('%')\n",
    "\n",
    "save_fig('company_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardware = ['processing-implementation-computations-frameworks-running','hardware-energy_consumption-power_consumption-energy_efficiency-fpga']\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "plot_topic_trend(analysis_fin,'company',cmap='Dark2',topics=topics_filtered,ax=ax,threshold=0.01,focus_topics=hardware,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "ax.set_title('Share of all papers with company presence')\n",
    "ax.set_ylabel('%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "plot_topic_trend(analysis_fin,'google',cmap='Dark2',topics=topics_filtered,ax=ax,threshold=0.01,focus_topics=core_ai_topics,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "ax.set_title('Share of all papers with Google presence')\n",
    "ax.set_ylabel('%')\n",
    "\n",
    "save_fig('google_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "plot_topic_trend(analysis_fin,'education',cmap='Dark2',topics=topics_filtered,ax=ax,threshold=0.01,focus_topics=core_ai_topics,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "ax.set_ylabel('Share of all papers with education presence')\n",
    "\n",
    "save_fig('ed_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# plot_topic_trend(analysis_fin,'no_education',cmap='Dark2',topics=topics_filtered,ax=ax,threshold=0.01,focus_topics=core_ai_topics,alpha=0.1,year_lims=[2004,2019])\n",
    "\n",
    "# ax.set_ylabel('Share of all papers with no education presence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the levels of university / industry collaboration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variable that captures collaborations\n",
    "analysis_fin['university_industry_collab'] = [all(entity in x for entity in ['Education','Company']) if type(x)==list else np.nan for x in analysis_fin['type_list']]\n",
    "analysis_fin['govt_industry_collab'] = [all(entity in x for entity in ['Government','Company']) if type(x)==list else np.nan for x in analysis_fin['type_list']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['university_industry_collab'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['govt_industry_collab'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*pd.crosstab(analysis_fin['year'],analysis_fin['university_industry_collab'],normalize=0))[True].rolling(window=3).mean().plot(\n",
    "    title='Share of papers with university industry collaborations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_university_industry_collab_trends(df,variable,topic,threshold=0.05):\n",
    "    '''\n",
    "    Study university industry collaborations\n",
    "    \n",
    "    Args:\n",
    "        df as usual\n",
    "        variable is the collaboration variable we want to study\n",
    "        topic the topic\n",
    "        threshold is the threshold for accept a paper in a topic\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    df_with_topic = df.loc[df[topic]>threshold]\n",
    "    \n",
    "\n",
    "    topic_collabs = (100*pd.crosstab(df_with_topic['year'],df_with_topic['university_industry_collab'],normalize=0))[True]\n",
    "    \n",
    "    \n",
    "    return(topic_collabs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract collaborations on 'core AI topics'\n",
    "\n",
    "collabs_in_topics = pd.concat([get_university_industry_collab_trends(analysis_fin,'university_industry_collab',t) for t in core_ai_topics],axis=1).fillna(0)\n",
    "\n",
    "collabs_in_topics.columns = core_ai_topics\n",
    "\n",
    "#Get average collaborations (we set a negative threshold to select all projects)\n",
    "all_collabs = get_university_industry_collab_trends(analysis_fin,'university_industry_collab',community_names[0],threshold=-1)\n",
    "all_collabs.name = 'All subjects'\n",
    "\n",
    "#Concatenate everything\n",
    "collabs_in_topics = pd.concat([all_collabs,collabs_in_topics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "ax = collabs_in_topics.loc[np.arange(1995,2019)].rolling(window=5).mean().dropna().plot(figsize=(14,6),linewidth=3)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "ax.set_ylabel('Share of all papers')\n",
    "ax.set_title('Collaborations between university and industry')\n",
    "\n",
    "hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs)],\n",
    "          labels=[x[1][:50] for x in zip(hand,labs)])\n",
    "\n",
    "\n",
    "\n",
    "save_fig('collaboration_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A network visualisation?\n",
    "\n",
    "Not for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_network_from_list(co_occ):\n",
    "#     '''\n",
    "#     Create a network from a document term matrix.\n",
    "    \n",
    "#     Args\n",
    "#         coocc - a list ehere every element is a collectio of cooccurrences\n",
    "        \n",
    "#     Returns: \n",
    "#         A network\n",
    "    \n",
    "#     '''\n",
    "    \n",
    "#     #Melt the topic mix and remove empty entries\n",
    "#     #cd = pd.melt(mat.reset_index(drop=False),id_vars=[id_var])\n",
    "\n",
    "#     #cd = cd.loc[cd['value']>threshold]\n",
    "\n",
    "#     #This gives us the topic co-occurrence matrix\n",
    "#     #co_occurrence = cd.groupby(id_var)['variable'].apply(lambda x: list(x))\n",
    "    \n",
    "#     #Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "#     #Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "#     sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in co_occ])\n",
    "#     sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "#     #Turn the sector combs into an edgelist\n",
    "#     edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "#     edge_list['weight']=1\n",
    "\n",
    "#     #Group over edge pairs to aggregate weights\n",
    "#     edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "#     edge_list_weighted.sort_values('weight',ascending=False).head(n=10)\n",
    "    \n",
    "#     #Create network and extract communities\n",
    "#     net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "    \n",
    "#     return(net)\n",
    "\n",
    "# # def show_org_network(net,\n",
    "# #                      top_edge_share,\n",
    "# #                      color_lookup=color_lookup,\n",
    "# #                      norm=1000,norm_2=1.2,\n",
    "# #                      layout=nx.kamada_kawai_layout,size_lookup=size_lookup):\n",
    "# #     '''\n",
    "# #     Plots a network visualisation of the topic network.\n",
    "    \n",
    "    \n",
    "# #     '''\n",
    "    \n",
    "# #     new_net = net.copy()\n",
    "    \n",
    "# #     #We drop the \n",
    "# #     #drop_bad_edges = [e for e in new_net.edges(data=True) if not any(x in topic_drop for x in e[:2])]\n",
    "\n",
    "# #     #new_net_2 = nx.Graph(drop_bad_edges)\n",
    "\n",
    "# #     net_weight = sorted(new_net.edges(data=True),key=lambda x: x[2]['weight'],reverse=True)\n",
    "\n",
    "# #     length = int(top_edge_share*len(net_weight))\n",
    "# #     #\n",
    "# #     print(length)\n",
    "    \n",
    "# #     top_edges = net_weight[:length]\n",
    "\n",
    "# #     new_net_2 = nx.Graph(top_edges)\n",
    "    \n",
    "# #     pos = layout(new_net_2,\n",
    "# #                  #weight='weight',\n",
    "# #                  center=(0.5,0.5)\n",
    "# #                 )\n",
    "    \n",
    "# #     #Get positions\n",
    "# #     x,y = [[v[val] for v in pos.values()] for val in [0,1]]    \n",
    "    \n",
    "# #     nx.draw_networkx_nodes(new_net_2,pos,\n",
    "# #                        #node_size=list([size_lookup[x]**norm_2 for x in dict(new_net_2.degree).keys()]),\n",
    "# #                        #node_color = [color_lookup[comm_names[comms[x]]] if comm_names[comms[x]] in color_lookup.keys() else 'white' for x in dict(new_net_2.nodes).keys()],\n",
    "# #                        #cmap='tab20c',\n",
    "# #                        alpha=0.9,edgecolors='darkgrey')\n",
    "    \n",
    "# #     #ax.annotate(label,xy=(np.min(x)+0.02,np.max(y)-0.02),size=16,color='white',fontweight='bold')\n",
    "\n",
    "# #     nx.draw_networkx_edges(new_net_2,pos,width=[e[2]['weight']/norm for e in new_net_2.edges(data=True)],edge_color='white')\n",
    "    \n",
    "# # org_net = make_network_from_list(analysis_fin['institute_list'].dropna())\n",
    "# # # # fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "# # # # show_org_network(org_net,top_edge_share=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis: places\n",
    "\n",
    "We load the lookup between article ids and lads we created in `supp_6` and use it to study the geography of AI research in the UK.\n",
    "\n",
    "More specifically, we want to create three charts:\n",
    "\n",
    "* Concentration trends\n",
    "* Concentration in AI 'core topics'\n",
    "* Comparison between concentration of AI activity and areas at risk of automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/17_8_2019_arxiv_lads.json','r') as infile:\n",
    "    lad_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_w_countries = analysis_fin.dropna(axis=0,subset=['country_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on papers in the UK\n",
    "analysis_uk = analysis_w_countries.loc[[any(var in x for var in ['United Kingdom','Australia']) for x in analysis_w_countries['country_list']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label papers with their lad codes and names\n",
    "analysis_uk['lad_code'],analysis_uk['lad_name'] = [[lad_lookup[x][var] if x in lad_lookup.keys() else np.nan for x in analysis_uk.index] for var in ['lad18cd','lad18nm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing LADs for this analysis\n",
    "analysis_uk = analysis_uk.dropna(axis=0,subset=['lad_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point one: Geographical trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the LADs in the data\n",
    "all_lads = pd.Series(flatten_list(analysis_uk['lad_name'])).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_local_research_concentration(df,top_n,ax,subset_topics=False,lad_list = all_lads,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    This function plots the concentration of research activity in LADs\n",
    "    \n",
    "    Args:\n",
    "        df (df) is the df with papers and lads (so this will have been processed as above)\n",
    "        top_n (int) is how many of the lads do we want to show\n",
    "        ax is the axis\n",
    "        lad_list (list) is the list of LADs to consider\n",
    "        subset_topics (list) is a list where the first element is the list of topics (or communities) we want to focus on; the second is the threshold for inclusion\n",
    "        year_lims is the years to consider\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if subset_topics!=False:\n",
    "        df = df.loc[df[subset_topics[0]].apply(lambda x: any(x>subset_topics[1]),axis=1)]\n",
    "        \n",
    "    \n",
    "        \n",
    "    activity_year = pd.concat([df.loc[[lad in x for x in df['lad_name']]]['year'].value_counts() for lad in lad_list],axis=1).fillna(0)\n",
    "    activity_year.columns = lad_list\n",
    "    \n",
    "    top_lads = activity_year.sum(axis=0).sort_values(ascending=False).index[:top_n]\n",
    "        \n",
    "\n",
    "    (100*activity_year.apply(lambda x: x/x.sum(),axis=1).rolling(window=3).mean()).dropna().loc[np.arange(year_lims[0],\n",
    "                                                                                                   year_lims[1]),top_lads].plot.bar(\n",
    "        stacked=True,width=0.9,cmap='Accent',edgecolor='lightgrey',ax=ax)\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lorenz Curves of concentration\n",
    "all_ai_concentration = pd.Series(\n",
    "    flatten_list(analysis_uk['lad_name'])).value_counts(normalize=True).cumsum()\n",
    "\n",
    "core_ai_concentration = pd.Series(\n",
    "    flatten_list(analysis_uk.loc[analysis_uk[core_ai_topics].apply(lambda x: any(x>0.05),axis=1)]['lad_name'])).value_counts(normalize=True).cumsum()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "pd.concat([x.reset_index(drop=True) for x in [all_ai_concentration,core_ai_concentration]],axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ai_concentration.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_ai_concentration.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "plot_local_research_concentration(analysis_uk,8,ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_ylabel('Share of all papers \\n with LAD presence')\n",
    "ax.set_title('Evolution of local AI research activity in the UK (top 8 locations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('ai_research_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Towwer Hamlets involves Queen Mary university\n",
    "#analysis_uk.loc[['Tower Hamlets' in x for x in analysis_uk['lad_name']]]['institute_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the core topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "plot_local_research_concentration(analysis_uk,8,ax=ax,subset_topics=[core_ai_topics,0.05],year_lims=[2005,2019])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_ylabel('Share of all papers with LAD presence')\n",
    "ax.set_title('Evolution of local AI research activity (state of the art AI topics) in the UK (top 8 locations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('ai_research_core.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_uk.loc[['Wokingham' in x for x in analysis_uk['lad_name']]]['institute_list']\n",
    "#Wokingham is University of Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about, say, health?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# health = [x for x in topics_filtered if comm_names[comms[x]]=='health']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "# plot_local_research_concentration(analysis_uk,8,ax=ax,subset_topics=[health,0.05],year_lims=[2005,2019])\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# #ax.set_ylabel('Share of all papers with LAD presence')\n",
    "# #ax.set_title('Evolution of local AI research activity (new AI topics) in the UK (top 8 locations)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare geography of AI activity and geography of automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load automation data\n",
    "aut = pd.read_csv('../data/processed/19_7_2019_ons_automation_clean.csv',index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lad_activity(df,name,subset_topics=False):\n",
    "    '''\n",
    "    Extracts the distribution of activity by LAD.\n",
    "    \n",
    "    Args:\n",
    "        df (df) with the data\n",
    "        topic_subset (list) if not false, the topics to focus on and their threshold for inclusion\n",
    "        name (str) is the name of the variable\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if subset_topics != False:\n",
    "        df = df.loc[df[subset_topics[0]].apply(lambda x: any(x>subset_topics[1]),axis=1)]\n",
    "        \n",
    "    counts = pd.concat([pd.Series(len(df.loc[[lad in x for x in df['lad_name']]]),name=lad,index=[name]) for lad in all_lads],axis=1).fillna(0).T\n",
    "    \n",
    "    return(counts)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine automation data with AI\n",
    "\n",
    "#List comprehension\n",
    "ai_lad_counts = pd.concat([get_lad_activity(analysis_uk,name,topic_subset) for name,topic_subset in zip(['All AI','Core AI topics'],[False,[core_ai_topics,0.02]])],axis=1)\n",
    "\n",
    "aut_ai = pd.concat([aut.set_index('lad_name'),ai_lad_counts],axis=1).dropna(axis=0,subset=['lad_code']).fillna(0)\n",
    "\n",
    "aut_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_concentrations(df,ranking_var,quantiles,comparisons,ax):\n",
    "    '''\n",
    "    \n",
    "    We create a df that compares share of automation, AI activity accounted by different locations.\n",
    "    \n",
    "    Args:\n",
    "        df is a table with automation and AI activity\n",
    "        ranking_var is the variable we use to create the groups to analyse the distribution\n",
    "        quantiles is the number of groups we create\n",
    "        comparisons are the variables we want to benchmark\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    df_2['aut_rank'] = pd.qcut(df_2[ranking_var],q=quantiles,labels=False)\n",
    "\n",
    "    df_norm = df_2[comparisons].apply(lambda x: x/x.sum())\n",
    "    df_norm['aut_rank'] = df_2['aut_rank']\n",
    "    \n",
    "    (100*df_norm.groupby('aut_rank')[comparisons].sum()).plot.bar(ax=ax)\n",
    "    \n",
    "    #print(df_norm.loc[df_norm['aut_rank']==4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "comps = ['number_high','All AI','Core AI topics']\n",
    "q = np.arange(0,1.1,0.25)\n",
    "\n",
    "benchmark_concentrations(aut_ai,'aut_prob',q,comps,ax)\n",
    "\n",
    "ax.set_xlabel('Workforce automation ranking (quartile)')\n",
    "ax.set_ylabel('% of the total in the UK')\n",
    "\n",
    "ax.legend(title='Variable',labels = ['Workforce with high risk of automation','AI research activity','AI state of the art activity'])\n",
    "ax.set_title('Distribution of AI activity and population at risk of automation')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('lad_comparison.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country comparison (free / not free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of activity in not free countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find top countries\n",
    "countries = pd.Series(flatten_list(analysis_fin['country_list'].dropna())).value_counts().index\n",
    "\n",
    "#Which are not free?\n",
    "not_free_countries_all = [c for c in [x for x in countries if (x in country_status_lookup.keys())] if country_status_lookup[c]=='NF']\n",
    "\n",
    "#Focus on the top countties\n",
    "not_free_countries = not_free_countries_all[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compare trends in not free countries vs average\n",
    "\n",
    "analysis_w_countries = analysis_fin.dropna(axis=0,subset=['country_list'])\n",
    "\n",
    "#Calculate activity for all countries\n",
    "all_country_activity = pd.concat(\n",
    "    [analysis_w_countries.loc[[x in countries for countries in analysis_w_countries['country_list']]]['year'].value_counts() for x in countries],axis=1).fillna(0)\n",
    "\n",
    "all_country_activity.columns = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalised country activity\n",
    "country_norm = all_country_activity.iloc[:,1:].apply(lambda x: x/x.sum(),axis=1).loc[np.arange(2000,2019)]\n",
    "\n",
    "\n",
    "country_ranked = country_norm.loc[:,not_free_countries_all + [x for x in country_norm.columns if x not in not_free_countries_all]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auth_colours = [red_cols(n) if c in not_free_countries else 'lightblue' for n,c in enumerate(country_ranked.columns)]\n",
    "\n",
    "# not_free_to_plot =  country_ranked.rolling(window=3).mean().dropna()\n",
    "\n",
    "# ax = (100*not_free_to_plot[not_free_countries[:-2]]).plot.bar(stacked=True,cmap='Accent',figsize=(10,6),width=0.9,edgecolor='lightgrey',linewidth=0.5)\n",
    "# ax.legend(bbox_to_anchor=(1,1))\n",
    "# ax.set_title('Share of all AI research activity in non-free countries')\n",
    "# ax.set_ylabel('%')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# save_fig('not_free_shares.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_country_activity_norm = all_country_activity.apply(lambda x: x/analysis_w_countries['year'].value_counts())\n",
    "\n",
    "all_country_activity_norm_2 = pd.concat([analysis_w_countries['year'].value_counts(normalize=True),all_country_activity.apply(lambda x: x/x.sum(),axis=0)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_country_activity_norm_2.rename(columns={'year':'All'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,8),nrows=2,sharex=False)\n",
    "\n",
    "(100*not_free_to_plot[not_free_countries[:-2]]).plot.bar(stacked=True,cmap='Accent',width=0.9,edgecolor='lightgrey',linewidth=0.5,ax=ax[0],legend=True)\n",
    "(100*all_country_activity_norm_2[not_free_countries[:-2]].loc[np.arange(2000,2019)]).rolling(window=4).mean().dropna().plot(cmap='Accent',ax=ax[1],legend=False,linewidth=3)\n",
    "(100*all_country_activity_norm_2['All'].loc[np.arange(2000,2019)]).rolling(window=4).mean().dropna().plot(color='black',legend=True,\n",
    "                                                                                                   #title='AI research trends in countries with low freedom indices',\n",
    "                                                                                                         linewidth=3,ax=ax[1],linestyle=':')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(1,1))\n",
    "ax[1].legend(bbox_to_anchor=(1,1))\n",
    "ax[0].set_title('AI Research activity in non-free countries')\n",
    "ax[0].set_ylabel('% of all \\n AI research activity')\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "plt.subplots_adjust(hspace=0.1)\n",
    "\n",
    "\n",
    "ax[1].set_ylabel('Year as \\n share of total AI research')\n",
    "\n",
    "\n",
    "save_fig('political_country_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('nf',size=(8,6))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('not_free_specialisation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the above just driven by China? We create a new variable excluding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['not_free_not_china'] = [(x['nf']==True)&(x['china']==False) for pid,x in analysis_fin.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['not_free_not_china'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients('not_free_not_china')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_topics = ['face-faces-identity-face_recognition-facial','person-surveillance-persons-pedestrian-pedestrians',\n",
    "               #'sentiment_analysis-aspect-sentiment-reviews-opinion',\n",
    "               #'malicious-files-malware-file-analysts',\n",
    "               #'security-privacy-private-secure-trust'\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-sectional comparison**\n",
    "\n",
    "Here we calculate how over (or under?) represented is a topic in a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.concat([cross_sectional_comp(analysis_fin,x,surv_topics,threshold=0.05)['difference'] for x in ['china','not_free_not_china']],axis=1)\n",
    "\n",
    "cross.columns = ['china','not_free_other_than_china']\n",
    "\n",
    "ax = (100*cross.T.iloc[::-1]).plot.barh(title='Specialisation in visual surveillance topics',figsize=(12,5))\n",
    "\n",
    "hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs)],\n",
    "          labels=[x[1][:50] for x in zip(hand,labs)])\n",
    "\n",
    "\n",
    "ax.vlines(x=0,ymin=-1,ymax=2,linestyle=':',color='red')\n",
    "\n",
    "save_fig('activity_in_surveillance_topics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_in_topic(df,topic,name,threshold=0.05,year_lim=[2005,2019],normalize=0):\n",
    "    '''\n",
    "    This returns trends of activity in a topic as a share of all activity\n",
    "    \n",
    "    Args:\n",
    "        df is the df\n",
    "        topic is the topic of interest\n",
    "        threshold is the threshold\n",
    "        year_lim is the years to consider\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if normalize!='none':\n",
    "        trend = pd.crosstab(df['year'],df[topic]>threshold,normalize=normalize)\n",
    "        \n",
    "    else:\n",
    "        trend = pd.crosstab(df['year'],df[topic]>threshold)\n",
    "    \n",
    "    \n",
    "    trend.rename(columns={True:name},inplace=True)\n",
    "    \n",
    "    return(trend.loc[np.arange(year_lim[0],year_lim[1])].fillna(0)[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fin['All']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_trends = [pd.concat(\n",
    "    [trend_in_topic(analysis_fin.loc[analysis_fin[var]==True],topic=t,name=var,threshold=0.05,normalize=0) for var in ['china','not_free_not_china','All']],axis=1) for t in surv_topics[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,7),nrows=2,sharex=True)\n",
    "\n",
    "for num,x in enumerate(surv_topics):\n",
    "    \n",
    "    (100*surv_trends[num]).rolling(window=3).mean().dropna().plot(ax=ax[num],linewidth=3)\n",
    "    \n",
    "    ax[num].set_title(x)\n",
    "    ax[num].set_ylabel('% of papers in topic')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "ax[1].legend().set_visible(False)\n",
    "\n",
    "save_fig('surveillance_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the bump in 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_rec = pd.concat(\n",
    "    [trend_in_topic(analysis_w_countries.loc[[c in c_list for c_list in analysis_w_countries['country_list']]],\n",
    "                                     topic=surv_topics[0],name=c,threshold=0.01,normalize='none') for c in not_free_countries],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iran_face = analysis_w_countries.loc[(analysis_w_countries[surv_topics[0]]>0.05)&(['Russia' in c for c in analysis_w_countries['country_list']])]\n",
    "\n",
    "# for f in iran_face['abstract']:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = analysis_w_countries.loc[(analysis_w_countries['health']>0.1)]\n",
    "\n",
    "# for f in h['abstract']:\n",
    "#     print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputs for the paper\n",
    "\n",
    "Here we will create a dictionary of key results which we will output as a text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Growth rates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = pd.crosstab(arx['year'],arx['is_ai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_growth_rate(series):\n",
    "    '''\n",
    "    This function creates a growth rate for a series\n",
    "    \n",
    "    It takes the series and divides a value by the next value. Divisions by zero are nan\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    growth_rate = []\n",
    "\n",
    "    for n,x in enumerate(series):\n",
    "\n",
    "        if n==0:\n",
    "            out=np.nan\n",
    "            growth_rate.append(np.nan)\n",
    "        else:\n",
    "            if div!=0:\n",
    "                out = 100*((x/div)-1)\n",
    "                growth_rate.append(out)\n",
    "            else:\n",
    "                growth_rate.append(np.nan)\n",
    "\n",
    "        div = x\n",
    "\n",
    "    return(growth_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_growth = year_counts.apply(make_growth_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ai = year_growth.iloc[-5:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_in_fields_total = pd.concat([pd.crosstab(arx.loc[arx[t]>0.5]['year'],\n",
    "                                     arx.loc[arx[t]>0.5]['is_ai'])[1] for t in field_names],axis=1).fillna(0)\n",
    "ai_in_fields_total.columns = field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_non_ai = ai_in_fields_total.apply(make_growth_rate).iloc[-5:].mean().loc[[x for x in field_names if x not in \n",
    "                                                                            ['field_machine_learning_data','field_statistics_probability','field_informatics']]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_trends_total = [pd.concat(\n",
    "    [trend_in_topic(analysis_fin.loc[analysis_fin[var]==True],topic=t,name=var,threshold=0.05,normalize='none') for var in ['china','not_free_not_china','All']],axis=1) for t in surv_topics[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_trends_total[0].apply(make_growth_rate).loc[2014:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_trends_total[1].apply(make_growth_rate).loc[2014:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in ['field_astrophysics','field_biological','field_complex_systems','field_materials_quantum','field_societal']:\n",
    "    \n",
    "#     print(x)\n",
    "#     print('====')\n",
    "    \n",
    "#     d = arx.loc[(arx['is_ai']==True) & (arx['top_field']==x)]\n",
    "    \n",
    "#     get_example(d,5,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
