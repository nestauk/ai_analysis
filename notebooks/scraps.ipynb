{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Graphtool experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = gt.load_graph('../data/processed/11_8_2019_network_all.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm= gt.inference.minimize_nested_blockmodel_dl(gr,B_min=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm.draw(output='../reports/figures/sbm_out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = sbm_comm.get_levels()\n",
    "\n",
    "levels[0].get_blocks()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the vertex names into communites\n",
    "\n",
    "comm_allocations = {gr.vertex_properties['_graphml_vertex_id'][v]:levels[0].get_blocks()[v] for v in gr.get_vertices()}\n",
    "\n",
    "gs = pd.Series(comm_allocations).reset_index(drop=False).groupby(0)['index'].apply(lambda x: ' '.join([el for el in x]))\n",
    "\n",
    "for x in np.arange(0,len(gs)):\n",
    "    \n",
    "    print(x)\n",
    "    print('===')\n",
    "    \n",
    "    \n",
    "    print(gs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.vertex_properties['_graphml_vertex_id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and tokenise all AI abstracts\n",
    "\n",
    "corpus = [re.sub('\\n',' ',x.strip()) for x in arx['abstract'].iloc[random.sample(list(np.arange(0,len(arx))),750000)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanTokenize(corpus).clean().bigram().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train word2vec model\n",
    "w2v = Word2Vec(ct.tokenised,window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_conceps(concept_list,topic_list,w2v=w2v):\n",
    "    '''\n",
    "    This function compares a list of terms associated with a 'concept' and the list of names in a topic\n",
    "    \n",
    "    Args:\n",
    "        -concept_list: a list of terms associated to a concept\n",
    "        -topic_list: list of terms associated with a topic\n",
    "        -w2v is the word embeddings representation\n",
    "    \n",
    "    Output:\n",
    "        -The mean of the pairwise distances between elements in the concept list and elements in the topic list\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the pairs\n",
    "    pairs = product(concept_list,topic_list)\n",
    "    \n",
    "    #Calculate the distances\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for p in pairs:\n",
    "        \n",
    "        try:\n",
    "            a_dist = w2v.wv.similarity(p[0],p[1])\n",
    "            dists.append(a_dist)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "        \n",
    "    return(np.mean(dists))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the keywords for each topic\n",
    "topics_l0 = [[x[0] for x in word_mix] for word_mix in model.topics(l=0).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noe we create the concept - term dict\n",
    "\n",
    "concept_dict = {\n",
    "    'product':['product','service'],\n",
    "    'ethics':['ethical','moral'],\n",
    "    'social':['societal'],\n",
    "    'user':['user','person'],\n",
    "    'theory':['theoretical'],\n",
    "    'military':['weapon','warfare'],\n",
    "    'surveillance':['surveillance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_similarities = []\n",
    "\n",
    "#For each key-value pair in the dict\n",
    "for k,v in concept_dict.items():\n",
    "    \n",
    "    #Compare terms in the concepts with concepts in all the topics\n",
    "    comp = [[t,compare_conceps(v,topics_l0[n])] for n,t in enumerate(topic_names)]\n",
    "    \n",
    "    #Create a df and turn the topics into an index for concatenation later\n",
    "    comp_df = pd.DataFrame(comp).set_index(0)\n",
    "    comp_df.index.name='topic_name'\n",
    "    \n",
    "    #Rename the column\n",
    "    comp_df.columns = [k]\n",
    "    \n",
    "    topic_similarities.append(comp_df)\n",
    "    \n",
    "topic_concepts_df = pd.concat(topic_similarities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_concepts_df.sort_values('social',ascending=False).head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_concepts_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([w_reg[1]['coefficient'],topic_concepts_df],axis=1).corr().iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the analysis binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = ['person-surveillance-persons-pedestrian-pedestrians',\n",
    "       'face-faces-identity-face_recognition-facial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_cross = pd.concat([pd.crosstab(nf_df['nf'],nf_df[t]>0.1,normalize=0)[True] for t in surv],axis=1)\n",
    "surv_cross.columns = surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_cross.T.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_trends = pd.concat([(analysis_df.loc[analysis_df.year==y,surv]>0.1).sum() for y in np.arange(2006,2019)],axis=1)\n",
    "\n",
    "year_trends.columns = np.arange(2006,2019)\n",
    "\n",
    "year_trends_cs = year_trends.T.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = year_trends_cs.plot()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst = analysis_df.loc[(analysis_df['person-surveillance-persons-pedestrian-pedestrians']>0.1),'abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in abst[:10]:\n",
    "    \n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Document Modelling\n",
    "\n",
    "\n",
    "Here we want to measure the similarities between documents with certain topics and 'concept topics' that we have obtained from Wikipedia (see `aux_5`).\n",
    "\n",
    "This involves:\n",
    "\n",
    "1. Combining the arXiv documents with the wikipedia summaries (all AI?)\n",
    "2. Preprocessing them\n",
    "3. Training a doc2vec model on the data\n",
    "4. Consider document distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the text we obtained from wikipedia before\n",
    "wiki_text = pd.read_csv('../data/external/11_8_2019_wiki_text.csv',index_col=None)\n",
    "\n",
    "#We give the text the same columns as in the arxiv papers so that we can concatenate them easily\n",
    "wiki_text.columns = ['paper_id','abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will focus on articles in AI (note that this includes all, not just the articles where we have trained the topic models)\n",
    "#This needs to be trained on all articles\n",
    "#arx_ai = arx.loc[arx['is_ai']==True]\n",
    "\n",
    "arx_ai=arx\n",
    "\n",
    "#We focus on the id and the abstract\n",
    "corpus_1 = arx_ai[['paper_id','abstract']]\n",
    "\n",
    "corpus_2 = pd.concat([corpus_1,wiki_text],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate and turn into a list of list where the first element is the id and the second is the abstract\n",
    "#We also clean some of the markup (eg line breaks)\n",
    "\n",
    "corpus = [[row['paper_id'],re.sub('\\n',' ',row['abstract']).strip()] for pid, row in corpus_2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the corpus into two lists we will use in Doc2Vec\n",
    "doc_corpus_id, doc_corpus_text = [[x[num] for x in corpus] for num in [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocess the text\n",
    "# documents_text = CleanTokenize(doc_corpus_text).clean().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the tagged document corpus - each element is the tokenised text and its id\n",
    "\n",
    "documents = [TaggedDocument(words, [doc_id]) for doc_id, words in zip(doc_corpus_id,documents_text.tokenised)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model!\n",
    "model = Doc2Vec(documents,vector_size=200, window=10, min_count=2, workers=4,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare documents in different topics with the 'concept' topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes all documents with a topic and compares their docvec with the concept topics.\n",
    "\n",
    "def concept_similarity(d2v,topic_mix,topic,concept_names,threshold=0):\n",
    "    '''\n",
    "    \n",
    "    This function calculates a vector of similarities between documents with a topic and concept topics extracted from Wikipedia.\n",
    "    \n",
    "    Args:\n",
    "        d2v: the Doc2Vec model that contains the similarities\n",
    "        topic_mix: the df with the topic distribution for each document\n",
    "        topic to compare with the concept vectors\n",
    "        concept_names: the conceps ids in the doc2vec model\n",
    "        threshold: the threshold for classifying a document in a vector\n",
    "    Returns:\n",
    "        A vector of distances, the mean and median distance.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Ids with topic\n",
    "    ids_with_topic = list(topic_mix.loc[topic_mix[topic]>threshold].index)\n",
    "    \n",
    "    #Store for the similarities\n",
    "    concept_store = {name:[] for name in concept_names}\n",
    "    \n",
    "    #For each concept...\n",
    "    for c in concept_names:\n",
    "        \n",
    "        #Calculate similarities with concept\n",
    "        sims = [d2v.docvecs.similarity(c,doc_id) for doc_id in ids_with_topic]\n",
    "        \n",
    "        #Append similarities to the concept name store\n",
    "        concept_store[c] = sims\n",
    "    \n",
    "    concept_stats = {k:np.median(v) for k,v in concept_store.items()}\n",
    "    \n",
    "    return([concept_store,pd.Series(concept_stats,name=topic)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_names = list(wiki_text['paper_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables of interest\n",
    "interesting_cuts = [['freedom_list','NF'],\n",
    "                    ['country_list','China'],['country_list','Russia'],['country_list','Turkey'],\n",
    "                    ['type_list','Company'],['type_list','Government'],['type_list','Education'],\n",
    "                    ['institute_list','Google'],['institute_list','Facebook'],['institute_list','IBM'],['institute_list','Microsoft']]\n",
    "\n",
    "#Create the expanded df\n",
    "analysis_df_expanded = analysis_df.copy()\n",
    "\n",
    "#For each interesting variable we expand the df\n",
    "for detect in interesting_cuts:\n",
    "    \n",
    "    analysis_df_expanded = make_exog(analysis_df_expanded,value_container=detect[0],value=detect[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf = topic_comparison(analysis_df_2,topics_filtered,'has_female',mean_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hf['health'].apply(lambda x: x/x.sum(),axis=0).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This doesn't work very well**\n",
    "\n",
    "There are several reasons for this:\n",
    "\n",
    "* The documents I am using to measure ethics, surveillance etc are not very good\n",
    "* The topics are too aggregated to pick up similarity with a concept\n",
    "* Topics co-occur with each other. Their relation with the concepts aren't linear.\n",
    "* Let's park this for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Datashader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_comm_names = [7,13,31,3,18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "from datashader.layout import random_layout, circular_layout, forceatlas2_layout\n",
    "from datashader.bundling import connect_edges, hammer_bundle\n",
    "\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ds_network_from_doc_term_matrix(mat,threshold,id_var):\n",
    "    '''\n",
    "    Create a network from a document term matrix.\n",
    "    \n",
    "    Args\n",
    "        Document term matrix where the rows are documents and the columns are topics\n",
    "        threshold is the threshold to consider that a topic is present in a matrix.\n",
    "        \n",
    "    Returns: \n",
    "        A network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Melt the topic mix and remove empty entries\n",
    "    cd = pd.melt(mat.reset_index(drop=False),id_vars=[id_var])\n",
    "\n",
    "    cd = cd.loc[cd['value']>threshold]\n",
    "\n",
    "    #This gives us the topic co-occurrence matrix\n",
    "    co_occurrence = cd.groupby(id_var)['variable'].apply(lambda x: list(x))\n",
    "    \n",
    "    #Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "    #Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "    sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in co_occurrence])\n",
    "    sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "    #Turn the sector combs into an edgelist\n",
    "    edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "    \n",
    "    node_list = pd.DataFrame([x for x in mat.columns],columns=['name'])\n",
    "    \n",
    "    node_map = {val:num for num,val in enumerate(node_list['name'])}\n",
    "    \n",
    "    edge_list_mapped = edge_list.applymap(lambda x: node_map[x])\n",
    "    \n",
    "    node_list_mapped = node_list.applymap(lambda x: 'node'+str(node_map[x]))\n",
    "    \n",
    "    node_list_mapped['cat'] = [comms[n] if comms[n] in my_comm_names else 0 for n in node_list['name']]\n",
    "    \n",
    "    node_list_mapped['cat']=node_list_mapped['cat'].astype('category')\n",
    "    \n",
    "    #return(node_map)\n",
    "    \n",
    "    return([node_map,edge_list_mapped,node_list_mapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = make_ds_network_from_doc_term_matrix(doc_topic_l0[topics_filtered],0.05,'paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = out[2]\n",
    "\n",
    "edges=out[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes['cat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circular  = circular_layout(nodes, uniform=False)\n",
    "randomloc = random_layout(nodes)\n",
    "randomloc.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import Accent\n",
    "\n",
    "cvsopts = dict(plot_height=400, plot_width=400)\n",
    "\n",
    "def nodesplot(nodes, name=None, canvas=None, cat=None):\n",
    "    canvas = ds.Canvas(**cvsopts) if canvas is None else canvas\n",
    "    aggregator=None if cat is None else ds.count_cat(cat)\n",
    "    agg=canvas.points(nodes,'x','y',aggregator)\n",
    "    return tf.spread(tf.shade(agg, cmap=Accent), px=5, name=name)\n",
    "\n",
    "tf.Images(nodesplot(randomloc,\"Random layout\",cat='cat'),\n",
    "          nodesplot(circular, \"Circular layout\",cat='cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.errors import NumbaDeprecationWarning, NumbaPendingDeprecationWarning, NumbaWarning\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "forcedirected = forceatlas2_layout(nodes, edges)\n",
    "tf.Images(nodesplot(forcedirected, \"ForceAtlas2 layout\",cat='cat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edgesplot(edges, name=None, canvas=None):\n",
    "    canvas = ds.Canvas(**cvsopts) if canvas is None else canvas\n",
    "    return tf.shade(canvas.line(edges, 'x','y', agg=ds.count()), name=name)\n",
    "\n",
    "def graphplot(nodes, edges, name=\"\", canvas=None, cat=None):\n",
    "    if canvas is None:\n",
    "        xr = nodes.x.min(), nodes.x.max()\n",
    "        yr = nodes.y.min(), nodes.y.max()\n",
    "        canvas = ds.Canvas(x_range=xr, y_range=yr, **cvsopts)\n",
    "\n",
    "    np = nodesplot(nodes, name + \" nodes\", canvas, cat)\n",
    "    ep = edgesplot(edges, name + \" edges\", canvas)\n",
    "    return tf.stack(ep, np, how=\"over\", name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = circular\n",
    "fd = forcedirected\n",
    "\n",
    "%time cd_d = graphplot(cd, connect_edges(cd,edges), \"Circular layout\",cat='cat')\n",
    "%time fd_d = graphplot(fd, connect_edges(fd,edges), \"Force-directed\",cat='cat')\n",
    "#%time cd_b = graphplot(cd, hammer_bundle(cd,edges), \"Circular layout, bundled\")\n",
    "#%time fd_b = graphplot(fd, hammer_bundle(fd,edges), \"Force-directed, bundled\")\n",
    "\n",
    "tf.Images(cd_d,fd_d,\n",
    "          #cd_b,fd_b\n",
    "         ).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This doesn't work very well**\n",
    "\n",
    "There are several reasons for this:\n",
    "\n",
    "* The documents I am using to measure ethics, surveillance etc are not very good\n",
    "* The topics are too aggregated to pick up similarity with a concept\n",
    "* Topics co-occur with each other. Their relation with the concepts aren't linear.\n",
    "* Let's park this for now\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### National disruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_countries_2 = list(flatten_freq(data_w_countries['country_list'])[1:6].index)+['multinational']\n",
    "\n",
    "# national_disr = pd.concat([make_disruption_tables(data_w_countries.loc[\n",
    "#     [c in c_list for c_list in data_w_countries['country_list']]])[1] for c in top_countries_2],axis=1)\n",
    "# national_disr.columns = top_countries_2\n",
    "\n",
    "type_disr = pd.concat([make_disruption_tables(data_w_countries.loc[\n",
    "    [t in t_list for t_list in data_w_countries['type_list']]])[1] for t in ['Company','Education']],axis=1)\n",
    "type_disr.columns = ['Company','Education']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = type_disr.dropna().rolling(window=3).mean().dropna().plot(cmap='Set1',linewidth=4,figsize=(6,8))\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['entropy'] = calculate_entropy(data,topics,'entropy')['entropy']\n",
    "\n",
    "d = data.loc[data['top_field']=='field_machine_learning_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period=list(np.arange(2000,2019))\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(13,5))\n",
    "\n",
    "gr = d.groupby('year')['entropy']\n",
    "\n",
    "ax.violinplot([gr.get_group(y) for y in period],widths=0.9,showmedians=True)\n",
    "ax.set_xticks(np.arange(0,len(period)+1))  \n",
    "ax.set_xticklabels(['']+period,rotation=90)  \n",
    "\n",
    "#d.groupby('year')['entropy'].median().plot(ax=ax)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data shader test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datashader as ds\n",
    "from datashader import transfer_functions as tf\n",
    "from datashader.colors import Greys9\n",
    "Greys9_r = list(reversed(Greys9))[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_matched_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_counts =pd.DataFrame(grid_matched_clean.groupby(['institute_lat','institute_lon']).size()).reset_index(drop=False)\n",
    "\n",
    "paper_counts.columns = ['lat','lon','counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = ds.Canvas(plot_width=1200, plot_height=700)\n",
    "agg = cvs.points(paper_counts, 'lon', 'lat',  ds.count('counts'))\n",
    "img = tf.shade(agg, cmap=[\"white\", 'darkblue'], how='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shade(agg, cmap=Greys9, how='eq_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "\n",
    "\n",
    "ax.hexbin(grid_matched['institute_lon'],grid_matched['institute_lat'],cmap='Reds',bins=10,gridsize=75,edgecolor='lightgrey',linewidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
