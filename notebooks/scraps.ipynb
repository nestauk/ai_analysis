{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Graphtool experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool as gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr = gt.load_graph('../data/processed/11_8_2019_network_all.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm= gt.inference.minimize_nested_blockmodel_dl(gr,B_min=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm.draw(output='../reports/figures/sbm_out.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbm_comm.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = sbm_comm.get_levels()\n",
    "\n",
    "levels[0].get_blocks()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the vertex names into communites\n",
    "\n",
    "comm_allocations = {gr.vertex_properties['_graphml_vertex_id'][v]:levels[0].get_blocks()[v] for v in gr.get_vertices()}\n",
    "\n",
    "gs = pd.Series(comm_allocations).reset_index(drop=False).groupby(0)['index'].apply(lambda x: ' '.join([el for el in x]))\n",
    "\n",
    "for x in np.arange(0,len(gs)):\n",
    "    \n",
    "    print(x)\n",
    "    print('===')\n",
    "    \n",
    "    \n",
    "    print(gs[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.vertex_properties['_graphml_vertex_id'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and tokenise all AI abstracts\n",
    "\n",
    "corpus = [re.sub('\\n',' ',x.strip()) for x in arx['abstract'].iloc[random.sample(list(np.arange(0,len(arx))),750000)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanTokenize(corpus).clean().bigram().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train word2vec model\n",
    "w2v = Word2Vec(ct.tokenised,window=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_conceps(concept_list,topic_list,w2v=w2v):\n",
    "    '''\n",
    "    This function compares a list of terms associated with a 'concept' and the list of names in a topic\n",
    "    \n",
    "    Args:\n",
    "        -concept_list: a list of terms associated to a concept\n",
    "        -topic_list: list of terms associated with a topic\n",
    "        -w2v is the word embeddings representation\n",
    "    \n",
    "    Output:\n",
    "        -The mean of the pairwise distances between elements in the concept list and elements in the topic list\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the pairs\n",
    "    pairs = product(concept_list,topic_list)\n",
    "    \n",
    "    #Calculate the distances\n",
    "    \n",
    "    dists = []\n",
    "    \n",
    "    for p in pairs:\n",
    "        \n",
    "        try:\n",
    "            a_dist = w2v.wv.similarity(p[0],p[1])\n",
    "            dists.append(a_dist)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "        \n",
    "    return(np.mean(dists))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the keywords for each topic\n",
    "topics_l0 = [[x[0] for x in word_mix] for word_mix in model.topics(l=0).values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noe we create the concept - term dict\n",
    "\n",
    "concept_dict = {\n",
    "    'product':['product','service'],\n",
    "    'ethics':['ethical','moral'],\n",
    "    'social':['societal'],\n",
    "    'user':['user','person'],\n",
    "    'theory':['theoretical'],\n",
    "    'military':['weapon','warfare'],\n",
    "    'surveillance':['surveillance']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_similarities = []\n",
    "\n",
    "#For each key-value pair in the dict\n",
    "for k,v in concept_dict.items():\n",
    "    \n",
    "    #Compare terms in the concepts with concepts in all the topics\n",
    "    comp = [[t,compare_conceps(v,topics_l0[n])] for n,t in enumerate(topic_names)]\n",
    "    \n",
    "    #Create a df and turn the topics into an index for concatenation later\n",
    "    comp_df = pd.DataFrame(comp).set_index(0)\n",
    "    comp_df.index.name='topic_name'\n",
    "    \n",
    "    #Rename the column\n",
    "    comp_df.columns = [k]\n",
    "    \n",
    "    topic_similarities.append(comp_df)\n",
    "    \n",
    "topic_concepts_df = pd.concat(topic_similarities,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_concepts_df.sort_values('social',ascending=False).head(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_concepts_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([w_reg[1]['coefficient'],topic_concepts_df],axis=1).corr().iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the analysis binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = ['person-surveillance-persons-pedestrian-pedestrians',\n",
    "       'face-faces-identity-face_recognition-facial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_cross = pd.concat([pd.crosstab(nf_df['nf'],nf_df[t]>0.1,normalize=0)[True] for t in surv],axis=1)\n",
    "surv_cross.columns = surv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_cross.T.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_trends = pd.concat([(analysis_df.loc[analysis_df.year==y,surv]>0.1).sum() for y in np.arange(2006,2019)],axis=1)\n",
    "\n",
    "year_trends.columns = np.arange(2006,2019)\n",
    "\n",
    "year_trends_cs = year_trends.T.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = year_trends_cs.plot()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst = analysis_df.loc[(analysis_df['person-surveillance-persons-pedestrian-pedestrians']>0.1),'abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in abst[:10]:\n",
    "    \n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Document Modelling\n",
    "\n",
    "\n",
    "Here we want to measure the similarities between documents with certain topics and 'concept topics' that we have obtained from Wikipedia (see `aux_5`).\n",
    "\n",
    "This involves:\n",
    "\n",
    "1. Combining the arXiv documents with the wikipedia summaries (all AI?)\n",
    "2. Preprocessing them\n",
    "3. Training a doc2vec model on the data\n",
    "4. Consider document distances\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the text we obtained from wikipedia before\n",
    "wiki_text = pd.read_csv('../data/external/11_8_2019_wiki_text.csv',index_col=None)\n",
    "\n",
    "#We give the text the same columns as in the arxiv papers so that we can concatenate them easily\n",
    "wiki_text.columns = ['paper_id','abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will focus on articles in AI (note that this includes all, not just the articles where we have trained the topic models)\n",
    "#This needs to be trained on all articles\n",
    "#arx_ai = arx.loc[arx['is_ai']==True]\n",
    "\n",
    "arx_ai=arx\n",
    "\n",
    "#We focus on the id and the abstract\n",
    "corpus_1 = arx_ai[['paper_id','abstract']]\n",
    "\n",
    "corpus_2 = pd.concat([corpus_1,wiki_text],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate and turn into a list of list where the first element is the id and the second is the abstract\n",
    "#We also clean some of the markup (eg line breaks)\n",
    "\n",
    "corpus = [[row['paper_id'],re.sub('\\n',' ',row['abstract']).strip()] for pid, row in corpus_2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn the corpus into two lists we will use in Doc2Vec\n",
    "doc_corpus_id, doc_corpus_text = [[x[num] for x in corpus] for num in [0,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #preprocess the text\n",
    "# documents_text = CleanTokenize(doc_corpus_text).clean().bigram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the tagged document corpus - each element is the tokenised text and its id\n",
    "\n",
    "documents = [TaggedDocument(words, [doc_id]) for doc_id, words in zip(doc_corpus_id,documents_text.tokenised)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model!\n",
    "model = Doc2Vec(documents,vector_size=200, window=10, min_count=2, workers=4,epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare documents in different topics with the 'concept' topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes all documents with a topic and compares their docvec with the concept topics.\n",
    "\n",
    "def concept_similarity(d2v,topic_mix,topic,concept_names,threshold=0):\n",
    "    '''\n",
    "    \n",
    "    This function calculates a vector of similarities between documents with a topic and concept topics extracted from Wikipedia.\n",
    "    \n",
    "    Args:\n",
    "        d2v: the Doc2Vec model that contains the similarities\n",
    "        topic_mix: the df with the topic distribution for each document\n",
    "        topic to compare with the concept vectors\n",
    "        concept_names: the conceps ids in the doc2vec model\n",
    "        threshold: the threshold for classifying a document in a vector\n",
    "    Returns:\n",
    "        A vector of distances, the mean and median distance.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Ids with topic\n",
    "    ids_with_topic = list(topic_mix.loc[topic_mix[topic]>threshold].index)\n",
    "    \n",
    "    #Store for the similarities\n",
    "    concept_store = {name:[] for name in concept_names}\n",
    "    \n",
    "    #For each concept...\n",
    "    for c in concept_names:\n",
    "        \n",
    "        #Calculate similarities with concept\n",
    "        sims = [d2v.docvecs.similarity(c,doc_id) for doc_id in ids_with_topic]\n",
    "        \n",
    "        #Append similarities to the concept name store\n",
    "        concept_store[c] = sims\n",
    "    \n",
    "    concept_stats = {k:np.median(v) for k,v in concept_store.items()}\n",
    "    \n",
    "    return([concept_store,pd.Series(concept_stats,name=topic)])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_names = list(wiki_text['paper_id'])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
