{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversity analysis\n",
    "\n",
    "This notebook will analyse the diversity of AI research.\n",
    "\n",
    "This entails:\n",
    "\n",
    "* Loading the data\n",
    "* Estimating diversity based on agglomerative clustering\n",
    "* Comparing diversities at different points\n",
    "* Comparing diversities of research involving different groups (eg universities vs companies, research involving women and not involving women)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings (for when I concatenate dfs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statsmodels.api import OLS, Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic functions\n",
    "def save_fig(name,path='../reports/figures/paper_rev/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')\n",
    "    \n",
    "    # Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy_lookup(names_list,length=False):\n",
    "    '''\n",
    "    \n",
    "    Creates a cheap lookup between names, removing underscores and capitalising\n",
    "    \n",
    "    Args:\n",
    "        names_list (list) is the list of names we want to tidy\n",
    "        length is if we want to only keep a certain length of the name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = {x:re.sub('_',' ',x).capitalize() for x in names_list}\n",
    "    return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_comp(df,variable,topics,threshold):\n",
    "    '''\n",
    "    This function compares activity by topics between categories.\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe we are using (generally analysis_fin, with rows = papers and columns = variables and metadata)\n",
    "        variable is the variable we are using for the comparison\n",
    "        topics is the topics where we want to compare (generally the community names)\n",
    "        threshold is the threshold we want to use to determine if a paper is in a topic or not\n",
    "    \n",
    "    Returns a df with the shares of papers in each topic sorted by their distances\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the counts df.\n",
    "    \n",
    "    #We are extracting, for each topics, the % of papers with at least one female author when the topic is present, and when it isn't.\n",
    "    group_counts = pd.concat([pd.crosstab(df[variable],df[t]>threshold,normalize=1).loc[True,:] for t in topics],axis=1)\n",
    "    \n",
    "    #Name\n",
    "    group_counts.columns = topics\n",
    "    \n",
    "    #Transpose\n",
    "    group_counts = group_counts.T\n",
    "    \n",
    "    #Rename variables\n",
    "    group_counts.columns = [variable+f'_{value}' for value in ['false','true']]\n",
    "    \n",
    "    #Create a measure of difference\n",
    "    group_counts['difference'] = (group_counts.iloc[:,1]/group_counts.iloc[:,0])-1\n",
    "    \n",
    "    #Output\n",
    "    out = group_counts.sort_values('difference',ascending=False)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def topic_regression(df,target_list,exog,controls,model,binarise=False,standardise=True,cov='HC1'):\n",
    "    '''\n",
    "    \n",
    "    This function regresses topic weights (or their binarisation) on predictors.\n",
    "    \n",
    "    Arguments:\n",
    "        -Df with the variables\n",
    "        -target_list: target variables. This is a list we loop over. \n",
    "        -exog: exogenous variable\n",
    "        -controls\n",
    "        -model type. OLS? Logit? TODO fix the logit\n",
    "        -Binarise in case we are using logit. If not False, the value is the threshold \n",
    "            TODO when we binarise the highly detailed models, some of them become all zeros. This will work better\n",
    "            with the mopre aggregate topics\n",
    "        -Standardise if we standardise and log the topic weights\n",
    "    \n",
    "    Returns\n",
    "        -A list of statsmodels summaries\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Drop rows with missing values - sm doesn't like them\n",
    "    df_2 = df[target_list+exog+controls].dropna(axis=0)\n",
    "    \n",
    "    #Standardise targets?\n",
    "    if standardise==True:\n",
    "        df_2[target_list] = (np.log(df_2[target_list]+0.00000001)).apply(zscore).astype(float)\n",
    "    \n",
    "    #Binarise targets if we are doing a logit\n",
    "    if binarise!=False:\n",
    "        df_2[target_list] = df_2[target_list].applymap(lambda x: x>binarise).astype(float)\n",
    "    \n",
    "    \n",
    "    #Extract the exogenous and controls, add constant and cast as float\n",
    "    exog_controls = add_constant(df_2[exog+controls]).astype(float)\n",
    "    \n",
    "\n",
    "    #Container output\n",
    "    out = []\n",
    "    coeffs = []\n",
    "    \n",
    "    #One regression for each target\n",
    "    for t in list(target_list):\n",
    "        \n",
    "        #There we gp. \n",
    "        reg = model(endog=df_2[t],exog=exog_controls).fit(cov_type=cov,disp=0)\n",
    "        \n",
    "        out.append(reg.summary())\n",
    "        \n",
    "        #coeffs.append(reg)\n",
    "        if model == OLS:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.rsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','r_square']\n",
    "    \n",
    "        else:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.prsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','pr_square']\n",
    " \n",
    "    \n",
    "    return([out,reg_coeff.sort_values('coefficient',ascending=False)])\n",
    "        \n",
    "       \n",
    "\n",
    "def plot_regression_coefficients(df,var,cov='HC1',size=(8,6),ax=False,ncols=3):\n",
    "    '''\n",
    "    Plots regression coefficients.\n",
    "    \n",
    "    Arg:\n",
    "        variable we use as predictor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reg = topic_regression(df,topics,[var],controls,OLS,cov='HC1')\n",
    "    \n",
    "    if ax==False:\n",
    "        fig,ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plot_topic_bar(reg[1]['coefficient'],cl=color_lookup,ax=ax,ncols=ncols)\n",
    "\n",
    "    ax.set_title(f'Regression coefficient using {var} as predictor')\n",
    "\n",
    "def topic_comparison(df,target_list,exog,concept_lookup,quantiles=np.arange(0,1.1,0.2),thres=0):\n",
    "    '''\n",
    "    This function compares the distribution of activity in various topics depending on an exogenous variable of interest. \n",
    "    \n",
    "    Args:\n",
    "        Df with the topic mix and metadata\n",
    "        target_list are the topics to consider\n",
    "        exog is the variable to crosstab topics against\n",
    "        concept_lookup is a df with the median proximity of each topic to the concepts\n",
    "        quantiles is how we discretise the concept lookup (default value is quintiles)\n",
    "        thres: =limit for considering a topic as present\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Copy df\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Discretise the concept lookup\n",
    "    \n",
    "    conc_discr = concept_lookup.apply(lambda x: pd.qcut(x,q=quantiles,labels=False,duplicates='drop'))\n",
    "\n",
    "    \n",
    "    #Calculate levels of activity per topic based on the exog variable\n",
    "    \n",
    "    topic_distr = pd.concat([pd.crosstab(df_2[exog],df_2[t]>thres)[True] for t in target_list],axis=1).T\n",
    "    topic_distr.index = target_list\n",
    "    \n",
    "    \n",
    "    #Merge the count with the concept lookup\n",
    "    disc = pd.melt(pd.concat([topic_distr,conc_discr],axis=1).reset_index(drop=False),id_vars=['index']+list(conc_discr.columns))\n",
    "    \n",
    "    #This is the list where we store the results\n",
    "    store={}\n",
    "    \n",
    "    for c in concept_lookup.columns:\n",
    "        \n",
    "        out = pd.pivot_table(disc.groupby([c,'variable'])['value'].sum().reset_index(drop=False),index=c,columns='variable',values='value')\n",
    "        #out.apply(lambda x: x/x.sum()).plot.bar()\n",
    "        \n",
    "        store[c] = out\n",
    "                                      \n",
    "    #Output dfs with the comparisons\n",
    "    return(store)\n",
    "\n",
    "def plot_topic_bar(table,cl,ax,ncols):\n",
    "    '''\n",
    "    Simple function to plot topic bars which includes colours based on the topic-label lookup\n",
    "    \n",
    "    Args:\n",
    "        table has topics in the index and a value to plot in the columns\n",
    "        cl is the colour lookup between communities and topics\n",
    "        ax is the plotting axe\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in table.index]\n",
    "    \n",
    "    table.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=ncols)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    \n",
    "def calculate_entropy(df,categories,category):\n",
    "    '''\n",
    "    We calculate entropy inside a paper using a distribution over semantic variables (eg discipline, community or topic). These have to be normalised\n",
    "    \n",
    "    arguments:\n",
    "        df is the analysis df with relevant topics and metadata\n",
    "        categories are the topics we want to compare\n",
    "        \n",
    "    outputs\n",
    "        A df with entropy measures by paper\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #Normalise\n",
    "    norm = df[categories].apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    ent = pd.DataFrame((norm.apply(lambda x: entropy(x),axis=1)),columns=['entropy'])\n",
    "    \n",
    "    ent['cat']=category\n",
    "    \n",
    "    return(ent)\n",
    "\n",
    "def make_exog(df,value_container,value,make_dummy=True):\n",
    "    '''\n",
    "    This creates exogenous variables for modelling later.\n",
    "    \n",
    "    Argument:\n",
    "        -df contains the variable where we want to find a value\n",
    "        -variable_container is the column where we want to look for the value\n",
    "        -value is the value we are looking for\n",
    "        -make_dummy: if true it just counts if the value is present. If false, it counts how many times it happens. \n",
    "        \n",
    "    Output\n",
    "        -A df with the new column (named)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Create a tidy variable name\n",
    "    column_name = re.sub(' ','_',value.lower())\n",
    "    \n",
    "    #If we want to create a dummy...\n",
    "    if make_dummy == True:\n",
    "        \n",
    "        #We just look for it in the value container\n",
    "        #There are some missing values so we have some control flow to manage that. \n",
    "        df_2[column_name] = [value in x if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Otherwise, we count how many times it occurs\n",
    "        #We deal with missing values ('non lists') as before\n",
    "        df_2[column_name] = [x.count(value) if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "        \n",
    "    return(df_2)\n",
    "    \n",
    "\n",
    "def extract_topic_trend(df,cat,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    Extracts evolution of a share of a category in a topic of interest\n",
    "    \n",
    "    Args:\n",
    "        df: the usual dataframe\n",
    "        cat: the category we are interested in\n",
    "        year_lims: first and last year to consider\n",
    "\n",
    "    '''\n",
    "    #rel_df = df.loc[df[cat]==True]\n",
    "    \n",
    "    out = pd.crosstab(df['year'],df[cat],normalize=0)\n",
    "    \n",
    "    return(out.loc[np.arange(year_lims[0],year_lims[1])])\n",
    "\n",
    "def plot_topic_trend(df,cat,topics,ax,cmap,year_lims=[2000,2019],threshold=0.05,focus_topics=False,alpha=0.2):\n",
    "    '''\n",
    "    Plots topic trends (shares of a category in a topic)\n",
    "    \n",
    "    Args:\n",
    "        df the usual dataframe\n",
    "        topics: topics we want to display\n",
    "        cat: the category of interest\n",
    "        year_lims: first and last year to consider\n",
    "    \n",
    "    '''\n",
    "    activity = []\n",
    "    names = []\n",
    "    \n",
    "    #Use a loop to deal with cases where a category has no activity in a topic\n",
    "    for t in topics:\n",
    "        try:\n",
    "            levels = extract_topic_trend(df.loc[df[t]>threshold],cat,year_lims)\n",
    "            activity.append(levels[True])\n",
    "            names.append(t)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    topic_trends = pd.concat(activity,axis=1).fillna(0)\n",
    "    topic_trends.columns = names\n",
    "    \n",
    "    if focus_topics !=False:\n",
    "        \n",
    "        topic_lookup = {name:val for val,name in enumerate(focus_topics)}\n",
    "\n",
    "        #Color map\n",
    "        cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "        #Create a vector of colors\n",
    "        cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in topic_trends.columns]\n",
    "\n",
    "        #Plot\n",
    "        (100*topic_trends.rolling(window=4).mean().dropna()).plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "        #Fix the legend to focus on key topics\n",
    "        hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in focus_topics],\n",
    "                  labels=[x[1][:50] for x in zip(hand,labs) if x[1] in focus_topics])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        topic_trends.rolling(window=4).mean().dropna().plot(ax=ax)\n",
    "        ax.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_university_industry_collab_trends(df,variable,topic,threshold=0.05):\n",
    "    '''\n",
    "    Study university industry collaborations\n",
    "    \n",
    "    Args:\n",
    "        df as usual\n",
    "        variable is the collaboration variable we want to study\n",
    "        topic the topic\n",
    "        threshold is the threshold for accept a paper in a topic\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    df_with_topic = df.loc[df[topic]>threshold]\n",
    "    \n",
    "\n",
    "    topic_collabs = (100*pd.crosstab(df_with_topic['year'],df_with_topic['university_industry_collab'],normalize=0))[True]\n",
    "    \n",
    "    \n",
    "    return(topic_collabs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "`analysis_pack` contains the metadata and data that we serialised at the end of the `06` data integration notebook.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Community names for the communities (`index->community name`)\n",
    "* Community indices for topics (`topic -> community index`)\n",
    "* Filtered topic names (`topic names`)\n",
    "* Network object with topic co-occurrences\n",
    "* Analysis df\n",
    "* arx is the enriched arXiv dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/24_8_2019_analysis_pack.p','rb') as infile:\n",
    "    analysis_pack = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = analysis_pack[0]\n",
    "comms = analysis_pack[1]\n",
    "topics = analysis_pack[2]\n",
    "network = analysis_pack[3]\n",
    "data = analysis_pack[4]\n",
    "arx = analysis_pack[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}\n",
    "\n",
    "#These are the field names\n",
    "field_names = ['field_astrophysics',\n",
    " 'field_biological',\n",
    " 'field_complex_systems',\n",
    " 'field_informatics',\n",
    " 'field_machine_learning_data',\n",
    " 'field_materials_quantum',\n",
    " 'field_mathematical_physics',\n",
    " 'field_mathematics_1',\n",
    " 'field_mathematics_2',\n",
    " 'field_optimisation',\n",
    " 'field_particle_physics',\n",
    " 'field_physics_education',\n",
    " 'field_societal',\n",
    " 'field_statistics_probability']\n",
    "\n",
    "#Create tidy field names for legend etc\n",
    "tidy_field_lookup = {x:re.sub('_',' ',x[6:]).capitalize() for x in field_names}\n",
    "\n",
    "community_names = [x for x in list(set((comm_names.values()))) if x!='mixed']\n",
    "\n",
    "tidy_comms_lookup = make_tidy_lookup(community_names)\n",
    "\n",
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comms_lookup[l],edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some minor processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables of interest\n",
    "interesting = [['type_list','Company'],['type_list','Government'],['type_list','Education']]\n",
    "              # ['institute_list_2','Google'],['institute_list_2','Facebook'],['institute_list_2','IBM'],['institute_list_2','Microsoft'],\n",
    "              #['institute_list_2','Apple'],['institute_list_2','Amazon']]\n",
    "\n",
    "#Create the expanded df\n",
    "data_2 = data.copy()\n",
    "\n",
    "#For each interesting variable we expand the df\n",
    "for detect in interesting:\n",
    "    \n",
    "    data_2 = make_exog(data_2,value_container=detect[0],value=detect[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['only_company'] = (data_2['company']==True) & (data_2['education']==False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "We want to analyse the diversity of AI research. \n",
    "\n",
    "Questions:\n",
    "\n",
    "* What definition of diversity do we use?\n",
    "* What definition of distance do we use?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.cluster.hierarchy import cophenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to be able to colour nodes based on their communities and their position in the chart\n",
    "#topic_lookup = {n:t for n,t in enumerate(topics)}\n",
    "\n",
    "color_lookup_names = {}\n",
    "\n",
    "for t in topics:\n",
    "    \n",
    "    comm_name = comm_names[comms[t]]\n",
    "    \n",
    "    \n",
    "    c = color_lookup[comm_name] if comm_name in color_lookup.keys() else 'grey'\n",
    "    \n",
    "    color_lookup_names[t] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dendrogram that colours links based on their topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_diversity(data,topics,metric,plot=False):\n",
    "    '''\n",
    "    This function creates a hierarchical tree based on distances between topics and calculates diversity (the length in the branches of that tree)\n",
    "    \n",
    "    Args:\n",
    "        data (df) is the topic mix for each paper\n",
    "        topics (list) are the topics to consider\n",
    "        metric (str) is the metric of distance that we use\n",
    "        plot (bool) is the boolean for whethet to plot or not\n",
    "    \n",
    "    Returns the measure of distance and (perhaps) a plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Plot distances\n",
    "    dists = hierarchy.linkage(data[topics].T,metric=metric)\n",
    "    \n",
    "    #This gives the presence of a topic in the population of papers. Currently using sum but it could be something else\n",
    "    topic_length = data[topics].sum()\n",
    "    \n",
    "    #If no plot\n",
    "    \n",
    "    if plot==False:\n",
    "        dend = hierarchy.dendrogram(dists,no_plot=True)\n",
    "        \n",
    "        #This gives the length of each branch at merge\n",
    "        lengths = [x[1] for x in dend['dcoord']]\n",
    "        \n",
    "        path_length = np.sum(lengths)\n",
    "        \n",
    "        return(path_length)                       \n",
    "        \n",
    "    else:\n",
    "        #Create axes etc\n",
    "        fig,ax = plt.subplots(figsize=(6,12),ncols=2)\n",
    "\n",
    "        dend = hierarchy.dendrogram(dists,\n",
    "                                   labels=topics,\n",
    "                                   ax=ax[0],orientation='left')\n",
    "        \n",
    "        #Labels to reorder the other plot \n",
    "        labels = [t.get_text() for t in ax[0].get_yticklabels()]\n",
    "\n",
    "        ax[0].get_yaxis().set_visible(False)\n",
    "\n",
    "        colors = [color_lookup_names[x] for x in labels]\n",
    "\n",
    "        #Plot topic activity\n",
    "        topic_length.loc[labels].plot.barh(ax=ax[1],color=colors,edgecolor='grey',linewidth=0.1)\n",
    "\n",
    "        #Remove axis\n",
    "        ax[1].get_yaxis().set_visible(False)\n",
    "\n",
    "        #This gives the length of each branch at merge\n",
    "        lengths = [x[1] for x in dend['dcoord']]\n",
    "        \n",
    "        path_length = print(np.sum(lengths))\n",
    "        \n",
    "        return(path_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons\n",
    "\n",
    "Here we compare the topic mix of papers with different compositions\n",
    "\n",
    "We will focus on a specific country and year to rule out compositional effects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2015\n",
    "\n",
    "m = 'cosine'\n",
    "\n",
    "country = 'United States'\n",
    "\n",
    "data_temp = data_2.dropna(axis=0,subset=['country_list'])\n",
    "\n",
    "data_3 = data_temp.loc[(data_temp['year']>y)&([country in c for c in data_temp['country_list']])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare papers with and without women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With and without women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_fem = [tree_diversity(d,topics,m) for d in [data_3,\n",
    "                                                       data_3.loc[data_3['has_female']==False]]]\n",
    "\n",
    "div_fem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare corporate and non-corporate papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_corp = [tree_diversity(d,topics,m) for d in [data_3,data_3.loc[data_3['company']==True]]]\n",
    "\n",
    "div_corp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare papers in different periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 'cosine'\n",
    "\n",
    "country = 'United States'\n",
    "\n",
    "data_temp = data_2.dropna(axis=0,subset=['country_list'])\n",
    "\n",
    "data_4 = data_temp.loc[[country in c for c in data_temp['country_list']]]\n",
    "\n",
    "y_thres=2017\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_year = [tree_diversity(d,topics,m) for d in [data_4.loc[data_4['year']<y_thres],data_4.loc[data_4['year']>y_thres]]]\n",
    "\n",
    "div_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we remove various topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 'cosine'\n",
    "\n",
    "div_base = tree_diversity(data_3,topics,m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_top = pd.Series([div_base-tree_diversity(data_3,\n",
    "                          [x for x in topics if x not in [k for k,v in comms.items() if v==comm_ind]],\n",
    "                          m) for comm_ind,comm_name in comm_names.items()],index=comm_names.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_top.sort_values(ascending=False).plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
