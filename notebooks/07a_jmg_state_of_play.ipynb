{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of Play\n",
    "\n",
    "In this notebook we concentrate on research trends on AI: \n",
    "\n",
    "* How has the field evolved\n",
    "* Where has the field spread\n",
    "* How has the field been disrupted\n",
    "* What is the situation in different countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings (for when I concatenate dfs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig(name,path='../reports/figures/paper_rev/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_analysis(topic_mix,topics,year_var='year',year_lim = [2000,2019],thres=0.1):\n",
    "    '''\n",
    "    Takes a df and analyses topics trends\n",
    "    \n",
    "    Args:\n",
    "        -The topic mix where the rows are papers and the columns are topics\n",
    "        -The topics to visualise\n",
    "        -The year variable to consider\n",
    "        -Threshold for topic occurrence.\n",
    "        -comms = community lookup (or false, if we are not using communities)\n",
    "    \n",
    "    Returns:\n",
    "        -A table with levels of activity per topic and year\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #Topic count per year\n",
    "    \n",
    "    topic_count = pd.concat([pd.crosstab(topic_mix[year_var],topic_mix[t]>thres)[True] for t in topics],axis=1).fillna(0)\n",
    "    topic_count.columns = topics\n",
    "    \n",
    "\n",
    "        #Count papers per topic\n",
    "        #topic_count = pd.concat([pd.crosstab(topic_mix[year_var],topic_mix[t]>0)[True] for t in topics],axis=1).fillna(0)\n",
    "        \n",
    "        #Add columns\n",
    "        \n",
    "        \n",
    "    #Normalise years\n",
    "    topic_count = topic_count.loc[np.arange(year_lim[0],year_lim[1])].fillna(0)\n",
    "        \n",
    "    return(topic_count)\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_trend_of_interest(trend_df,topics,ax,wind=3,norm=False,**kwargs):\n",
    "    '''\n",
    "    Plots a trend of interest.\n",
    "    \n",
    "    Args: \n",
    "        trend_df: the df where rows = years and column = topics\n",
    "        topic: topic or topics of interest\n",
    "        wind: rolling mean normalisation\n",
    "        norm: if 2 = normalise for year (importance of a topic in the period) if 1 = normalise for topic (share of year activity in the topic). If False = don't normalise\n",
    "        \n",
    "    Returns the plot\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #Normalise or not?\n",
    "    \n",
    "    if norm==False:\n",
    "        trend_df[topics].rolling(window=wind).mean().dropna().plot(ax=ax,**kwargs)\n",
    "        \n",
    "    else:\n",
    "        trend_norm = trend_df.apply(lambda x: x/x.sum(),norm-1).fillna(0)\n",
    "        \n",
    "        #print(trend_norm)\n",
    "    \n",
    "        trend_norm[topics].rolling(window=wind).mean().dropna().plot(ax=ax,**kwargs)\n",
    "    \n",
    "\n",
    "def trend_comparison(topic_mix,topics,var,ax,year_var='year',year_lim = [2000,2019],thres=0,norm=2):\n",
    "    '''\n",
    "    Compares two groups in a trend of interest\n",
    "    \n",
    "    Args:\n",
    "        -topic_mix = topic mix\n",
    "        -topics: topics of interest\n",
    "        -var: variable we want to compare\n",
    "        -ax will generaly be a matplotlib axis with two rows \n",
    "        -The year variable to consider\n",
    "        -Threshold for topic occurrence.\n",
    "        -comms = community lookup (or false, if we are not using communities)\n",
    "    \n",
    "    Returns the plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    outputs = [trend_analysis(topic_mix.loc[topic_mix[var]==val],topics) for val in [False,True]]\n",
    "    \n",
    "    for n,out in enumerate(topics):\n",
    "        \n",
    "        #print(out)\n",
    "        plot_trend_of_interest(out,topics,norm=norm,ax=ax[n])\n",
    "    \n",
    "def make_network_from_doc_term_matrix(mat,threshold,id_var):\n",
    "    '''\n",
    "    Create a network from a document term matrix.\n",
    "    \n",
    "    Args\n",
    "        Document term matrix where the rows are documents and the columns are topics\n",
    "        threshold is the threshold to consider that a topic is present in a matrix.\n",
    "        \n",
    "    Returns: \n",
    "        A network\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Melt the topic mix and remove empty entries\n",
    "    cd = pd.melt(mat.reset_index(drop=False),id_vars=[id_var])\n",
    "\n",
    "    cd = cd.loc[cd['value']>threshold]\n",
    "\n",
    "    #This gives us the topic co-occurrence matrix\n",
    "    co_occurrence = cd.groupby(id_var)['variable'].apply(lambda x: list(x))\n",
    "    \n",
    "    #Here the idea is to create a proximity matrix based on co-occurrences\n",
    "\n",
    "    #Turn co-occurrences into combinations of pairs we can use to construct a similarity matrix\n",
    "    sector_combs = flatten_list([sorted(list(combinations(x,2))) for x in co_occurrence])\n",
    "    sector_combs = [x for x in sector_combs if len(x)>0]\n",
    "\n",
    "    #Turn the sector combs into an edgelist\n",
    "    edge_list = pd.DataFrame(sector_combs,columns=['source','target'])\n",
    "\n",
    "    edge_list['weight']=1\n",
    "\n",
    "    #Group over edge pairs to aggregate weights\n",
    "    edge_list_weighted = edge_list.groupby(['source','target'])['weight'].sum().reset_index(drop=False)\n",
    "\n",
    "    edge_list_weighted.sort_values('weight',ascending=False).head(n=10)\n",
    "    \n",
    "    #Create network and extract communities\n",
    "    net = nx.from_pandas_edgelist(edge_list_weighted,edge_attr=True)\n",
    "    \n",
    "    return(net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_highlight_plot(trends,vars_interest,ax,cmap,alpha=0.3,lab_map=False):\n",
    "    '''\n",
    "    Creates a df where we select the topics to focus on\n",
    "    \n",
    "    \n",
    "    Args:\n",
    "        Trend is a trend df\n",
    "        vars_interest are the topics or variables we eanrt to focus on\n",
    "        ax the axis\n",
    "        cmap is the color map we want to use\n",
    "        lab_map is the tidy label map we use\n",
    "    \n",
    "    Returns a plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create a lookup with numbers for values\n",
    "    topic_lookup = {name:val for val,name in enumerate(vars_interest)}\n",
    "\n",
    "    #Color map\n",
    "    cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "    #Create a vector of colors\n",
    "    cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in trends.columns]\n",
    "    lw = [1 if v not in topic_lookup.keys() else 3 for v in trends.columns]\n",
    "    \n",
    "    #Plot\n",
    "    (100*trends.rolling(window=4).mean()).dropna().plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "    #Fix the legend to focus on key topics\n",
    "    hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "    ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in vars_interest],\n",
    "              #labels=map(lambda z: lab_map[z], [x in [x[1][:50] for x in zip(hand,labs) if x[1] in vars_interest]])\n",
    "              labels=map(lambda x: lab_map[x],[x[1][:50] for x in zip(hand,labs) if x[1] in vars_interest]) if lab_map!=False\n",
    "              else [x[1][:50] for x in zip(hand,labs) if x[1] in vars_interest]\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import seaborn as sns\n",
    "\n",
    "def make_tidy_lookup(names_list,length=False):\n",
    "    '''\n",
    "    \n",
    "    Creates a cheap lookup between names, removing underscores and capitalising\n",
    "    \n",
    "    Args:\n",
    "        names_list (list) is the list of names we want to tidy\n",
    "        length is if we want to only keep a certain length of the name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = {x:re.sub('_',' ',x).capitalize() for x in names_list}\n",
    "    return(out)\n",
    "\n",
    "def show_network(ax,net,top_edge_share,label,loc,\n",
    "                 size_lookup,\n",
    "                 color_lookup,norm=2000,norm_2=1.2,layout=nx.kamada_kawai_layout,ec='white',alpha=0.6):\n",
    "    '''\n",
    "    Plots a network visualisation of the topic network.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    new_net = net.copy()\n",
    "    \n",
    "    #Get the weights\n",
    "    net_weight = sorted(new_net.edges(data=True),key=lambda x: x[2]['weight'],reverse=True)\n",
    "\n",
    "    #Select how many top edges do we want to visualise\n",
    "    length = int(top_edge_share*len(net_weight))\n",
    "    \n",
    "    #Select the top edges\n",
    "    top_edges = net_weight[:length]\n",
    "\n",
    "    #Create a network with them\n",
    "    new_net_2 = nx.Graph(top_edges)\n",
    "    \n",
    "    #Calculate the layout\n",
    "    pos = layout(new_net_2,\n",
    "                 #weight='weight',\n",
    "                 center=(0.5,0.5)\n",
    "                )\n",
    "    \n",
    "    #Draw the network. There is quite a lot of complexity here\n",
    "    nx.draw_networkx_nodes(new_net_2,pos,\n",
    "                       node_size=list([size_lookup[x]**norm_2 for x in dict(new_net_2.degree).keys()]),\n",
    "                       node_color = [color_lookup[comm_names[comms[x]]] if comm_names[comms[x]] in color_lookup.keys() else 'white' for x in dict(new_net_2.nodes).keys()],\n",
    "                       cmap='tab20c',\n",
    "                       alpha=0.7,edgecolors='darkgrey',ax=ax)\n",
    "\n",
    "    nx.draw_networkx_edges(new_net_2,pos,width=[e[2]['weight']/norm for e in new_net_2.edges(data=True)],edge_color=ec,ax=ax,alpha=alpha)\n",
    "\n",
    "def make_time_net(ax,dataset,size_lookup,my_label,ec='darkgrey',alpha=0.8):\n",
    "    '''\n",
    "    Function to visualise a network\n",
    "    \n",
    "    Args:\n",
    "        dataset (df) is the topic co-occurrence matrix we want to use to create the network\n",
    "        label (str) is the title for the network\n",
    "        size_lookup (dict) is the size (level of activity) for each topic\n",
    "        fig_save (str) is the name we use to save the figure\n",
    "        \n",
    "    '''\n",
    "    #Extract the network based on the input co-occurrence matrix\n",
    "    \n",
    "    top_net_old= make_network_from_doc_term_matrix(dataset,0.025,'paper_id')\n",
    "\n",
    "    #Show the network\n",
    "    show_network(ax,top_net_old,0.02,label=my_label,norm=500,norm_2=0.9,\n",
    "                 color_lookup=color_lookup,size_lookup=size_lookup,\n",
    "                 layout=nx.kamada_kawai_layout,loc=(-0.29,1.1),ec=ec,\n",
    "                 alpha=alpha)\n",
    "    \n",
    "    #Remove ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ax.set_title(my_label,size=18)\n",
    "\n",
    "def plot_centrality(network,measure,cl,ax,plot_name):\n",
    "    '''\n",
    "    This is to plot the centrality of different topics inside the topic network.\n",
    "    \n",
    "    Args:\n",
    "        -network is the network whose centralities we want to plot\n",
    "        -measure is the measure we want to plot\n",
    "        -colour lookup is to colour the bars in the network\n",
    "        -ax is the axis\n",
    "    \n",
    "    Returns a plot of the distributions of centrality\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Calculate the centrality measure and normalise it\n",
    "    c = pd.Series(measure(network,weight='weight'))\n",
    "    \n",
    "    #Normalise the centrality\n",
    "    c_norm =  pd.Series(zscore(c),index=c.index)\n",
    "    \n",
    "    #Sort by centralities\n",
    "    c_sorted = c_norm.sort_values(ascending=False)\n",
    "    \n",
    "    #Add colors based on the colour lookup\n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in c_sorted.index]\n",
    "    \n",
    "    #Plot\n",
    "    c_sorted.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    #Some final changes in the plot\n",
    "    ax.legend(handles=patches,ncol=3)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_ylabel('Normalised centrality')\n",
    "\n",
    "def make_disruption_tables(df,period=np.arange(2000,2019)):\n",
    "    '''\n",
    "    This function creates two datasets capturing inter-year changes in activity, which we consider a proxy for 'disruption'\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is a dataframe with the topics\n",
    "        period is the period we are interested in capturing\n",
    "        \n",
    "    Will return a df and a table of mean changes ready for visualisation\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #We want to measure distances between activity profiles in years\n",
    "\n",
    "    #We create a vector with counts of papers with activity in a year\n",
    "    year_topics = pd.concat([(df.loc[df['year']==y,topics]>0.05).sum() for y in period],axis=1)\n",
    "\n",
    "    year_topics.columns = period\n",
    "\n",
    "    #We normalise the results (we want to consider the relative importance of topics, not absolute)\n",
    "    topics_years_norm = year_topics.T.apply(lambda x: zscore(x)).dropna(axis=1)\n",
    "    \n",
    "    #We calculate distances between years\n",
    "    year_sims = pd.DataFrame(1-pairwise_distances(topics_years_norm,metric='cosine'),index=period,columns=period)\n",
    "\n",
    "    \n",
    "    #We also calculate rolling intra-year distances. We focus on the diagonal for visualisation\n",
    "    mean_sims = pd.Series(np.diag(np.matrix(year_sims.rolling(window=3).mean())))\n",
    "    mean_sims.index = period\n",
    "    \n",
    "    return([year_sims,mean_sims])\n",
    "\n",
    "def make_disruption_plot(disr_inputs,ax):\n",
    "    '''\n",
    "    This function creates a disruption plot\n",
    "    \n",
    "    Arguments:\n",
    "        -disr_inputs (list) is the output of the make_disruption_tables\n",
    "    \n",
    "    '''\n",
    "    #Get the period\n",
    "    period = disr_inputs[0].columns\n",
    "    \n",
    "    #This is to select the lower triangular matrix for the visualisation\n",
    "    year_sims_2 =  pd.DataFrame(np.tril(disr_inputs[0], \n",
    "                                        k=0),index=period,columns=period).applymap(\n",
    "        lambda x: np.nan if x==0 else x) #We make the zeroes nans to colour things later\n",
    "    \n",
    "    \n",
    "    #Get the colourmap\n",
    "\n",
    "    my_map = plt.cm.get_cmap('seismic')\n",
    "    \n",
    "    #Set missing values to white\n",
    "    my_map.set_bad('white')\n",
    "\n",
    "\n",
    "    #fig,ax = plt.subplots(figsize=(10,8),nrows=2,gridspec_kw={'height_ratios':[3,1.2]})\n",
    "    \n",
    "    #Create the heatmap\n",
    "    im = ax[0].imshow(year_sims_2,cmap=my_map,aspect='auto')\n",
    "    \n",
    "    \n",
    "    #Some formatting of labels etc\n",
    "    #You always have to add the ticks and the ticklabels when doing imshow\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_xticklabels([])\n",
    "    ax[0].set_yticks(np.arange(0,len(period)))\n",
    "    ax[0].set_yticklabels(period)\n",
    "    \n",
    "\n",
    "    #ax[0].set_title('Year on year topic similarity',size=14)\n",
    "\n",
    "    #We remove the top and right-and lines of the frame\n",
    "    ax[0].spines['top'].set_edgecolor('white')\n",
    "    ax[0].spines['right'].set_edgecolor('white')\n",
    "    \n",
    "    #Plot the mean 'disruption'\n",
    "    \n",
    "    disr_inputs[1].plot(ax=ax[1])\n",
    "    \n",
    "    #Add a y label \n",
    "    ax[1].set_ylabel('Year-on-year \\n similarity \\n (rolling mean)')\n",
    "    \n",
    "    #Fix the x axis\n",
    "    ax[1].set_xticks(np.arange(1999,2019))\n",
    "    ax[1].set_xticklabels(np.arange(2000,2019))\n",
    "\n",
    "    #Remove the top line\n",
    "    ax[1].spines['top'].set_edgecolor('white')\n",
    "    \n",
    "    #Add a vertical grid to connect both charts\n",
    "    ax[1].grid(which='both', axis='x', linestyle='--')\n",
    "\n",
    "    #And finally the colour bar\n",
    "    #The list there describes the position of the new axis\n",
    "    cbaxes = fig.add_axes([0.55, 0.8, 0.3, 0.02]) \n",
    "    \n",
    "    #We draw the axis\n",
    "    cb = plt.colorbar(im, cax = cbaxes,orientation='horizontal')  \n",
    "    \n",
    "    #And name it\n",
    "    cb.ax.set_title('Inter-year cosine similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_shares(reps,drop_china=False):\n",
    "    '''\n",
    "    Returns a df with mean changes in representation per country.\n",
    "    \n",
    "    Args:\n",
    "        Reps: dfs with shares of a country in total activity in different years\n",
    "        drop_china whether we want to drop China from the analysis\n",
    "    \n",
    "    We can use this to calculate means, variances, etc.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    mean_change= []\n",
    "\n",
    "    for l2 in all_reps:\n",
    "        \n",
    "        l = l2.copy()\n",
    "        \n",
    "        if drop_china==True:\n",
    "            l.drop('China',axis=0,inplace=True)\n",
    "    \n",
    "        l['change'] = 1-(l.iloc[:,1]/l.iloc[:,0])\n",
    "\n",
    "        #print(l)\n",
    "\n",
    "        mean_change.append(l['change'])\n",
    "        \n",
    "    return(pd.DataFrame(mean_change,index=['All arXiv','All AI','SoTA AI']).T)\n",
    "    #return(mean_change)\n",
    "\n",
    "def share_all(df,countries,years,n):\n",
    "    '''\n",
    "    This function compares shares of activity before and after a year for all papers\n",
    "    \n",
    "    We use it instead of the one above because the data is in a different format\n",
    "    \n",
    "    Args:\n",
    "        df (df) with activity and country information\n",
    "        countries (list) countries we want to focus on\n",
    "        years (list) of threshold years\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #AI activity\n",
    "    p_1 = df.loc[df['year']<years[0]]\n",
    "    p_2 = df.loc[df['year']>years[1]]\n",
    "    \n",
    "    #All share\n",
    "    act_share = pd.concat([p['institute_country'].value_counts(normalize=True) for p in [p_1,p_2]],axis=1)\n",
    "    \n",
    "    act_share.columns = [f'{n} before {years[0]}',f' {n} after {years[1]}']\n",
    "    \n",
    "    return(act_share.loc[countries])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_freq(nested_list):\n",
    "    '''\n",
    "    \n",
    "    Function to calculate frequencies of elements within a nested list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return(pd.Series(flatten_list(nested_list))).value_counts()\n",
    "\n",
    "def share_comp(df,countries,years,n):\n",
    "    '''\n",
    "    Function to compare shares of activity before and after a year\n",
    "    \n",
    "    Arguments:\n",
    "        df (df) is the usual dataframe\n",
    "        countries (list) is the list of top countries\n",
    "        years (list) are the max year for period 1 and the min period for year 2\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Extract both periods\n",
    "    p_1 = df.loc[df['year']<years[0]]\n",
    "    p_2 = df.loc[df['year']>years[1]]\n",
    "    \n",
    "    #Extract share of papers by country\n",
    "    ai_share = pd.concat([pd.DataFrame([pd.Series(len(p.loc[[c in c_list for c_list in p['country_list']]])/len(p),name=c) for\n",
    "                c in countries]) for p in [p_1,p_2]],axis=1)\n",
    "    \n",
    "    ai_share.columns = [f'{n} before {years[0]}',f' {n} after {years[1]}']\n",
    "    \n",
    "    return(ai_share) \n",
    "\n",
    "\n",
    "def make_comp_plot_2(comp_list,ax):\n",
    "    '''\n",
    "    Creates a comparison plot between variables\n",
    "    \n",
    "    Args:\n",
    "        list is a list of three objects with research activities in different times\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    s=70\n",
    "    \n",
    "    colors = [['mistyrose','lightblue'],['salmon','cornflowerblue'],['red','blue']]\n",
    "    \n",
    "    for offs,element,color in zip([-0.2,0,0.2],comp_list,colors):\n",
    "        \n",
    "        direction = [element.iloc[n,1]-element.iloc[n,0] for n in np.arange(0,len(element))]\n",
    "        \n",
    "        ax.scatter([x+offs for x in np.arange(0,len(element))],100*element.iloc[:,0],color=color[0],marker='o',\n",
    "                   edgecolors='darkgrey',\n",
    "                   s=s)\n",
    "       \n",
    "\n",
    "        for n,c in enumerate(element.index):\n",
    "            ax.scatter(n+offs,100*element.iloc[n,1],\n",
    "                       color=color[1],\n",
    "                       marker='^' if direction[n]>0 else 'v',\n",
    "                       edgecolors='darkgrey',\n",
    "                       s=s+20)\n",
    "            \n",
    "            ax.vlines(x=n+offs,ymin=100*element.loc[c].min(),ymax=100*element.loc[c].max(),\n",
    "                      color=color[1] if element.loc[c][1]>element.loc[c][0] else color[0],\n",
    "                      #linestyle=':',\n",
    "                      linewidth=1)\n",
    "\n",
    "        \n",
    "    ax.set_xticks(np.arange(0,len(comp_list[0])))\n",
    "    ax.set_xticklabels(comp_list[0].index,rotation=90)\n",
    "    \n",
    "    \n",
    "    for n in np.arange(0,len(comp_list[0])):\n",
    "        ax.vlines(x=n-0.5,ymin=0,\n",
    "                  ymax=40,color='darkgrey',linestyle=':')\n",
    "        \n",
    "        ax.vlines(x=n+0.5,ymin=0,\n",
    "                  ymax=40,color='darkgrey',linestyle=':')\n",
    "\n",
    "                       \n",
    "                       \n",
    "                       \n",
    "                       \n",
    "    \n",
    "    ax.set_ylabel('% of all activity with presence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "`analysis_pack` contains the metadata and data that we serialised at the end of the `06` data integration notebook.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Community names for the communities (`index->community name`)\n",
    "* Community indices for topics (`topic -> community index`)\n",
    "* Filtered topic names (`topic names`)\n",
    "* Network object with topic co-occurrences\n",
    "* Analysis df\n",
    "* arx is the enriched arXiv dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/24_8_2019_analysis_pack.p','rb') as infile:\n",
    "    analysis_pack = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = analysis_pack[0]\n",
    "comms = analysis_pack[1]\n",
    "topics = analysis_pack[2]\n",
    "network = analysis_pack[3]\n",
    "data = analysis_pack[4]\n",
    "arx = analysis_pack[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arx['has_female'].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We load this to consider overall research trends\n",
    "arx_geo = pd.read_csv('../data/external/17_8_2019_papers_institution_ucl_cleaned.csv',compression='zip',dtype={'article_id':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx['year'] = [int(x) for x in arx['year']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Total activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1_data = (100*pd.crosstab(arx['year'],arx['is_ai'],normalize=1))\n",
    "\n",
    "ax = fig_1_data.plot(figsize=(8,5),linewidth=3)\n",
    "\n",
    "ax.legend(title='AI paper',labels=['Not AI','AI'])\n",
    "\n",
    "ax.set_ylabel('% of all papers in year')\n",
    "#ax.set_title('AI research trends (overall)')\n",
    "\n",
    "save_fig('fig_1_trends_overall.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative research trends\n",
    "\n",
    "fig_1_data.iloc[::-1].cumsum()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three quarters of the AI papers in the data have been written in the last 5 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Activity by field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the field names\n",
    "field_names = ['field_astrophysics',\n",
    " 'field_biological',\n",
    " 'field_complex_systems',\n",
    " 'field_informatics',\n",
    " 'field_machine_learning_data',\n",
    " 'field_materials_quantum',\n",
    " 'field_mathematical_physics',\n",
    " 'field_mathematics_1',\n",
    " 'field_mathematics_2',\n",
    " 'field_optimisation',\n",
    " 'field_particle_physics',\n",
    " 'field_physics_education',\n",
    " 'field_societal',\n",
    " 'field_statistics_probability']\n",
    "\n",
    "#Create tidy field names for legend etc\n",
    "tidy_field_lookup = {x:re.sub('_',' ',x[6:]).capitalize() for x in field_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will not plot maths as we have two categories\n",
    "fields_to_plot = [x for x in field_names if not any(num in x for num in ['1','2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI in fields\n",
    "ai_in_fields = pd.concat([pd.crosstab(arx.loc[arx[t]>0.5]['year'],\n",
    "                                     arx.loc[arx[t]>0.5]['is_ai'],normalize=0)[1] for t in fields_to_plot],axis=1).fillna(0)\n",
    "\n",
    "ai_in_fields.columns = fields_to_plot\n",
    "\n",
    "#Sort top fields (for the legend)\n",
    "top_ai_fields = ai_in_fields.loc[2018].sort_values().index[::-1][:9]\n",
    "\n",
    "top_ai_fields_all = ai_in_fields.loc[2018].sort_values().index[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AI in fields share of activity\n",
    "\n",
    "fig,ax = plt.subplots(nrows=6,ncols=2,figsize=(6,10),\n",
    "                      sharey='row',\n",
    "                      sharex=True)\n",
    "\n",
    "row=0\n",
    "col=0\n",
    "\n",
    "for n,f in enumerate(top_ai_fields_all):\n",
    "    \n",
    "    \n",
    "    \n",
    "    rel = arx.loc[arx[f]>0.5]\n",
    "\n",
    "    year_act = pd.crosstab(rel['year'],rel['is_ai'],normalize=1).loc[np.arange(2000,2019)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    (100*year_act[0].rolling(window=3).mean()).dropna().plot(ax=ax[row,col],legend=False,linewidth=1.5,linestyle='--')\n",
    "    (100*year_act[1].rolling(window=3).mean()).dropna().plot(ax=ax[row,col],legend=False,linewidth=3)\n",
    "    \n",
    "    ax[row,col].set_title(tidy_field_lookup[f])\n",
    "    \n",
    "    if n % 2==0:\n",
    "        \n",
    "        row=row\n",
    "        col=1\n",
    "    \n",
    "    else:\n",
    "        row=row+1\n",
    "        col=0\n",
    "        \n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax[0,0].legend(labels=['Not AI','AI'])\n",
    "\n",
    "save_fig('fig_2_trend_fields.pdf')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*pd.crosstab(arx['is_ai'],arx['top_field'],normalize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = (100*ai_in_fields.loc[np.arange(2000,2019),top_ai_fields].rolling(window=3).mean()).dropna().plot(figsize=(10,6),cmap='tab10',linewidth=3)\n",
    "\n",
    "# ax.legend(bbox_to_anchor=(1,1),title='Scientific field',labels=list(map(lambda x: tidy_field_lookup[x],top_ai_fields)))\n",
    "\n",
    "# ax.set_title('AI intensity by scientific field')\n",
    "\n",
    "# ax.set_ylabel('AI as % of papers in topic')\n",
    "# ax.set_xlabel('')\n",
    "\n",
    "# save_fig('fig_2_trends_field.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in ['field_astrophysics','field_biological','field_complex_systems','field_materials_quantum','field_societal']:\n",
    "    \n",
    "#     print(x)\n",
    "#     print('====')\n",
    "    \n",
    "#     d = arx.loc[(arx['is_ai']==True) & (arx[x]>0.75)].reset_index(drop=True)\n",
    "    \n",
    "#     get_example(d,5,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iii. Activity by topic community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create community names.\n",
    "# We remove 'mixed' since this is an excluded community\n",
    "community_names = list(set(comm_names.values()))\n",
    "\n",
    "community_names.remove('mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot some of the topics\n",
    "topics_for_plot = ['symbolic','statistics',\n",
    "                   'deep_learning','computer_vision','robotics_agents','language']\n",
    "\n",
    "tidy_comm_names = make_tidy_lookup(community_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to normalise the years\n",
    "comm_trends = trend_analysis(data,community_names,thres=0.05)\n",
    "all_years = data['year'].value_counts()\n",
    "comm_norm = comm_trends.apply(lambda x: x/all_years).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the communities for the legend\n",
    "sorted_comms = [x for x in comm_trends.loc[2018].sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "make_highlight_plot(comm_norm[sorted_comms],\n",
    "                    topics_for_plot,cmap='Dark2_r',ax=ax,alpha=0.15,lab_map=tidy_comm_names)\n",
    "\n",
    "#ax.set_title('Evolution of activity by topic_community')\n",
    "\n",
    "ax.set_ylabel('% of all AI papers with topic presence')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_3_topic_community.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### v. Activity by detailed topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_topics = [\n",
    "    'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "    'cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "    'training-trained-deep_learning-deep-train',\n",
    "    'generator-gan-discriminator-generative_adversarial_networks_gans-gans',\n",
    "    'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "    'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_trends = trend_analysis(data,topics,thres=0.05)\n",
    "all_years = data['year'].value_counts()\n",
    "topic_trends_norm = topic_trends.apply(lambda x: x/all_years).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tedious sorting of topics\n",
    "sorted_topics = topic_trends_norm.rolling(window=3).mean().loc[2018].sort_values(ascending=False).index\n",
    "notable_sorted = [x for x in sorted_topics if x in notable_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(14,6))\n",
    "\n",
    "make_highlight_plot(topic_trends_norm.loc[np.arange(2005,2019),sorted_topics],notable_sorted,cmap='Dark2',ax=ax,alpha=0.1,lab_map=False)\n",
    "\n",
    "#ax.set_title('Evolutio of activity by detailed topic')\n",
    "\n",
    "ax.set_ylabel('Share of AI papers with topic')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_4_trending_topics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Networks\n",
    "\n",
    "We will combine a plot of network structure and centrality\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to make the size of the nodes comparable between years\n",
    "size_lookup = pd.concat([(data.loc[[x in year_set for x in data['year']]][topics]>0.05).sum() for \n",
    "                         year_set in [\n",
    "                             set(np.arange(1990,2019)),\n",
    "                             set(np.arange(1990,2012)),\n",
    "                             set(np.arange(2012,2015)),\n",
    "                             set(np.arange(2015,2019))]],axis=1)\n",
    "\n",
    "size_lookup.columns = ['all','pre','mid','late']\n",
    "\n",
    "size_lookup_dict = size_lookup.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comm_names[l],edgecolor='black') for l,c in color_lookup.items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "#Show the network\n",
    "show_network(ax,network,0.05,norm=200,norm_2=0.9,color_lookup=color_lookup,size_lookup=size_lookup['all'],\n",
    "             layout=nx.kamada_kawai_layout,label='All years',loc=(-0.5,1.48),ec='black',alpha=0.7)\n",
    "\n",
    "#Draw the legend\n",
    "ax.legend(handles=patches,facecolor='white',loc='upper right',title='Area')\n",
    "\n",
    "#Remove ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_5_network_all_years.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color_lookup_2 = {\n",
    "#     'deep_learning':'blue',\n",
    "#     #'robotics_agents':'cornflowerblue',\n",
    "#     'computer_vision':'aqua',\n",
    "#     'symbolic':'red',\n",
    "#     'statistics':'orange',\n",
    "#     #'language':'yellow'\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comm_names[l],edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the integrated network - centrality plot\n",
    "fig,ax = plt.subplots(figsize=(14,8),nrows=2,ncols=2,gridspec_kw={'width_ratios':[1.5,2]})\n",
    "\n",
    "#Subset the old period\n",
    "old_period = data.loc[data['year']<2011][topics]\n",
    "\n",
    "#Make the network\n",
    "make_time_net(ax[0][0],old_period,size_lookup['pre'],my_label='Before 2012')\n",
    "\n",
    "#Plot the centrality\n",
    "plot_centrality(make_network_from_doc_term_matrix(old_period,0.025,'paper_id'),\n",
    "                nx.eigenvector_centrality,cl=color_lookup,ax=ax[0][1],plot_name='Before 2012')\n",
    "\n",
    "#Same as above but with the more modern data\n",
    "late_period = data.loc[(data['year']>2015)][topics]\n",
    "\n",
    "#Make networks\n",
    "make_time_net(ax[1][0],late_period,size_lookup['late'],my_label='After 2015')\n",
    "\n",
    "#Plot centrality\n",
    "plot_centrality(\n",
    "    make_network_from_doc_term_matrix(late_period,0.025,'paper_id'),nx.eigenvector_centrality,cl=color_lookup,ax=ax[1][1],plot_name='After 2015')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#save_fig('fig_6_network_comp.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create the integrated network - centrality plot\n",
    "# fig,ax = plt.subplots(figsize=(14,6),nrows=2,ncols=2,gridspec_kw={'width_ratios':[1.5,2]})\n",
    "\n",
    "# #Subset the old period\n",
    "# old_period = data.loc[data['year']<2011][topics]\n",
    "\n",
    "# #Make the network\n",
    "# make_time_net(ax[0][0],old_period,size_lookup['pre'],my_label='Before 2012')\n",
    "\n",
    "# #Plot the centrality\n",
    "# plot_centrality(make_network_from_doc_term_matrix(old_period,0.025,'paper_id'),\n",
    "#                 nx.eigenvector_centrality,cl=color_lookup,ax=ax[0][1],plot_name='Before 2012')\n",
    "\n",
    "# #Same as above but with the more modern data\n",
    "# late_period = data.loc[(data['year']>2015)][topics]\n",
    "\n",
    "# #Make networks\n",
    "# make_time_net(ax[1][0],late_period,size_lookup['late'],my_label='After 2015')\n",
    "\n",
    "# #Plot centrality\n",
    "# plot_centrality(\n",
    "#     make_network_from_doc_term_matrix(late_period,0.025,'paper_id'),nx.eigenvector_centrality,cl=color_lookup,ax=ax[1][1],plot_name='After 2015')\n",
    "\n",
    "# plt.tight_layout()\n",
    "\n",
    "# #save_fig('fig_6_network_comp.png')\n",
    "\n",
    "# save_fig('neurips_network_comp.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Thematic disruption\n",
    "\n",
    "Our final descriptive analysis considers disruption over time: what have been the changes in the composition of AI since the 2000s?\n",
    "\n",
    "We create a matrix that compares the topic vector for every year (a normalised sum) across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the function that we defined above\n",
    "disr = make_disruption_tables(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We plot the results, which show quite starkly the disruption in AI research before and after 2012.\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(10,8),nrows=2,gridspec_kw={'height_ratios':[3,1.2]})\n",
    "\n",
    "make_disruption_plot(disr,ax=ax)\n",
    "\n",
    "plt.subplots_adjust(hspace=0)\n",
    "\n",
    "plt.savefig(f'../reports/figures/paper_rev/{today_str}_fig_7_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we calculate the half life of similarity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spatial disruption\n",
    "\n",
    "Here we want to calculate disruption measures for the top 10 countries by activity\n",
    "\n",
    "* Changes in the share of the total by country and in notable topics\n",
    "\n",
    "* Measures of disruptions like above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We merge these two so we can look at geographical activity by year\n",
    "arx_geo_year = pd.merge(arx_geo,arx[['article_id','year']],left_on='article_id',right_on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on data with country information\n",
    "data_w_countries = data.dropna(axis=0,subset=['country_list'])\n",
    "\n",
    "#Focus on top countries - we ignore #1 because it is 'multinational'\n",
    "top_countries = list(flatten_freq(data_w_countries['country_list'])[1:11].index)\n",
    "\n",
    "data_w_countries_core = data_w_countries.loc[data_w_countries[notable_topics].apply(lambda x: any(x>0.05),axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate national representation in AI and its components\n",
    "national_rep = [share_comp(d,top_countries,[2012,2015],n=n) for d,n in zip([data_w_countries,data_w_countries_core],['All AI','SotA AI'])]\n",
    "\n",
    "#Calculate representation in all activity \n",
    "national_rep_all = share_all(arx_geo_year,top_countries,[2012,2015],n='All arXiv')\n",
    "\n",
    "#Combine them\n",
    "all_reps =  [national_rep_all]+national_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patches for the legend\n",
    "patches_2 = [mpatches.Patch(facecolor=c, label=l,edgecolor='black') for l,c in \n",
    "          zip(['All arXiv before 2012','All AI before 2012','All SotA before 2012',\n",
    "               'All arXiv after 2015','All AI after 2015','All SotA after 2015'],\n",
    "              ['mistyrose','salmon','red','lightblue','cornflowerblue','blue'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "make_comp_plot_2(all_reps,ax)\n",
    "\n",
    "ax.legend(handles=patches_2,ncol=2)\n",
    "\n",
    "save_fig('fig_8_geo_changes.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WHat is the variance in changes in representation (including China)\n",
    "changes = change_shares(all_reps)\n",
    "\n",
    "changes.var()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And excluding China?\n",
    "changes_nc = change_shares(all_reps,drop_china=True)\n",
    "\n",
    "changes_nc.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: the geography of AI research is changing faster than the geography of research overall, specially in State of the Art topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.18/0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reps[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
