{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organisational character\n",
    "\n",
    "In this notebook we concentrate on the link between types of organisations involved in research and the topics they focus on\n",
    "\n",
    "* Levels of company activity\n",
    "* Topical differences in focus between industry and research\n",
    "* Collaboration differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings (for when I concatenate dfs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statsmodels.api import OLS, Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic functions\n",
    "def save_fig(name,path='../reports/figures/paper_rev/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')\n",
    "    \n",
    "    # Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy_lookup(names_list,length=False):\n",
    "    '''\n",
    "    \n",
    "    Creates a cheap lookup between names, removing underscores and capitalising\n",
    "    \n",
    "    Args:\n",
    "        names_list (list) is the list of names we want to tidy\n",
    "        length is if we want to only keep a certain length of the name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = {x:re.sub('_',' ',x).capitalize() for x in names_list}\n",
    "    return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_comp(df,variable,topics,threshold):\n",
    "    '''\n",
    "    This function compares activity by topics between categories.\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe we are using (generally analysis_fin, with rows = papers and columns = variables and metadata)\n",
    "        variable is the variable we are using for the comparison\n",
    "        topics is the topics where we want to compare (generally the community names)\n",
    "        threshold is the threshold we want to use to determine if a paper is in a topic or not\n",
    "    \n",
    "    Returns a df with the shares of papers in each topic sorted by their distances\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the counts df.\n",
    "    \n",
    "    #We are extracting, for each topics, the % of papers with at least one female author when the topic is present, and when it isn't.\n",
    "    group_counts = pd.concat([pd.crosstab(df[variable],df[t]>threshold,normalize=1).loc[True,:] for t in topics],axis=1)\n",
    "    \n",
    "    #Name\n",
    "    group_counts.columns = topics\n",
    "    \n",
    "    #Transpose\n",
    "    group_counts = group_counts.T\n",
    "    \n",
    "    #Rename variables\n",
    "    group_counts.columns = [variable+f'_{value}' for value in ['false','true']]\n",
    "    \n",
    "    #Create a measure of difference\n",
    "    group_counts['difference'] = (group_counts.iloc[:,1]/group_counts.iloc[:,0])-1\n",
    "    \n",
    "    #Output\n",
    "    out = group_counts.sort_values('difference',ascending=False)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def topic_regression(df,target_list,exog,controls,model,binarise=False,standardise=True,cov='HC1'):\n",
    "    '''\n",
    "    \n",
    "    This function regresses topic weights (or their binarisation) on predictors.\n",
    "    \n",
    "    Arguments:\n",
    "        -Df with the variables\n",
    "        -target_list: target variables. This is a list we loop over. \n",
    "        -exog: exogenous variable\n",
    "        -controls\n",
    "        -model type. OLS? Logit? TODO fix the logit\n",
    "        -Binarise in case we are using logit. If not False, the value is the threshold \n",
    "            TODO when we binarise the highly detailed models, some of them become all zeros. This will work better\n",
    "            with the mopre aggregate topics\n",
    "        -Standardise if we standardise and log the topic weights\n",
    "    \n",
    "    Returns\n",
    "        -A list of statsmodels summaries\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Drop rows with missing values - sm doesn't like them\n",
    "    df_2 = df[target_list+exog+controls].dropna(axis=0)\n",
    "    \n",
    "    #Standardise targets?\n",
    "    if standardise==True:\n",
    "        df_2[target_list] = (np.log(df_2[target_list]+0.00000001)).apply(zscore).astype(float)\n",
    "    \n",
    "    #Binarise targets if we are doing a logit\n",
    "    if binarise!=False:\n",
    "        df_2[target_list] = df_2[target_list].applymap(lambda x: x>binarise).astype(float)\n",
    "    \n",
    "    \n",
    "    #Extract the exogenous and controls, add constant and cast as float\n",
    "    exog_controls = add_constant(df_2[exog+controls]).astype(float)\n",
    "    \n",
    "\n",
    "    #Container output\n",
    "    out = []\n",
    "    coeffs = []\n",
    "    \n",
    "    #One regression for each target\n",
    "    for t in list(target_list):\n",
    "        \n",
    "        #There we gp. \n",
    "        reg = model(endog=df_2[t],exog=exog_controls).fit(cov_type=cov,disp=0)\n",
    "        \n",
    "        out.append(reg.summary())\n",
    "        \n",
    "        #coeffs.append(reg)\n",
    "        if model == OLS:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.rsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','r_square']\n",
    "    \n",
    "        else:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.prsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','pr_square']\n",
    " \n",
    "    \n",
    "    return([out,reg_coeff.sort_values('coefficient',ascending=False)])\n",
    "        \n",
    "       \n",
    "\n",
    "def plot_regression_coefficients(df,var,cov='HC1',size=(8,6),ax=False,ncols=3):\n",
    "    '''\n",
    "    Plots regression coefficients.\n",
    "    \n",
    "    Arg:\n",
    "        variable we use as predictor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reg = topic_regression(df,topics,[var],controls,OLS,cov='HC1')\n",
    "    \n",
    "    if ax==False:\n",
    "        fig,ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plot_topic_bar(reg[1]['coefficient'],cl=color_lookup,ax=ax,ncols=ncols)\n",
    "\n",
    "    ax.set_title(f'Regression coefficient using {var} as predictor')\n",
    "\n",
    "def topic_comparison(df,target_list,exog,concept_lookup,quantiles=np.arange(0,1.1,0.2),thres=0):\n",
    "    '''\n",
    "    This function compares the distribution of activity in various topics depending on an exogenous variable of interest. \n",
    "    \n",
    "    Args:\n",
    "        Df with the topic mix and metadata\n",
    "        target_list are the topics to consider\n",
    "        exog is the variable to crosstab topics against\n",
    "        concept_lookup is a df with the median proximity of each topic to the concepts\n",
    "        quantiles is how we discretise the concept lookup (default value is quintiles)\n",
    "        thres: =limit for considering a topic as present\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Copy df\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Discretise the concept lookup\n",
    "    \n",
    "    conc_discr = concept_lookup.apply(lambda x: pd.qcut(x,q=quantiles,labels=False,duplicates='drop'))\n",
    "\n",
    "    \n",
    "    #Calculate levels of activity per topic based on the exog variable\n",
    "    \n",
    "    topic_distr = pd.concat([pd.crosstab(df_2[exog],df_2[t]>thres)[True] for t in target_list],axis=1).T\n",
    "    topic_distr.index = target_list\n",
    "    \n",
    "    \n",
    "    #Merge the count with the concept lookup\n",
    "    disc = pd.melt(pd.concat([topic_distr,conc_discr],axis=1).reset_index(drop=False),id_vars=['index']+list(conc_discr.columns))\n",
    "    \n",
    "    #This is the list where we store the results\n",
    "    store={}\n",
    "    \n",
    "    for c in concept_lookup.columns:\n",
    "        \n",
    "        out = pd.pivot_table(disc.groupby([c,'variable'])['value'].sum().reset_index(drop=False),index=c,columns='variable',values='value')\n",
    "        #out.apply(lambda x: x/x.sum()).plot.bar()\n",
    "        \n",
    "        store[c] = out\n",
    "                                      \n",
    "    #Output dfs with the comparisons\n",
    "    return(store)\n",
    "\n",
    "def plot_topic_bar(table,cl,ax,ncols):\n",
    "    '''\n",
    "    Simple function to plot topic bars which includes colours based on the topic-label lookup\n",
    "    \n",
    "    Args:\n",
    "        table has topics in the index and a value to plot in the columns\n",
    "        cl is the colour lookup between communities and topics\n",
    "        ax is the plotting axe\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in table.index]\n",
    "    \n",
    "    table.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=ncols)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    \n",
    "def calculate_entropy(df,categories,category):\n",
    "    '''\n",
    "    We calculate entropy inside a paper using a distribution over semantic variables (eg discipline, community or topic). These have to be normalised\n",
    "    \n",
    "    arguments:\n",
    "        df is the analysis df with relevant topics and metadata\n",
    "        categories are the topics we want to compare\n",
    "        \n",
    "    outputs\n",
    "        A df with entropy measures by paper\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #Normalise\n",
    "    norm = df[categories].apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    ent = pd.DataFrame((norm.apply(lambda x: entropy(x),axis=1)),columns=['entropy'])\n",
    "    \n",
    "    ent['cat']=category\n",
    "    \n",
    "    return(ent)\n",
    "\n",
    "def make_exog(df,value_container,value,make_dummy=True):\n",
    "    '''\n",
    "    This creates exogenous variables for modelling later.\n",
    "    \n",
    "    Argument:\n",
    "        -df contains the variable where we want to find a value\n",
    "        -variable_container is the column where we want to look for the value\n",
    "        -value is the value we are looking for\n",
    "        -make_dummy: if true it just counts if the value is present. If false, it counts how many times it happens. \n",
    "        \n",
    "    Output\n",
    "        -A df with the new column (named)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Create a tidy variable name\n",
    "    column_name = re.sub(' ','_',value.lower())\n",
    "    \n",
    "    #If we want to create a dummy...\n",
    "    if make_dummy == True:\n",
    "        \n",
    "        #We just look for it in the value container\n",
    "        #There are some missing values so we have some control flow to manage that. \n",
    "        df_2[column_name] = [value in x if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Otherwise, we count how many times it occurs\n",
    "        #We deal with missing values ('non lists') as before\n",
    "        df_2[column_name] = [x.count(value) if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "        \n",
    "    return(df_2)\n",
    "    \n",
    "\n",
    "def extract_topic_trend(df,cat,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    Extracts evolution of a share of a category in a topic of interest\n",
    "    \n",
    "    Args:\n",
    "        df: the usual dataframe\n",
    "        cat: the category we are interested in\n",
    "        year_lims: first and last year to consider\n",
    "\n",
    "    '''\n",
    "    #rel_df = df.loc[df[cat]==True]\n",
    "    \n",
    "    out = pd.crosstab(df['year'],df[cat],normalize=0)\n",
    "    \n",
    "    return(out.loc[np.arange(year_lims[0],year_lims[1])])\n",
    "\n",
    "def plot_topic_trend(df,cat,topics,ax,cmap,year_lims=[2000,2019],threshold=0.05,focus_topics=False,alpha=0.2):\n",
    "    '''\n",
    "    Plots topic trends (shares of a category in a topic)\n",
    "    \n",
    "    Args:\n",
    "        df the usual dataframe\n",
    "        topics: topics we want to display\n",
    "        cat: the category of interest\n",
    "        year_lims: first and last year to consider\n",
    "    \n",
    "    '''\n",
    "    activity = []\n",
    "    names = []\n",
    "    \n",
    "    #Use a loop to deal with cases where a category has no activity in a topic\n",
    "    for t in topics:\n",
    "        try:\n",
    "            levels = extract_topic_trend(df.loc[df[t]>threshold],cat,year_lims)\n",
    "            activity.append(levels[True])\n",
    "            names.append(t)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    topic_trends = pd.concat(activity,axis=1).fillna(0)\n",
    "    topic_trends.columns = names\n",
    "    \n",
    "    if focus_topics !=False:\n",
    "        \n",
    "        topic_lookup = {name:val for val,name in enumerate(focus_topics)}\n",
    "\n",
    "        #Color map\n",
    "        cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "        #Create a vector of colors\n",
    "        cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in topic_trends.columns]\n",
    "\n",
    "        #Plot\n",
    "        (100*topic_trends.rolling(window=4).mean().dropna()).plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "        #Fix the legend to focus on key topics\n",
    "        hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in focus_topics],\n",
    "                  labels=[x[1][:50] for x in zip(hand,labs) if x[1] in focus_topics])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        topic_trends.rolling(window=4).mean().dropna().plot(ax=ax)\n",
    "        ax.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_university_industry_collab_trends(df,variable,topic,threshold=0.05):\n",
    "    '''\n",
    "    Study university industry collaborations\n",
    "    \n",
    "    Args:\n",
    "        df as usual\n",
    "        variable is the collaboration variable we want to study\n",
    "        topic the topic\n",
    "        threshold is the threshold for accept a paper in a topic\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    df_with_topic = df.loc[df[topic]>threshold]\n",
    "    \n",
    "\n",
    "    topic_collabs = (100*pd.crosstab(df_with_topic['year'],df_with_topic['university_industry_collab'],normalize=0))[True]\n",
    "    \n",
    "    \n",
    "    return(topic_collabs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "`analysis_pack` contains the metadata and data that we serialised at the end of the `06` data integration notebook.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Community names for the communities (`index->community name`)\n",
    "* Community indices for topics (`topic -> community index`)\n",
    "* Filtered topic names (`topic names`)\n",
    "* Network object with topic co-occurrences\n",
    "* Analysis df\n",
    "* arx is the enriched arXiv dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/24_8_2019_analysis_pack.p','rb') as infile:\n",
    "    analysis_pack = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = analysis_pack[0]\n",
    "comms = analysis_pack[1]\n",
    "topics = analysis_pack[2]\n",
    "network = analysis_pack[3]\n",
    "data = analysis_pack[4]\n",
    "arx = analysis_pack[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}\n",
    "\n",
    "#These are the field names\n",
    "field_names = ['field_astrophysics',\n",
    " 'field_biological',\n",
    " 'field_complex_systems',\n",
    " 'field_informatics',\n",
    " 'field_machine_learning_data',\n",
    " 'field_materials_quantum',\n",
    " 'field_mathematical_physics',\n",
    " 'field_mathematics_1',\n",
    " 'field_mathematics_2',\n",
    " 'field_optimisation',\n",
    " 'field_particle_physics',\n",
    " 'field_physics_education',\n",
    " 'field_societal',\n",
    " 'field_statistics_probability']\n",
    "\n",
    "#Create tidy field names for legend etc\n",
    "tidy_field_lookup = {x:re.sub('_',' ',x[6:]).capitalize() for x in field_names}\n",
    "\n",
    "community_names = [x for x in list(set((comm_names.values()))) if x!='mixed']\n",
    "\n",
    "tidy_comms_lookup = make_tidy_lookup(community_names)\n",
    "\n",
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comms_lookup[l],edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we going to do?\n",
    "\n",
    "* Measure the distribution over terms as before\n",
    "* Study trends (share of DL / Reinforcement learning / Computer vision accounted by companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional processing\n",
    "\n",
    "We detected a few companies that were not classified as multinationals even though they seem to be.\n",
    "\n",
    "We will remove their country parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look for these patterns and split on the space if present\n",
    "my_comps = ['Google (','Apple (', 'Amazon (']\n",
    "\n",
    "data['institute_list_2'] = [[x if not any(var in str(x) for var in my_comps) else x.split(' ')[0] for x in company_list] \n",
    "                            if type(company_list)==list else np.nan for company_list in data['institute_list']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrich the data with relevant variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables of interest\n",
    "interesting = [['type_list','Company'],['type_list','Government'],['type_list','Education'],\n",
    "               ['institute_list_2','Google'],['institute_list_2','Facebook'],['institute_list_2','IBM'],['institute_list_2','Microsoft'],\n",
    "              ['institute_list_2','Apple'],['institute_list_2','Amazon']]\n",
    "\n",
    "#Create the expanded df\n",
    "data_2 = data.copy()\n",
    "\n",
    "#For each interesting variable we expand the df\n",
    "for detect in interesting:\n",
    "    \n",
    "    data_2 = make_exog(data_2,value_container=detect[0],value=detect[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some basic descriptives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many companies?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(data_2['company'])/len(data_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = 100*pd.Series(flatten_list([list(set([inst for inst in x if type(inst)==str])) for x in data_2['institute_list_2'].dropna()])).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = pd.concat([pd.crosstab(data_2['year'],data_2[var],normalize=0)[True] for var in ['company','google','microsoft','ibm','facebook','amazon']],axis=1)\n",
    "comps.columns = ['company','google','microsoft','ibm','facebook','amazon']\n",
    "comps['other companies'] = comps['company']-comps.iloc[:,1:].sum(axis=1)\n",
    "\n",
    "comps_data = 100*comps.loc[np.arange(2000,2019)].iloc[:,1:].rolling(window=3).mean().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,2.5))\n",
    "\n",
    "pal = sns.color_palette('Accent_r')\n",
    "\n",
    "ax.stackplot(comps_data.index,comps_data.T,cmap='Dark2',labels=[x.capitalize() for x in comps_data.columns],\n",
    "             colors=pal,edgecolor='black',linewidth=0.3)\n",
    "\n",
    "handles,labels = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.26,1),handles=handles[::-1],labels=labels[::-1])\n",
    "\n",
    "ax.set_ylabel('% of all AI papers')\n",
    "ax.set_title('Corporate participation in AI research')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "#save_fig('fig_12_corporate_participation.pdf')\n",
    "\n",
    "\n",
    "save_fig('neurips_corp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(comps.loc[2018][1:]/comps.loc[2018][1:].sum()).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# company_topic_comp = cross_sectional_comp(data_2,'company',topics,threshold=0.05)\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "# plot_topic_bar(company_topic_comp['difference'],cl=color_lookup,ax=ax)\n",
    "\n",
    "# ax.set_title('Representation of papers involving companies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_topic_comp = cross_sectional_comp(data_2,'google',topics,threshold=0.05)\n",
    "\n",
    "# fig,ax = plt.subplots(figsize=(8,5))\n",
    "\n",
    "# plot_topic_bar(google_topic_comp['difference'],cl=color_lookup,ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = ['year']+list(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(15,6),ncols=3,sharey=True)\n",
    "\n",
    "plot_regression_coefficients(data_2,'google',ax=ax[0],ncols=2)\n",
    "plot_regression_coefficients(data_2,'company',ax=ax[1],ncols=2)\n",
    "plot_regression_coefficients(data_2,'education',ax=ax[2],ncols=2)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)\n",
    "\n",
    "ax[0].set_ylabel('Regression coefficient in multivariate model')\n",
    "\n",
    "#ax[0].legend([])\n",
    "#ax[1].legend([])\n",
    "#ax[2].legend([])\n",
    "\n",
    "#plt.tight_layout()\n",
    "\n",
    "save_fig('fig_13_estimates.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series analysis\n",
    "\n",
    "I want to study the level of activity in a topic accounted by different types of organisations. \n",
    "\n",
    "The target chart contains share of all papers in a topic accounted by different types of organisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2['no_education'] = data_2['education']==False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_ai_topics = ['cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "                  'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks',\n",
    "                 'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "                 'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "                  'latent-generative_model-generative-generative_models-latent_variables',\n",
    "                  'training-trained-deep_learning-deep-train'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,6),nrows=2,sharex=True)\n",
    "\n",
    "plot_topic_trend(data_2,'company',cmap='Dark2',topics=topics,ax=ax[0],threshold=0.02,focus_topics=core_ai_topics,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "#ax[0].set_title('Share of all papers with company presence')\n",
    "ax[0].set_ylabel('% of all papers in topic \\n with company presence')\n",
    "\n",
    "\n",
    "plot_topic_trend(data_2,'google',cmap='Dark2',topics=topics,ax=ax[1],threshold=0.02,focus_topics=core_ai_topics,alpha=0.07,year_lims=[2004,2019])\n",
    "\n",
    "ax[1].set_ylabel('% of all papers in topic \\n with Google presence')\n",
    "\n",
    "save_fig('fig_14_trends.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the levels of university / industry collaboration?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a variable that captures collaborations\n",
    "data_2['university_industry_collab'] = [all(entity in x for entity in ['Education','Company']) if type(x)==list else np.nan for x in data_2['type_list']]\n",
    "data_2['govt_industry_collab'] = [all(entity in x for entity in ['Government','Company']) if type(x)==list else np.nan for x in data_2['type_list']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*data_2['university_industry_collab'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*data_2['university_industry_collab'].sum()/data_2['company'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract collaborations on 'core AI topics'\n",
    "\n",
    "collabs_in_topics = pd.concat([get_university_industry_collab_trends(data_2,'university_industry_collab',t) for t in core_ai_topics],axis=1).fillna(0)\n",
    "\n",
    "collabs_in_topics.columns = core_ai_topics\n",
    "\n",
    "#Get average collaborations (we set a negative threshold to select all projects)\n",
    "all_collabs = get_university_industry_collab_trends(data_2,'university_industry_collab',community_names[0],threshold=-1)\n",
    "all_collabs.name = 'All subjects'\n",
    "\n",
    "#Concatenate everything\n",
    "collabs_in_topics = pd.concat([all_collabs,collabs_in_topics],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot\n",
    "\n",
    "to_plot = collabs_in_topics.loc[np.arange(1995,2019)].rolling(window=4).mean().dropna()\n",
    "\n",
    "ax = to_plot.plot(figsize=(8,4.5),linewidth=3)\n",
    "\n",
    "\n",
    "hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(handles = [x[0] for x in zip(hand,labs)],\n",
    "          labels=[x[1][:50] for x in zip(hand,labs)],loc='upper left',title='Topic')\n",
    "\n",
    "\n",
    "ax.set_xticks(np.arange(to_plot.index[0],2019,2))\n",
    "\n",
    "ax.set_xlim(2000,2018.1)\n",
    "\n",
    "ax.set_ylabel('% of all papers in topic with a \\n university-industry collaboration')\n",
    "\n",
    "\n",
    "save_fig('fig_15_collab_trends.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
