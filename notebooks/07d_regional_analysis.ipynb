{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional analysis\n",
    "\n",
    "Here we focus on the regional analysis. As part of this we look at:\n",
    "\n",
    "* Levels of concentration and its evolution at the subnational level for all arXiv, AI and SotA topics\n",
    "* Detailed evolution of concentration in the UK\n",
    "* Comparison with distribution of automation in England"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings (for when I concatenate dfs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statsmodels.api import OLS, Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic functions\n",
    "def save_fig(name,path='../reports/figures/paper_rev/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')\n",
    "    \n",
    "    # Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')\n",
    "        \n",
    "def flatten_freq(nested_list):\n",
    "    '''\n",
    "    \n",
    "    Function to calculate frequencies of elements within a nested list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return(pd.Series(flatten_list(nested_list))).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy_lookup(names_list,length=False):\n",
    "    '''\n",
    "    \n",
    "    Creates a cheap lookup between names, removing underscores and capitalising\n",
    "    \n",
    "    Args:\n",
    "        names_list (list) is the list of names we want to tidy\n",
    "        length is if we want to only keep a certain length of the name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = {x:re.sub('_',' ',x).capitalize() for x in names_list}\n",
    "    return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_comp(df,variable,topics,threshold):\n",
    "    '''\n",
    "    This function compares activity by topics between categories.\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe we are using (generally analysis_fin, with rows = papers and columns = variables and metadata)\n",
    "        variable is the variable we are using for the comparison\n",
    "        topics is the topics where we want to compare (generally the community names)\n",
    "        threshold is the threshold we want to use to determine if a paper is in a topic or not\n",
    "    \n",
    "    Returns a df with the shares of papers in each topic sorted by their distances\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the counts df.\n",
    "    \n",
    "    #We are extracting, for each topics, the % of papers with at least one female author when the topic is present, and when it isn't.\n",
    "    group_counts = pd.concat([pd.crosstab(df[variable],df[t]>threshold,normalize=1).loc[True,:] for t in topics],axis=1)\n",
    "    \n",
    "    #Name\n",
    "    group_counts.columns = topics\n",
    "    \n",
    "    #Transpose\n",
    "    group_counts = group_counts.T\n",
    "    \n",
    "    #Rename variables\n",
    "    group_counts.columns = [variable+f'_{value}' for value in ['false','true']]\n",
    "    \n",
    "    #Create a measure of difference\n",
    "    group_counts['difference'] = (group_counts.iloc[:,1]/group_counts.iloc[:,0])-1\n",
    "    \n",
    "    #Output\n",
    "    out = group_counts.sort_values('difference',ascending=False)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def topic_regression(df,target_list,exog,controls,model,binarise=False,standardise=True,cov='HC1'):\n",
    "    '''\n",
    "    \n",
    "    This function regresses topic weights (or their binarisation) on predictors.\n",
    "    \n",
    "    Arguments:\n",
    "        -Df with the variables\n",
    "        -target_list: target variables. This is a list we loop over. \n",
    "        -exog: exogenous variable\n",
    "        -controls\n",
    "        -model type. OLS? Logit? TODO fix the logit\n",
    "        -Binarise in case we are using logit. If not False, the value is the threshold \n",
    "            TODO when we binarise the highly detailed models, some of them become all zeros. This will work better\n",
    "            with the mopre aggregate topics\n",
    "        -Standardise if we standardise and log the topic weights\n",
    "    \n",
    "    Returns\n",
    "        -A list of statsmodels summaries\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Drop rows with missing values - sm doesn't like them\n",
    "    df_2 = df[target_list+exog+controls].dropna(axis=0)\n",
    "    \n",
    "    #Standardise targets?\n",
    "    if standardise==True:\n",
    "        df_2[target_list] = (np.log(df_2[target_list]+0.00000001)).apply(zscore).astype(float)\n",
    "    \n",
    "    #Binarise targets if we are doing a logit\n",
    "    if binarise!=False:\n",
    "        df_2[target_list] = df_2[target_list].applymap(lambda x: x>binarise).astype(float)\n",
    "    \n",
    "    \n",
    "    #Extract the exogenous and controls, add constant and cast as float\n",
    "    exog_controls = add_constant(df_2[exog+controls]).astype(float)\n",
    "    \n",
    "\n",
    "    #Container output\n",
    "    out = []\n",
    "    coeffs = []\n",
    "    \n",
    "    #One regression for each target\n",
    "    for t in list(target_list):\n",
    "        \n",
    "        #There we gp. \n",
    "        reg = model(endog=df_2[t],exog=exog_controls).fit(cov_type=cov,disp=0)\n",
    "        \n",
    "        out.append(reg.summary())\n",
    "        \n",
    "        #coeffs.append(reg)\n",
    "        if model == OLS:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.rsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','r_square']\n",
    "    \n",
    "        else:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.prsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','pr_square']\n",
    " \n",
    "    \n",
    "    return([out,reg_coeff.sort_values('coefficient',ascending=False)])\n",
    "        \n",
    "       \n",
    "\n",
    "def plot_regression_coefficients(df,var,cov='HC1',size=(8,6),ax=False,ncols=3):\n",
    "    '''\n",
    "    Plots regression coefficients.\n",
    "    \n",
    "    Arg:\n",
    "        variable we use as predictor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reg = topic_regression(df,topics,[var],controls,OLS,cov='HC1')\n",
    "    \n",
    "    if ax==False:\n",
    "        fig,ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plot_topic_bar(reg[1]['coefficient'],cl=color_lookup,ax=ax,ncols=ncols)\n",
    "\n",
    "    ax.set_title(f'Regression coefficient using {var} as predictor')\n",
    "\n",
    "def topic_comparison(df,target_list,exog,concept_lookup,quantiles=np.arange(0,1.1,0.2),thres=0):\n",
    "    '''\n",
    "    This function compares the distribution of activity in various topics depending on an exogenous variable of interest. \n",
    "    \n",
    "    Args:\n",
    "        Df with the topic mix and metadata\n",
    "        target_list are the topics to consider\n",
    "        exog is the variable to crosstab topics against\n",
    "        concept_lookup is a df with the median proximity of each topic to the concepts\n",
    "        quantiles is how we discretise the concept lookup (default value is quintiles)\n",
    "        thres: =limit for considering a topic as present\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Copy df\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Discretise the concept lookup\n",
    "    \n",
    "    conc_discr = concept_lookup.apply(lambda x: pd.qcut(x,q=quantiles,labels=False,duplicates='drop'))\n",
    "\n",
    "    \n",
    "    #Calculate levels of activity per topic based on the exog variable\n",
    "    \n",
    "    topic_distr = pd.concat([pd.crosstab(df_2[exog],df_2[t]>thres)[True] for t in target_list],axis=1).T\n",
    "    topic_distr.index = target_list\n",
    "    \n",
    "    \n",
    "    #Merge the count with the concept lookup\n",
    "    disc = pd.melt(pd.concat([topic_distr,conc_discr],axis=1).reset_index(drop=False),id_vars=['index']+list(conc_discr.columns))\n",
    "    \n",
    "    #This is the list where we store the results\n",
    "    store={}\n",
    "    \n",
    "    for c in concept_lookup.columns:\n",
    "        \n",
    "        out = pd.pivot_table(disc.groupby([c,'variable'])['value'].sum().reset_index(drop=False),index=c,columns='variable',values='value')\n",
    "        #out.apply(lambda x: x/x.sum()).plot.bar()\n",
    "        \n",
    "        store[c] = out\n",
    "                                      \n",
    "    #Output dfs with the comparisons\n",
    "    return(store)\n",
    "\n",
    "def plot_topic_bar(table,cl,ax,ncols):\n",
    "    '''\n",
    "    Simple function to plot topic bars which includes colours based on the topic-label lookup\n",
    "    \n",
    "    Args:\n",
    "        table has topics in the index and a value to plot in the columns\n",
    "        cl is the colour lookup between communities and topics\n",
    "        ax is the plotting axe\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in table.index]\n",
    "    \n",
    "    table.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=ncols)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    \n",
    "def calculate_entropy(df,categories,category):\n",
    "    '''\n",
    "    We calculate entropy inside a paper using a distribution over semantic variables (eg discipline, community or topic). These have to be normalised\n",
    "    \n",
    "    arguments:\n",
    "        df is the analysis df with relevant topics and metadata\n",
    "        categories are the topics we want to compare\n",
    "        \n",
    "    outputs\n",
    "        A df with entropy measures by paper\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #Normalise\n",
    "    norm = df[categories].apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    ent = pd.DataFrame((norm.apply(lambda x: entropy(x),axis=1)),columns=['entropy'])\n",
    "    \n",
    "    ent['cat']=category\n",
    "    \n",
    "    return(ent)\n",
    "\n",
    "def make_exog(df,value_container,value,make_dummy=True):\n",
    "    '''\n",
    "    This creates exogenous variables for modelling later.\n",
    "    \n",
    "    Argument:\n",
    "        -df contains the variable where we want to find a value\n",
    "        -variable_container is the column where we want to look for the value\n",
    "        -value is the value we are looking for\n",
    "        -make_dummy: if true it just counts if the value is present. If false, it counts how many times it happens. \n",
    "        \n",
    "    Output\n",
    "        -A df with the new column (named)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Create a tidy variable name\n",
    "    column_name = re.sub(' ','_',value.lower())\n",
    "    \n",
    "    #If we want to create a dummy...\n",
    "    if make_dummy == True:\n",
    "        \n",
    "        #We just look for it in the value container\n",
    "        #There are some missing values so we have some control flow to manage that. \n",
    "        df_2[column_name] = [value in x if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Otherwise, we count how many times it occurs\n",
    "        #We deal with missing values ('non lists') as before\n",
    "        df_2[column_name] = [x.count(value) if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "        \n",
    "    return(df_2)\n",
    "    \n",
    "\n",
    "def extract_topic_trend(df,cat,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    Extracts evolution of a share of a category in a topic of interest\n",
    "    \n",
    "    Args:\n",
    "        df: the usual dataframe\n",
    "        cat: the category we are interested in\n",
    "        year_lims: first and last year to consider\n",
    "\n",
    "    '''\n",
    "    #rel_df = df.loc[df[cat]==True]\n",
    "    \n",
    "    out = pd.crosstab(df['year'],df[cat],normalize=0)\n",
    "    \n",
    "    return(out.loc[np.arange(year_lims[0],year_lims[1])])\n",
    "\n",
    "def plot_topic_trend(df,cat,topics,ax,cmap,year_lims=[2000,2019],threshold=0.05,focus_topics=False,alpha=0.2):\n",
    "    '''\n",
    "    Plots topic trends (shares of a category in a topic)\n",
    "    \n",
    "    Args:\n",
    "        df the usual dataframe\n",
    "        topics: topics we want to display\n",
    "        cat: the category of interest\n",
    "        year_lims: first and last year to consider\n",
    "    \n",
    "    '''\n",
    "    activity = []\n",
    "    names = []\n",
    "    \n",
    "    #Use a loop to deal with cases where a category has no activity in a topic\n",
    "    for t in topics:\n",
    "        try:\n",
    "            levels = extract_topic_trend(df.loc[df[t]>threshold],cat,year_lims)\n",
    "            activity.append(levels[True])\n",
    "            names.append(t)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    topic_trends = pd.concat(activity,axis=1).fillna(0)\n",
    "    topic_trends.columns = names\n",
    "    \n",
    "    if focus_topics !=False:\n",
    "        \n",
    "        topic_lookup = {name:val for val,name in enumerate(focus_topics)}\n",
    "\n",
    "        #Color map\n",
    "        cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "        #Create a vector of colors\n",
    "        cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in topic_trends.columns]\n",
    "\n",
    "        #Plot\n",
    "        (100*topic_trends.rolling(window=4).mean().dropna()).plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "        #Fix the legend to focus on key topics\n",
    "        hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in focus_topics],\n",
    "                  labels=[x[1][:50] for x in zip(hand,labs) if x[1] in focus_topics])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        topic_trends.rolling(window=4).mean().dropna().plot(ax=ax)\n",
    "        ax.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_university_industry_collab_trends(df,variable,topic,threshold=0.05):\n",
    "    '''\n",
    "    Study university industry collaborations\n",
    "    \n",
    "    Args:\n",
    "        df as usual\n",
    "        variable is the collaboration variable we want to study\n",
    "        topic the topic\n",
    "        threshold is the threshold for accept a paper in a topic\n",
    "    \n",
    "\n",
    "    '''\n",
    "    \n",
    "    df_with_topic = df.loc[df[topic]>threshold]\n",
    "    \n",
    "\n",
    "    topic_collabs = (100*pd.crosstab(df_with_topic['year'],df_with_topic['university_industry_collab'],normalize=0))[True]\n",
    "    \n",
    "    \n",
    "    return(topic_collabs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "`analysis_pack` contains the metadata and data that we serialised at the end of the `06` data integration notebook.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Community names for the communities (`index->community name`)\n",
    "* Community indices for topics (`topic -> community index`)\n",
    "* Filtered topic names (`topic names`)\n",
    "* Network object with topic co-occurrences\n",
    "* Analysis df\n",
    "* arx is the enriched arXiv dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/24_8_2019_analysis_pack.p','rb') as infile:\n",
    "    analysis_pack = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = analysis_pack[0]\n",
    "comms = analysis_pack[1]\n",
    "topics = analysis_pack[2]\n",
    "network = analysis_pack[3]\n",
    "data = analysis_pack[4]\n",
    "arx = analysis_pack[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo = pd.read_csv('../data/processed/26_8_2019_grid_geo_admin_all.csv',compression='zip',dtype={'article_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some lookups etc\n",
    "\n",
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}\n",
    "\n",
    "#These are the field names\n",
    "field_names = ['field_astrophysics',\n",
    " 'field_biological',\n",
    " 'field_complex_systems',\n",
    " 'field_informatics',\n",
    " 'field_machine_learning_data',\n",
    " 'field_materials_quantum',\n",
    " 'field_mathematical_physics',\n",
    " 'field_mathematics_1',\n",
    " 'field_mathematics_2',\n",
    " 'field_optimisation',\n",
    " 'field_particle_physics',\n",
    " 'field_physics_education',\n",
    " 'field_societal',\n",
    " 'field_statistics_probability']\n",
    "\n",
    "core_ai_topics = ['cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "                  'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks',\n",
    "                 'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "                 'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "                  'latent-generative_model-generative-generative_models-latent_variables',\n",
    "                  'training-trained-deep_learning-deep-train'\n",
    "                 ]\n",
    "\n",
    "#Create tidy field names for legend etc\n",
    "tidy_field_lookup = {x:re.sub('_',' ',x[6:]).capitalize() for x in field_names}\n",
    "\n",
    "community_names = [x for x in list(set((comm_names.values()))) if x!='mixed']\n",
    "\n",
    "tidy_comms_lookup = make_tidy_lookup(community_names)\n",
    "\n",
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comms_lookup[l],edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Minor processing\n",
    "\n",
    "Add AI and SotA topic labels to the geo data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ids = set(arx.loc[arx['is_ai']==True]['paper_id'])\n",
    "\n",
    "sota_ids = set(data.loc[[any(x>0.05 for x in row[core_ai_topics]) for pid,row in data.iterrows()]].index)\n",
    "\n",
    "#These are the IDS of the datasets that we have modelled\n",
    "\n",
    "modelled_ai = set(data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label with years\n",
    "\n",
    "arx_year = arx[['paper_id','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label the geo-coded df with AI and SoTa\n",
    "#Here we need to turn the not-modelled paper ids into missing\n",
    "\n",
    "arx_geo['has_ai'],arx_geo['has_sota'] = [[x in relevant_set if x in modelled_ai else np.nan for x in arx_geo['article_id']] for relevant_set in [ai_ids,sota_ids]]\n",
    "\n",
    "arx_geo = arx_geo.loc[arx_geo['is_multinational']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo = pd.merge(arx_geo,arx_year,left_on='article_id',right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo_ded = arx_geo.drop_duplicates(['article_id','city_country'])[[\n",
    "    'article_id','city_country','name_en','institute_country','has_ai','has_sota','year']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo_ded['year'] = arx_geo_ded['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo_ded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo_ded.shape[0]/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*arx_geo_ded.loc[arx_geo['has_ai']==1]['name_en'].value_counts(normalize=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*arx_geo_ded.loc[arx_geo['has_sota']==1]['name_en'].value_counts(normalize=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we going to do?\n",
    "\n",
    "* Study levels and evolution of sub-national AI concentration\n",
    "* COnsider the UK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concentration analysis\n",
    "\n",
    "**Steps**\n",
    "\n",
    "* What is the level of subnational concentration of AI research and how has it evolved over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_concentration(df,agg_cat,var='All',thres=2015,n=3):\n",
    "    '''\n",
    "    \n",
    "    Takes a dataframe and creates a cumulative distribution of activity in var\n",
    "    \n",
    "    Args:\n",
    "        df (df) a dataframe where every row is an observation with a category whose distribution we want to study.\n",
    "        agg_cat is the variable whose distribution we want to study\n",
    "        var is a variable to subset the dataframe (we assume this is a boolean)\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Make copy\n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #If we want to focus on a particular variable\n",
    "    if var!='All':\n",
    "        df_2 = df_2.loc[df_2[var]==True]\n",
    "    \n",
    "    #Group and rank\n",
    "    #Calculate Herdindahl index (sum of the squares of the shares)\n",
    "    gr = np.sum([x**2 for x in df_2[agg_cat].value_counts(normalize=True)])\n",
    "    \n",
    "    top= df_2[agg_cat].value_counts(normalize=True).reset_index(drop=True)\n",
    "    \n",
    "    #Calculate concentration cnange\n",
    "    #change\n",
    "    \n",
    "\n",
    "    p_1 = df_2.loc[(df_2['year']==2018)][agg_cat].value_counts(normalize=True)[:n].sum()\n",
    "    \n",
    "    p_2 = df_2.loc[(df_2['year']==2017)][agg_cat].value_counts(normalize=True)[:n].sum()\n",
    "\n",
    "    ch = (100*p_1/p_2)-100\n",
    "    #print(ch)\n",
    "    \n",
    "    return([gr,top,ch])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_c = arx_geo_ded['institute_country'].value_counts().index[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['All','has_ai','has_sota']\n",
    "conc_countries = [[top_concentration(arx_geo_ded.loc[arx_geo_ded['institute_country']==c],'name_en',var) for c in top_10_c] for var in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_concentration_bar(df,ax):\n",
    "    '''\n",
    "    \n",
    "    Plots a concentration horizontal bar for a country\n",
    "    \n",
    "    Args:\n",
    "        df is a df with the information to plot\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df = 100*df\n",
    "    \n",
    "    #Sorts by the countries with the largest share of activity accounted by the top 3 regions\n",
    "    top_3_cuml = df.loc[0:2].sum().sort_values()\n",
    "    \n",
    "    sort_countries = top_3_cuml.index\n",
    "    \n",
    "    df = df[sort_countries]\n",
    "    \n",
    "    df.T.plot.barh(cmap='Reds',stacked=True,legend=False,edgecolor='grey',width=0.75,ax=ax)\n",
    "\n",
    "    \n",
    "    for n,el in enumerate(df.T.index):\n",
    "        \n",
    "        ax.vlines(x=top_3_cuml[el],ymin=n-0.375,ymax=n+0.375,linewidth=3,color='black')\n",
    "        \n",
    "    return(sort_countries)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10),nrows=3,ncols=2,\n",
    "                      sharex='col',sharey='row',gridspec_kw={'width_ratios':[1,0.3]})\n",
    "\n",
    "name = ['ArXiv','AI','SotA']\n",
    "\n",
    "out = []\n",
    "\n",
    "for n in np.arange(0,3):\n",
    "    c = pd.concat([x[1] for x in conc_countries[n]],axis=1)\n",
    "    c.columns = top_10_c\n",
    "    \n",
    "    #This also outputs the columns so we can rearrange the \n",
    "    cols = plot_concentration_bar(c,ax=ax[n][0])\n",
    "    \n",
    "    ax[n][0].set_title(name[n])\n",
    "    \n",
    "    \n",
    "    ch = pd.Series([x[2] for x in conc_countries[n]])\n",
    "    ch.index = top_10_c\n",
    "    \n",
    "    ch = ch[cols]\n",
    "    \n",
    "    for num,ind in enumerate(ch):\n",
    "        ax[n][1].scatter(ind,num,color='coral',edgecolor='grey',s=50,\n",
    "                    marker = '>' if ind>0 else '<' )\n",
    "        \n",
    "        ax[n][1].hlines(y=num,xmin=0,xmax=ind,color='grey',linestyle='-',linewidth=1)\n",
    "    \n",
    "    \n",
    "    ax[n][1].vlines(x=0,ymin=0,ymax=len(ch),linestyle=':',color='black',linewidth=1)\n",
    "    \n",
    "ax[2][0].set_xlabel('% of all activity accounted by region')\n",
    "ax[2][1].set_xlabel('% change in % \\n accounted by top 3')\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05)    \n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.savefig('../reports/figures/paper_rev/fig_15_subn_share.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x[1] for x in conc_countries[1]],axis=1).loc[:2].sum().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([x[1] for x in conc_countries[2]],axis=1).loc[:2].sum().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final analysis: places\n",
    "\n",
    "We load the lookup between article ids and lads we created in `supp_6` and use it to study the geography of AI research in the UK.\n",
    "\n",
    "More specifically, we want to create three charts:\n",
    "\n",
    "* Concentration trends\n",
    "* Concentration in AI 'core topics'\n",
    "* Comparison between concentration of AI activity and areas at risk of automation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/26_8_2019_arxiv_lads.json','r') as infile:\n",
    "    lad_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w_countries = data.dropna(axis=0,subset=['country_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Focus on papers in the UK. We include Australia because there was a mess-up with some of the geocoding\n",
    "\n",
    "data_uk = data_w_countries.loc[[any(var in x for var in ['United Kingdom','Australia']) for x in data_w_countries['country_list']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label papers with their lad codes and names\n",
    "data_uk['lad_code'],data_uk['lad_name'] = [[lad_lookup[x][var] if x in lad_lookup.keys() else np.nan for x in data_uk.index] for var in ['lad18cd','lad18nm']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing LADs for this analysis\n",
    "data_uk = data_uk.dropna(axis=0,subset=['lad_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point one: Geographical trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#All the LADs in the data\n",
    "all_lads = pd.Series(flatten_list(data_uk['lad_name'])).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_local_research_concentration(df,top_n,ax,subset_topics=False,lad_list = all_lads,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    This function plots the concentration of research activity in LADs\n",
    "    \n",
    "    Args:\n",
    "        df (df) is the df with papers and lads (so this will have been processed as above)\n",
    "        top_n (int) is how many of the lads do we want to show\n",
    "        ax is the axis\n",
    "        lad_list (list) is the list of LADs to consider\n",
    "        subset_topics (list) is a list where the first element is the list of topics (or communities) we want to focus on; the second is the threshold for inclusion\n",
    "        year_lims is the years to consider\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if subset_topics!=False:\n",
    "        df = df.loc[df[subset_topics[0]].apply(lambda x: any(x>subset_topics[1]),axis=1)]\n",
    "        \n",
    "    \n",
    "        \n",
    "    activity_year = pd.concat([df.loc[[lad in x for x in df['lad_name']]]['year'].value_counts() for lad in lad_list],axis=1).fillna(0)\n",
    "    activity_year.columns = lad_list\n",
    "    \n",
    "    top_lads = activity_year.sum(axis=0).sort_values(ascending=False).index[:top_n]\n",
    "        \n",
    "\n",
    "    (100*activity_year.apply(lambda x: x/x.sum(),axis=1).rolling(window=3).mean()).dropna().loc[np.arange(year_lims[0],\n",
    "                                                                                                   year_lims[1]),top_lads].plot.bar(\n",
    "        stacked=True,width=0.9,cmap='Accent',edgecolor='lightgrey',ax=ax)\n",
    "    \n",
    "    ax.legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "plot_local_research_concentration(data_uk,8,ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_ylabel('Share of all papers \\n with LAD presence')\n",
    "#ax.set_title('Evolution of local AI research activity in the UK (top 8 locations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_16_ai_research_all.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Towwer Hamlets involves Queen Mary university\n",
    "#analysis_uk.loc[['Tower Hamlets' in x for x in analysis_uk['lad_name']]]['institute_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the core topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,4))\n",
    "\n",
    "plot_local_research_concentration(data_uk,8,ax=ax,subset_topics=[core_ai_topics,0.05],year_lims=[2009,2019])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_ylabel('Share of all papers with LAD presence')\n",
    "#ax.set_title('Evolution of local AI research activity (state of the art AI topics) in the UK (top 8 locations)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_17_ai_research_core.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analysis_uk.loc[['Wokingham' in x for x in analysis_uk['lad_name']]]['institute_list']\n",
    "#Wokingham is University of Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare geography of AI activity and geography of automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load automation data\n",
    "aut = pd.read_csv('../data/processed/19_7_2019_ons_automation_clean.csv',index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lad_activity(df,name,subset_topics=False):\n",
    "    '''\n",
    "    Extracts the distribution of activity by LAD.\n",
    "    \n",
    "    Args:\n",
    "        df (df) with the data\n",
    "        topic_subset (list) if not false, the topics to focus on and their threshold for inclusion\n",
    "        name (str) is the name of the variable\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    if subset_topics != False:\n",
    "        df = df.loc[df[subset_topics[0]].apply(lambda x: any(x>subset_topics[1]),axis=1)]\n",
    "        \n",
    "    counts = pd.concat([pd.Series(len(df.loc[[lad in x for x in df['lad_name']]]),name=lad,index=[name]) for lad in all_lads],axis=1).fillna(0).T\n",
    "    \n",
    "    return(counts)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine automation data with AI\n",
    "\n",
    "#List comprehension\n",
    "ai_lad_counts = pd.concat([get_lad_activity(data_uk,name,topic_subset) for name,topic_subset in zip(['All AI','Core AI topics'],[False,[core_ai_topics,0.02]])],axis=1)\n",
    "\n",
    "aut_ai = pd.concat([aut.set_index('lad_name'),ai_lad_counts],axis=1).dropna(axis=0,subset=['lad_code']).fillna(0)\n",
    "\n",
    "aut_ai.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_concentrations(df,ranking_var,quantiles,comparisons,ax):\n",
    "    '''\n",
    "    \n",
    "    We create a df that compares share of automation, AI activity accounted by different locations.\n",
    "    \n",
    "    Args:\n",
    "        df is a table with automation and AI activity\n",
    "        ranking_var is the variable we use to create the groups to analyse the distribution\n",
    "        quantiles is the number of groups we create\n",
    "        comparisons are the variables we want to benchmark\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    df_2['aut_rank'] = pd.qcut(df_2[ranking_var],q=quantiles,labels=False)\n",
    "\n",
    "    df_norm = df_2[comparisons].apply(lambda x: x/x.sum())\n",
    "    df_norm['aut_rank'] = df_2['aut_rank']\n",
    "    \n",
    "    (100*df_norm.groupby('aut_rank')[comparisons].sum()).plot.bar(ax=ax)\n",
    "    \n",
    "    #print(df_norm.loc[df_norm['aut_rank']==4])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(8,3.5))\n",
    "\n",
    "comps = ['number_high','All AI','Core AI topics']\n",
    "q = np.arange(0,1.1,0.25)\n",
    "\n",
    "benchmark_concentrations(aut_ai,'aut_prob',q,comps,ax)\n",
    "\n",
    "ax.set_xlabel('Workforce automation ranking (quartile)')\n",
    "ax.set_ylabel('% of the total in the UK')\n",
    "\n",
    "ax.legend(title='Variable',labels = ['Workforce with high risk of automation','AI research activity','AI state of the art activity'])\n",
    "#ax.set_title('Distribution of AI activity and population at risk of automation')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_18_lad_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo.loc[arx_geo['institute_country']=='Canada']['name_en'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
