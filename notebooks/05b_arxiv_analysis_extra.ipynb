{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# arXiv additional analysis\n",
    "\n",
    "This notebook contains some additional, final analysis for our arXiv mapping project. We are integrating them here because the main analytical notebook has become too big and complex (we should disentangle it too)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy\n",
    "\n",
    "import geopandas as gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action 1: arXiv files in Import AI\n",
    "\n",
    "How many links to arXiv papers can we find in import AI?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the first page from import AI\n",
    "iai = requests.get('https://jack-clark.net/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse\n",
    "soup = BeautifulSoup(iai.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get links and remove non documents\n",
    "links = [l.get('href') for l in soup.find_all('a')]\n",
    "\n",
    "links_clean = [x for x in links if not any(nope in x for nope in ['jack-clark','twitter','wordpress','github'])]\n",
    "\n",
    "#How many are to arXiv?\n",
    "arx = np.mean(['arxiv' in x for x in links_clean])\n",
    "\n",
    "arx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action 2: Visualise AI geographies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = gp.read_file('../data/external/countries/Countries_WGS84.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_geo = pd.read_csv('../data/external/17_8_2019_papers_institution_ucl_cleaned.csv',compression='zip',dtype={'article_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = pd.read_csv('../data/processed/18_8_2019_analysis_fin.csv',compression='zip',dtype={'paper_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_all = pd.read_csv('../data/processed/1_8_2019_arxiv_enriched.csv',compression='zip',dtype={'id':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Grid GEO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ai = set(list(arx_all.loc[arx_all['is_ai']==True]['id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_topics = [\n",
    "    #'face-faces-identity-face_recognition-facial','person-surveillance-persons-pedestrian-pedestrians',\n",
    "    #'attacks-attack-adversary-vulnerable-threat',\n",
    "    #'emotions-emotion-neutral-emotional-spontaneous',\n",
    "    'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "    'cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "    'training-trained-deep_learning-deep-train',\n",
    "    'generator-gan-discriminator-generative_adversarial_networks_gans-gans',\n",
    "    'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "    'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_notable = set(list([row['paper_id'] for pid, row in analysis_df.iterrows() if any(x>0.05 for x in row[notable_topics])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_geo['is_ai'],grid_geo['is_notable_ai'] = [[x in this_set for x in grid_geo['article_id']] for this_set in [is_ai,is_notable]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,7))\n",
    "\n",
    "my_grid = grid_geo.loc[(grid_geo['is_ai']==True)]\n",
    "\n",
    "#my_grid = grid_geo\n",
    "\n",
    "ax.hexbin(my_grid['institute_lon'],my_grid['institute_lat'],cmap='Reds',bins=10,gridsize=40,edgecolor='lightgrey',linewidth=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = plt.subplots(figsize=(18,12))\n",
    "\n",
    "# my_grid = grid_geo.loc[(grid_geo['is_ai']==True)]\n",
    "\n",
    "# ax.hexbin(my_grid['institute_lon'],my_grid['institute_lat'],cmap='Reds',bins=10,gridsize=40,edgecolor='lightgrey',linewidth=0.07)\n",
    "\n",
    "# #countries.plot(ax=ax,color=(0,0,0,0),edgecolor='lightgrey',linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
