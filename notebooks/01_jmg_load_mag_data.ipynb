{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MAG data\n",
    "\n",
    "This is the arXiv corpus post-matching with GRID\n",
    "\n",
    "We want to use it in a institutional analysis of top research trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_getters.inspector import get_schemas\n",
    "config_path = '../mysqldb_team.config'\n",
    "\n",
    "schemas = get_schemas(conf_path=config_path)\n",
    "# Show datasets\n",
    "print(schemas.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_getters.core import get_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = get_engine(config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_sql_table('arxiv_articles', con , chunksize=1000)\n",
    "\n",
    "df_container = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in chunks:\n",
    "    df_container.append(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df = pd.concat(df_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df.reset_index(drop=False,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df['year'] = [x.year for x in arxiv_df['created']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract affiliations from the author dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Firt remove papers without MAG metadata\n",
    "arxiv_df_2 = arxiv_df.loc[[type(x)==list for x in arxiv_df['mag_authors']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_2['mag_authors'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now \n",
    "\n",
    "for v in ['author_name','author_affiliation']:\n",
    "    \n",
    "    arxiv_df_2[v+'_list'] = [[x[v] if v in x.keys() else np.nan for x in author] for author in arxiv_df_2['mag_authors']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I want to label these papers with AI ids\n",
    "\n",
    "I will use the papers identified by Kostas in his Women in AI analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ids = pd.read_csv('../data/external/dl_paper_ids.csv',dtype={'paper_id':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_labelled = set(ai_ids['paper_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only focus on papers that we have labelled as AI (or not)\n",
    "arxiv_df_3 = pd.merge(arxiv_df_2,ai_ids[['paper_id','terms','is_AI','number_of_terms']],left_on='id',right_on='paper_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_3.rename(columns={'is_AI':'is_ai'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick check of institutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df_3.to_csv(f'../data/external/{today_str}_arxiv_mag.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAG-GRID data\n",
    "\n",
    "I am also downloading this data for the geographical analysis of arXiv. As a bonus, the data comes with the grid id so we can use that for the institutional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_getters.arxiv_grid import get_arxiv_grid\n",
    "#df = get_arxiv_grid_deep_change(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the query\n",
    "df = get_arxiv_grid(conf_path=config_path,all_articles=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "df.to_csv(f'../data/external/{today_str}_paper_institute_locations.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some small explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_year_counts = pd.DataFrame(arxiv_df['year'].value_counts().loc[np.arange(1986,2019)].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rate = []\n",
    "\n",
    "for n,x in enumerate(arxiv_year_counts['year']):\n",
    "    \n",
    "    if n==0:\n",
    "        out=np.nan\n",
    "        growth_rate.append(np.nan)\n",
    "    else:\n",
    "        if div!=0:\n",
    "            out = 100*((x/div)-1)\n",
    "            growth_rate.append(out)\n",
    "        else:\n",
    "            growth_rate.append(np.nan)\n",
    "        \n",
    "    div = x\n",
    "\n",
    "#len(growth_rate)\n",
    "arxiv_year_counts['growth_rate']=growth_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "arxiv_year_counts['growth_rate'].loc[np.arange(2000,2019)].rolling(window=3).mean().plot()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
