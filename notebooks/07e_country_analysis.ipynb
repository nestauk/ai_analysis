{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Country analysis\n",
    "\n",
    "This is the country analysis that concludes our mapping of AI research\n",
    "\n",
    "We will load and process the data, analyse semantic differences between Free / Not Free countries, and study research trends in controversial, surveillance enabling technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore future warnings (for when I concatenate dfs)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from statsmodels.api import OLS, Logit\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import entropy\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a bunch of exogenous variables to the analysis df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generic functions\n",
    "def save_fig(name,path='../reports/figures/paper_rev/'):\n",
    "    '''\n",
    "    Saves a figure\n",
    "    '''\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(path+f'{today_str}_{name}')\n",
    "    \n",
    "    # Put functions etc here\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def get_example(df,number,length):\n",
    "    '''\n",
    "    Gets random examples in a field\n",
    "    \n",
    "    Args:\n",
    "        Df is the dataframe we want to use\n",
    "        number is the number of examples we want\n",
    "        length is the length of the examples\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    choose = random.sample(list(df.index),number)\n",
    "    \n",
    "    for x in df.loc[choose]['abstract']:\n",
    "        \n",
    "        print(x[:length])\n",
    "        print('\\n')\n",
    "        \n",
    "def flatten_freq(nested_list):\n",
    "    '''\n",
    "    \n",
    "    Function to calculate frequencies of elements within a nested list\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    return(pd.Series(flatten_list(nested_list))).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tidy_lookup(names_list,length=False):\n",
    "    '''\n",
    "    \n",
    "    Creates a cheap lookup between names, removing underscores and capitalising\n",
    "    \n",
    "    Args:\n",
    "        names_list (list) is the list of names we want to tidy\n",
    "        length is if we want to only keep a certain length of the name\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    out = {x:re.sub('_',' ',x).capitalize() for x in names_list}\n",
    "    return(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_comp(df,variable,topics,threshold):\n",
    "    '''\n",
    "    This function compares activity by topics between categories.\n",
    "    \n",
    "    Args:\n",
    "        df is the dataframe we are using (generally analysis_fin, with rows = papers and columns = variables and metadata)\n",
    "        variable is the variable we are using for the comparison\n",
    "        topics is the topics where we want to compare (generally the community names)\n",
    "        threshold is the threshold we want to use to determine if a paper is in a topic or not\n",
    "    \n",
    "    Returns a df with the shares of papers in each topic sorted by their distances\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Create the counts df.\n",
    "    \n",
    "    #We are extracting, for each topics, the % of papers with at least one female author when the topic is present, and when it isn't.\n",
    "    group_counts = pd.concat([pd.crosstab(df[variable],df[t]>threshold,normalize=1).loc[True,:] for t in topics],axis=1)\n",
    "    \n",
    "    #Name\n",
    "    group_counts.columns = topics\n",
    "    \n",
    "    #Transpose\n",
    "    group_counts = group_counts.T\n",
    "    \n",
    "    #Rename variables\n",
    "    group_counts.columns = [variable+f'_{value}' for value in ['false','true']]\n",
    "    \n",
    "    #Create a measure of difference\n",
    "    group_counts['difference'] = (group_counts.iloc[:,1]/group_counts.iloc[:,0])-1\n",
    "    \n",
    "    #Output\n",
    "    out = group_counts.sort_values('difference',ascending=False)\n",
    "    \n",
    "    return(out)\n",
    "\n",
    "def topic_regression(df,target_list,exog,controls,model,binarise=False,standardise=True,cov='HC1'):\n",
    "    '''\n",
    "    \n",
    "    This function regresses topic weights (or their binarisation) on predictors.\n",
    "    \n",
    "    Arguments:\n",
    "        -Df with the variables\n",
    "        -target_list: target variables. This is a list we loop over. \n",
    "        -exog: exogenous variable\n",
    "        -controls\n",
    "        -model type. OLS? Logit? TODO fix the logit\n",
    "        -Binarise in case we are using logit. If not False, the value is the threshold \n",
    "            TODO when we binarise the highly detailed models, some of them become all zeros. This will work better\n",
    "            with the mopre aggregate topics\n",
    "        -Standardise if we standardise and log the topic weights\n",
    "    \n",
    "    Returns\n",
    "        -A list of statsmodels summaries\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Drop rows with missing values - sm doesn't like them\n",
    "    df_2 = df[target_list+exog+controls].dropna(axis=0)\n",
    "    \n",
    "    #Standardise targets?\n",
    "    if standardise==True:\n",
    "        df_2[target_list] = (np.log(df_2[target_list]+0.00000001)).apply(zscore).astype(float)\n",
    "    \n",
    "    #Binarise targets if we are doing a logit\n",
    "    if binarise!=False:\n",
    "        df_2[target_list] = df_2[target_list].applymap(lambda x: x>binarise).astype(float)\n",
    "    \n",
    "    \n",
    "    #Extract the exogenous and controls, add constant and cast as float\n",
    "    exog_controls = add_constant(df_2[exog+controls]).astype(float)\n",
    "    \n",
    "\n",
    "    #Container output\n",
    "    out = []\n",
    "    coeffs = []\n",
    "    \n",
    "    #One regression for each target\n",
    "    for t in list(target_list):\n",
    "        \n",
    "        #There we gp. \n",
    "        reg = model(endog=df_2[t],exog=exog_controls).fit(cov_type=cov,disp=0)\n",
    "        \n",
    "        out.append(reg.summary())\n",
    "        \n",
    "        #coeffs.append(reg)\n",
    "        if model == OLS:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.rsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','r_square']\n",
    "    \n",
    "        else:\n",
    "            coeffs.append(pd.Series([float(reg.params[exog]),float(reg.pvalues[exog]),float(reg.prsquared)],name=t))\n",
    "            reg_coeff = pd.concat(coeffs,axis=1).T\n",
    "            reg_coeff.columns = ['coefficient','p_value','pr_square']\n",
    " \n",
    "    \n",
    "    return([out,reg_coeff.sort_values('coefficient',ascending=False)])\n",
    "        \n",
    "       \n",
    "def plot_regression_coefficients(df,var,cov='HC1',size=(8,6),ax=False,ncols=3):\n",
    "    '''\n",
    "    Plots regression coefficients.\n",
    "    \n",
    "    Arg:\n",
    "        variable we use as predictor.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    reg = topic_regression(df,topics,[var],controls,OLS,cov='HC1')\n",
    "    \n",
    "    if ax==False:\n",
    "        fig,ax = plt.subplots(figsize=size)\n",
    "\n",
    "    plot_topic_bar(reg[1]['coefficient'],cl=color_lookup,ax=ax,ncols=ncols)\n",
    "\n",
    "    ax.set_title(f'Regression coefficient using {var} as predictor')\n",
    "\n",
    "def topic_comparison(df,target_list,exog,concept_lookup,quantiles=np.arange(0,1.1,0.2),thres=0):\n",
    "    '''\n",
    "    This function compares the distribution of activity in various topics depending on an exogenous variable of interest. \n",
    "    \n",
    "    Args:\n",
    "        Df with the topic mix and metadata\n",
    "        target_list are the topics to consider\n",
    "        exog is the variable to crosstab topics against\n",
    "        concept_lookup is a df with the median proximity of each topic to the concepts\n",
    "        quantiles is how we discretise the concept lookup (default value is quintiles)\n",
    "        thres: =limit for considering a topic as present\n",
    "\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Copy df\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Discretise the concept lookup\n",
    "    \n",
    "    conc_discr = concept_lookup.apply(lambda x: pd.qcut(x,q=quantiles,labels=False,duplicates='drop'))\n",
    "\n",
    "    \n",
    "    #Calculate levels of activity per topic based on the exog variable\n",
    "    \n",
    "    topic_distr = pd.concat([pd.crosstab(df_2[exog],df_2[t]>thres)[True] for t in target_list],axis=1).T\n",
    "    topic_distr.index = target_list\n",
    "    \n",
    "    \n",
    "    #Merge the count with the concept lookup\n",
    "    disc = pd.melt(pd.concat([topic_distr,conc_discr],axis=1).reset_index(drop=False),id_vars=['index']+list(conc_discr.columns))\n",
    "    \n",
    "    #This is the list where we store the results\n",
    "    store={}\n",
    "    \n",
    "    for c in concept_lookup.columns:\n",
    "        \n",
    "        out = pd.pivot_table(disc.groupby([c,'variable'])['value'].sum().reset_index(drop=False),index=c,columns='variable',values='value')\n",
    "        #out.apply(lambda x: x/x.sum()).plot.bar()\n",
    "        \n",
    "        store[c] = out\n",
    "                                      \n",
    "    #Output dfs with the comparisons\n",
    "    return(store)\n",
    "\n",
    "def plot_topic_bar(table,cl,ax,ncols):\n",
    "    '''\n",
    "    Simple function to plot topic bars which includes colours based on the topic-label lookup\n",
    "    \n",
    "    Args:\n",
    "        table has topics in the index and a value to plot in the columns\n",
    "        cl is the colour lookup between communities and topics\n",
    "        ax is the plotting axe\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    cols = [cl[comm_names[comms[x]]] if comm_names[comms[x]] in cl.keys() else 'lightgrey' for x in table.index]\n",
    "    \n",
    "    table.plot.bar(color=cols,ax=ax,width=1)\n",
    "    \n",
    "    ax.legend(handles=patches,ncol=ncols)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xticklabels([])\n",
    "    \n",
    "    \n",
    "def calculate_entropy(df,categories,category):\n",
    "    '''\n",
    "    We calculate entropy inside a paper using a distribution over semantic variables (eg discipline, community or topic). These have to be normalised\n",
    "    \n",
    "    arguments:\n",
    "        df is the analysis df with relevant topics and metadata\n",
    "        categories are the topics we want to compare\n",
    "        \n",
    "    outputs\n",
    "        A df with entropy measures by paper\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    #Normalise\n",
    "    norm = df[categories].apply(lambda x: x/x.sum(),axis=1)\n",
    "    \n",
    "    ent = pd.DataFrame((norm.apply(lambda x: entropy(x),axis=1)),columns=['entropy'])\n",
    "    \n",
    "    ent['cat']=category\n",
    "    \n",
    "    return(ent)\n",
    "\n",
    "def make_exog(df,value_container,value,make_dummy=True):\n",
    "    '''\n",
    "    This creates exogenous variables for modelling later.\n",
    "    \n",
    "    Argument:\n",
    "        -df contains the variable where we want to find a value\n",
    "        -variable_container is the column where we want to look for the value\n",
    "        -value is the value we are looking for\n",
    "        -make_dummy: if true it just counts if the value is present. If false, it counts how many times it happens. \n",
    "        \n",
    "    Output\n",
    "        -A df with the new column (named)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    df_2 = df.copy()\n",
    "    \n",
    "    #Create a tidy variable name\n",
    "    column_name = re.sub(' ','_',value.lower())\n",
    "    \n",
    "    #If we want to create a dummy...\n",
    "    if make_dummy == True:\n",
    "        \n",
    "        #We just look for it in the value container\n",
    "        #There are some missing values so we have some control flow to manage that. \n",
    "        df_2[column_name] = [value in x if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        #Otherwise, we count how many times it occurs\n",
    "        #We deal with missing values ('non lists') as before\n",
    "        df_2[column_name] = [x.count(value) if type(x)==list else np.nan for x in df_2[value_container]]\n",
    "        \n",
    "    return(df_2)\n",
    "    \n",
    "\n",
    "def extract_topic_trend(df,cat,year_lims=[2000,2019]):\n",
    "    '''\n",
    "    Extracts evolution of a share of a category in a topic of interest\n",
    "    \n",
    "    Args:\n",
    "        df: the usual dataframe\n",
    "        cat: the category we are interested in\n",
    "        year_lims: first and last year to consider\n",
    "\n",
    "    '''\n",
    "    #rel_df = df.loc[df[cat]==True]\n",
    "    \n",
    "    out = pd.crosstab(df['year'],df[cat],normalize=0)\n",
    "    \n",
    "    return(out.loc[np.arange(year_lims[0],year_lims[1])])\n",
    "\n",
    "def plot_topic_trend(df,cat,topics,ax,cmap,year_lims=[2000,2019],threshold=0.05,focus_topics=False,alpha=0.2):\n",
    "    '''\n",
    "    Plots topic trends (shares of a category in a topic)\n",
    "    \n",
    "    Args:\n",
    "        df the usual dataframe\n",
    "        topics: topics we want to display\n",
    "        cat: the category of interest\n",
    "        year_lims: first and last year to consider\n",
    "    \n",
    "    '''\n",
    "    activity = []\n",
    "    names = []\n",
    "    \n",
    "    #Use a loop to deal with cases where a category has no activity in a topic\n",
    "    for t in topics:\n",
    "        try:\n",
    "            levels = extract_topic_trend(df.loc[df[t]>threshold],cat,year_lims)\n",
    "            activity.append(levels[True])\n",
    "            names.append(t)\n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "    topic_trends = pd.concat(activity,axis=1).fillna(0)\n",
    "    topic_trends.columns = names\n",
    "    \n",
    "    if focus_topics !=False:\n",
    "        \n",
    "        topic_lookup = {name:val for val,name in enumerate(focus_topics)}\n",
    "\n",
    "        #Color map\n",
    "        cols = plt.cm.get_cmap(cmap)\n",
    "\n",
    "        #Create a vector of colors\n",
    "        cols_to_show = [(0.5,0.5,0.5,alpha) if v not in topic_lookup.keys() else cols(topic_lookup[v]) for v in topic_trends.columns]\n",
    "\n",
    "        #Plot\n",
    "        (100*topic_trends.rolling(window=4).mean().dropna()).plot(color=cols_to_show,ax=ax,linewidth=3)\n",
    "\n",
    "        #Fix the legend to focus on key topics\n",
    "        hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1,1),handles = [x[0] for x in zip(hand,labs) if x[1] in focus_topics],\n",
    "                  labels=[x[1][:50] for x in zip(hand,labs) if x[1] in focus_topics])\n",
    "    \n",
    "    else:\n",
    "\n",
    "        topic_trends.rolling(window=4).mean().dropna().plot(ax=ax)\n",
    "        ax.legend(bbox_to_anchor=(1,1))\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "labels=['China','Not free excluding China','All']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_growth_rate(series):\n",
    "    '''\n",
    "    This function creates a growth rate for a series\n",
    "    \n",
    "    It takes the series and divides a value by the next value. Divisions by zero are nan\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    growth_rate = []\n",
    "\n",
    "    for n,x in enumerate(series):\n",
    "\n",
    "        if n==0:\n",
    "            out=np.nan\n",
    "            growth_rate.append(np.nan)\n",
    "        else:\n",
    "            if div!=0:\n",
    "                out = 100*((x/div)-1)\n",
    "                growth_rate.append(out)\n",
    "            else:\n",
    "                growth_rate.append(np.nan)\n",
    "\n",
    "        div = x\n",
    "\n",
    "    return(growth_rate)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trend_in_topic(df,topic,name,threshold=0.05,year_lim=[2005,2019],normalize=0):\n",
    "    '''\n",
    "    This returns trends of activity in a topic as a share of all activity\n",
    "    \n",
    "    Args:\n",
    "        df is the df\n",
    "        topic is the topic of interest\n",
    "        threshold is the threshold\n",
    "        year_lim is the years to consider\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if normalize!='none':\n",
    "        trend = pd.crosstab(df['year'],df[topic]>threshold,normalize=normalize)\n",
    "        \n",
    "    else:\n",
    "        trend = pd.crosstab(df['year'],df[topic]>threshold)\n",
    "    \n",
    "    \n",
    "    trend.rename(columns={True:name},inplace=True)\n",
    "    \n",
    "    return(trend.loc[np.arange(year_lim[0],year_lim[1])].fillna(0)[name])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data\n",
    "\n",
    "`analysis_pack` contains the metadata and data that we serialised at the end of the `06` data integration notebook.\n",
    "\n",
    "This includes:\n",
    "\n",
    "* Community names for the communities (`index->community name`)\n",
    "* Community indices for topics (`topic -> community index`)\n",
    "* Filtered topic names (`topic names`)\n",
    "* Network object with topic co-occurrences\n",
    "* Analysis df\n",
    "* arx is the enriched arXiv dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/24_8_2019_analysis_pack.p','rb') as infile:\n",
    "    analysis_pack = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_names = analysis_pack[0]\n",
    "comms = analysis_pack[1]\n",
    "topics = analysis_pack[2]\n",
    "network = analysis_pack[3]\n",
    "data = analysis_pack[4]\n",
    "arx = analysis_pack[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some lookups etc\n",
    "\n",
    "color_lookup = {\n",
    "    'deep_learning':'blue',\n",
    "    'robotics_agents':'cornflowerblue',\n",
    "    'computer_vision':'aqua',\n",
    "    'symbolic':'red',\n",
    "    'health':'lime',\n",
    "    'social':'forestgreen',\n",
    "    'technology':'magenta',\n",
    "    'statistics':'orange',\n",
    "    'language':'yellow'\n",
    "}\n",
    "\n",
    "#These are the field names\n",
    "field_names = ['field_astrophysics',\n",
    " 'field_biological',\n",
    " 'field_complex_systems',\n",
    " 'field_informatics',\n",
    " 'field_machine_learning_data',\n",
    " 'field_materials_quantum',\n",
    " 'field_mathematical_physics',\n",
    " 'field_mathematics_1',\n",
    " 'field_mathematics_2',\n",
    " 'field_optimisation',\n",
    " 'field_particle_physics',\n",
    " 'field_physics_education',\n",
    " 'field_societal',\n",
    " 'field_statistics_probability']\n",
    "\n",
    "core_ai_topics = ['cnn-convolutional_neural_networks-cnns-convolutional_neural_network-convolutional_neural_network_cnn',\n",
    "                  'recurrent-lstm-rnn-recurrent_neural_network-recurrent_neural_networks',\n",
    "                 'reinforcement_learning-policy-policies-reward-deep_reinforcement_learning',\n",
    "                 'translation-neural_machine_translation-machine_translation-translate-translations',\n",
    "                  'latent-generative_model-generative-generative_models-latent_variables',\n",
    "                  'training-trained-deep_learning-deep-train'\n",
    "                 ]\n",
    "\n",
    "surv_topics = ['face-faces-identity-face_recognition-facial','person-surveillance-persons-pedestrian-pedestrians'\n",
    "              ]\n",
    "\n",
    "#Create tidy field names for legend etc\n",
    "tidy_field_lookup = {x:re.sub('_',' ',x[6:]).capitalize() for x in field_names}\n",
    "\n",
    "community_names = [x for x in list(set((comm_names.values()))) if x!='mixed']\n",
    "\n",
    "tidy_comms_lookup = make_tidy_lookup(community_names)\n",
    "\n",
    "patches = [mpatches.Patch(facecolor=c, label=tidy_comms_lookup[l],edgecolor='black') for l,c in color_lookup.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/processed/26_8_2019_country_status_lookup','r') as infile:\n",
    "    country_status_lookup = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with the full geo-data\n",
    "\n",
    "We will use the full geo-data to analyse evolution of activity in arXiv, AI and surveillance topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo = pd.read_csv(\n",
    "    '../data/processed/26_8_2019_grid_geo_admin_all.csv',compression='zip',dtype={'article_id':str})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the geo data with various relevant fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ids = set(arx.loc[arx['is_ai']==True]['paper_id'])\n",
    "\n",
    "sota_ids = set(data.loc[[any(x>0.05 for x in row[core_ai_topics]) for pid,row in data.iterrows()]].index)\n",
    "\n",
    "modelled_ai = set(data.index)\n",
    "\n",
    "surv_ids = set(data.loc[data[surv_topics].apply(lambda x: any(x>0.05),axis=1)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label with years\n",
    "\n",
    "arx_year = arx[['paper_id','year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label the geo-coded df with AI and SoTa\n",
    "\n",
    "arx_geo['has_ai'],arx_geo[\n",
    "    'has_sota'], arx_geo['has_surv'] = [\n",
    "    [x in relevant_set if x in modelled_ai else np.nan for x in arx_geo['article_id']] for relevant_set in [ai_ids,sota_ids,surv_ids]]\n",
    "\n",
    "arx_geo = arx_geo.loc[arx_geo['is_multinational']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo = pd.merge(arx_geo,arx_year,left_on='article_id',right_on='paper_id')\n",
    "\n",
    "arx_geo['year'] = arx_geo['year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country comparison (free / not free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables of interest\n",
    "interesting_cuts = [['freedom_list','NF'],\n",
    "                    ['country_list','China'],['country_list','Russia'],['country_list','Turkey'],\n",
    "                    ['country_list','United States'],['country_list','United Kingdom'],['country_list','Germany'],\n",
    "                    ['type_list','Company'],['type_list','Government'],['type_list','Education']]\n",
    "\n",
    "#Create the expanded df\n",
    "data_2 = data.copy()\n",
    "\n",
    "#For each interesting variable we expand the df\n",
    "for detect in interesting_cuts:\n",
    "    \n",
    "    data_2 = make_exog(data_2,value_container=detect[0],value=detect[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution of activity in not free countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find top countries\n",
    "countries = pd.Series(flatten_list(data_2['country_list'].dropna())).value_counts().index\n",
    "\n",
    "#Which are not free?\n",
    "not_free_countries_all = [c for c in [x for x in countries if (x in country_status_lookup.keys())] if country_status_lookup[c]=='NF']\n",
    "\n",
    "#Focus on the top countties\n",
    "not_free_countries = not_free_countries_all[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo['not_free'] = [x in not_free_countries_all for x in arx_geo['institute_country']]\n",
    "\n",
    "arx_geo['year'] = arx_geo['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulate_trends_2(df,years=[2000,2019],select=False):\n",
    "    '''\n",
    "    TODO I am sure I have already written this function\n",
    "    \n",
    "    Function to calculate geo shares of activity\n",
    "    \n",
    "    Args:\n",
    "        df including information about the country\n",
    "        years is the years to focus the analysis on\n",
    "        all is whether we want to focus on all variables or a subset\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    if select!=False:\n",
    "        df = df.loc[df[select]==True]\n",
    "        \n",
    "    #return(df['year'].value_counts().loc[np.arange(years[0],years[1])])\n",
    "    return(df['year'].value_counts())\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This extracts a category (country) share of activity in the total\n",
    "\n",
    "nf_out = []\n",
    "\n",
    "for x in [False,'has_ai','has_sota','has_surv']:\n",
    "    nf_count = pd.concat([calulate_trends_2(arx_geo.loc[arx_geo['not_free']==v],select=x) for v in [False,True]],axis=1)\n",
    "    all_activity = calulate_trends_2(arx_geo,select=x)\n",
    "    \n",
    "    nf_count.columns = ['Free','Not Free']\n",
    "    nf_count_norm = nf_count.apply(lambda x: x/all_activity) \n",
    "    \n",
    "    #nf_count = nf_count.apply(lambda x: x/x.sum())\n",
    "    \n",
    "    nf_out.append(nf_count_norm)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_free_shares = pd.concat([x['Not Free'] for x in nf_out],axis=1).fillna(0)\n",
    "\n",
    "not_free_shares.columns = ['All','has_ai','has_sota','has_surv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(7,3))\n",
    "\n",
    "\n",
    "(100*not_free_shares.rolling(window=3).mean()).dropna().plot(linewidth=3,ax=ax)\n",
    "\n",
    "ax.legend(labels=['All arXiv','AI','SotA','Surveillance topics'])\n",
    "ax.set_ylabel('Not free countries as share of all')\n",
    "\n",
    "save_fig('fig_19_topic_focus.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the top 10 not free countries with all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([calulate_trends_2(arx_geo.loc[arx_geo['institute_country']==v],select='has_ai') for v in focus_not_free],axis=1,join='outer').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_not_free = not_free_countries[:8]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_detailed = []\n",
    "\n",
    "for x in [False,'has_ai','has_sota','has_surv']:\n",
    "    nf_count = pd.concat([calulate_trends_2(arx_geo.loc[arx_geo['institute_country']==v],select=x) for v in focus_not_free],axis=1).fillna(0)\n",
    "    \n",
    "    \n",
    "    all_activity = calulate_trends_2(arx_geo,select=x)\n",
    "    \n",
    "    nf_count.columns = focus_not_free\n",
    "    nf_count_norm = nf_count.apply(lambda x: x/all_activity).fillna(0) \n",
    "    \n",
    "    \n",
    "    nf_detailed.append(nf_count_norm)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,5),nrows=2,ncols=4,sharey='row',sharex=True)\n",
    "\n",
    "titles = ['All arXiv','AI','SotA','Surveillance']\n",
    "\n",
    "for n,p in enumerate(nf_detailed):\n",
    "    \n",
    "    (100*p.iloc[:,0].rolling(window=5).mean()).plot(ax=ax[0][n],linewidth=2,legend=False,c='black')    \n",
    "    (100*p.iloc[:,1:].rolling(window=5).mean()).plot(ax=ax[1][n],linewidth=2,legend=False)    \n",
    "    \n",
    "\n",
    "ax[0][0].set_ylabel('%')\n",
    "ax[1][0].set_ylabel('%')\n",
    "\n",
    "\n",
    "# #ax[0].legend(ncol=3)\n",
    "# ax[0].legend().set_visible(False)\n",
    "# ax[1].legend().set_visible(False)\n",
    "# ax[2].legend().set_visible(False)\n",
    "# ax[3].legend().set_visible(False)\n",
    "# #ax[2].legend(bbox_to_anchor=(1,2))\n",
    "\n",
    "ax[0][3].legend(bbox_to_anchor=(1.05,1),ncol=2)\n",
    "ax[1][3].legend(bbox_to_anchor=(1,1.1),ncol=2)\n",
    "\n",
    "[ax[0][n].set_title(t) for n,t in enumerate(titles)]\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(f'../reports/figures/paper_rev/{today_str}_fig_20_not_free_detail.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-sectional comparison**\n",
    "\n",
    "Here we calculate how over (or under?) represented is a topic in a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_w_countries = data_2.loc[[type(x)==list for x in data_2['country_list']]]\n",
    "\n",
    "#Calculate activity for all countries\n",
    "all_country_activity = pd.concat(\n",
    "    [analysis_w_countries.loc[[x in countries for countries in analysis_w_countries['country_list']]]['year'].value_counts() for x in countries],axis=1).fillna(0)\n",
    "\n",
    "all_country_activity.columns = countries\n",
    "\n",
    "analysis_w_countries['not_free_not_china'] = [(x['nf']==True)&(x['china']==False) for pid,x in analysis_w_countries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross = pd.concat([cross_sectional_comp(analysis_w_countries,x,surv_topics,threshold=0.05)['difference'] for x in ['china','not_free_not_china']],axis=1)\n",
    "\n",
    "cross.columns = ['china','not_free_other_than_china']\n",
    "\n",
    "ax = (100*cross.T.iloc[::-1]).plot.barh(title='Specialisation in visual surveillance topics',figsize=(10,3))\n",
    "\n",
    "hand,labs = ax.get_legend_handles_labels()\n",
    "\n",
    "ax.legend(loc='lower right',handles = [x[0] for x in zip(hand,labs)],\n",
    "          labels=[x[1][:50] for x in zip(hand,labs)])\n",
    "\n",
    "\n",
    "ax.set_xlabel('% deviation from the average')\n",
    "ax.set_yticklabels(['Not Free (Excluding China)','China'])\n",
    "\n",
    "ax.vlines(x=0,ymin=-1,ymax=2,linestyle=':',color='red')\n",
    "\n",
    "save_fig('fig_21_activity_in_surveillance_topics.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot regression coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controls = ['year']+list(field_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_regression_coefficients(analysis_w_countries,'nf',size=(8,4))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_fig('fig_22_nf_specialisation.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the above just driven by China? We create a new variable excluding it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_w_countries['not_free_not_china'] = [(x['nf']==True)&(x['china']==False) for pid,x in analysis_w_countries.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_w_countries['not_free_not_china'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who is doing the facial recognition research?\n",
    "\n",
    "We want to see if government organisations are over or underepresented in facial recognition research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Share of all activity and all surveillance activity accounted for by governments\n",
    "govt = 100*pd.crosstab(\n",
    "    data_2[surv_topics].apply(lambda x: any(x>0.05),axis=1),data_2['government'],normalize=1).loc[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Share of all activity and all surveillance activity involving Chinese projects with government involvement\n",
    "ch_govt = 100*pd.crosstab(\n",
    "    data_2[surv_topics].apply(lambda x: any(x>0.05),axis=1),data_2['government']*data_2['china'],normalize=1).loc[True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.concat([ch_govt,govt],axis=1).T.plot.barh()\n",
    "\n",
    "ax.set_yticklabels(['China and government involved in research','Government involved in research'])\n",
    "ax.set_xlabel('Share of Activity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the levels of international collaboration in AI research?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv = data_2.loc[data_2[surv_topics].apply(lambda x: any(x>0.05),axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_china = surv.loc[surv['china']==True]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([flatten_freq(surv_china['type_list']),flatten_freq(surv['type_list'])],axis=1).fillna(0).apply(lambda x: x/x.sum()).plot.bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other countries and facial technologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_surv_count = pd.concat([calulate_trends_2(arx_geo.loc[arx_geo['institute_country']==v],select='has_surv') for v in \n",
    "                            ['United Kingdom','United States','Germany']],axis=1).fillna(0)\n",
    "\n",
    "all_surv_count.columns = ['United Kingdom','United States','Germany']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_surv_count.loc[2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d map\n",
    "\n",
    "Here we want to create a 3d map of facial recognition technology activity by country.\n",
    "\n",
    "We use some code that we found [here](https://medium.com/@lkhphuc/how-to-plot-a-3d-earth-map-using-basemap-and-matplotlib-2bc026483fe4)\n",
    "\n",
    "See also [here](https://basemaptutorial.readthedocs.io/en/latest/basemap3d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1. Plot the basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "world_map = Basemap()\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "\n",
    "#ax.azim = 360\n",
    "ax.elev = 45\n",
    "ax.dist = 9.5\n",
    "\n",
    "ax.add_collection3d(world_map.drawcoastlines(linewidth=0.5))\n",
    "ax.add_collection3d(world_map.drawcountries(linewidth=0.25))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with arxGeo\n",
    "\n",
    "* Label papers with whether they relate to surveillance or not\n",
    "* Label countries with their freedom status (free / not free / partially free?)\n",
    "* Calculate LQs and Totals by country\n",
    "* Log the totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arx_geo_plot = arx_geo.loc[arx_geo['is_multinational']==False].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label surveillance papers\n",
    "arx_geo_plot['is_surv'] = [x in surv_ids for x in arx_geo['article_id']]\n",
    "\n",
    "arx_geo_plot['free'] = [country_status_lookup[x] if x in country_status_lookup.keys() else np.nan for x in arx_geo['institute_country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple plot\n",
    "(100*pd.crosstab(arx_geo_plot['is_surv'],arx_geo_plot['free'],normalize=1)).loc[True].sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specialisation\n",
    "\n",
    "def create_lq(X, binary=False):\n",
    "    \"\"\" Calculate the location quotient.\n",
    "\n",
    "    Divides the share of activity in a location by the share of activity in the UK total\n",
    "\n",
    "    Args:\n",
    "        X (pandas.DataFrame): DataFrame where rows are locations, columns are sectors and values are activity in a given sector at a location.\n",
    "        binary (bool, optional): If True, discretise the data with a cut-off value of 1\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame\n",
    "    \"\"\"\n",
    "    Xm = X.values\n",
    "    X = pd.DataFrame((Xm/Xm.sum(1)[:, np.newaxis])/(Xm.sum(0)/Xm.sum()),\n",
    "            index=X.index, columns=X.columns)\n",
    "    \n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will focus on the top 100 countries**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_75 = arx_geo_plot['institute_country'].value_counts()[:50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure surveillance related activity\n",
    "spec = create_lq(pd.crosstab(arx_geo_plot['institute_country'],arx_geo_plot['is_surv'])).loc[top_75].sort_values(True,ascending=False)[True]\n",
    "\n",
    "tots = pd.crosstab(arx_geo_plot['institute_country'],arx_geo_plot['is_surv']).loc[top_75].sort_values(True,ascending=False)[True]\n",
    "\n",
    "surv_activity = pd.concat([spec,tots],axis=1)\n",
    "\n",
    "surv_activity.columns = ['spec','total']\n",
    "\n",
    "surv_activity['total_discretised'] = pd.qcut(surv_activity['total'],q=np.arange(0,1.1,0.2),labels=False,\n",
    "                                            duplicates='drop').apply(lambda x: x/20)\n",
    "\n",
    "surv_activity['total_logged'] = np.log(surv_activity['total']+0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure surveillance related status\n",
    "surv_activity['status'] = [country_status_lookup[x] if x in country_status_lookup.keys() else np.nan for x in surv_activity.index]\n",
    "surv_activity['color'] = ['red' if x=='NF' else 'orange' if x == 'PF' else 'lightgreen' for x in surv_activity['status']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate median lat lons for countries (proxy for centroids)\n",
    "\n",
    "country_lat_lon = arx_geo_plot.groupby('institute_country')[['institute_lon','institute_lat']].median().to_dict(orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_activity['lon'],surv_activity['lat'] = [[country_lat_lon[x][var] for x in surv_activity.index] for var in ['institute_lon','institute_lat']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "\n",
    "world_map = Basemap()\n",
    "\n",
    "fig = plt.figure(figsize=(15,8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "\n",
    "#ax.azim = 360\n",
    "ax.elev = 45\n",
    "ax.dist = 9.5\n",
    "\n",
    "ax.add_collection3d(world_map.drawcoastlines(linewidth=0.1))\n",
    "ax.add_collection3d(world_map.drawcountries(linewidth=0.25))\n",
    "\n",
    "polys = []\n",
    "for polygon in world_map.landpolygons:\n",
    "    polys.append(polygon.get_coords())\n",
    "\n",
    "\n",
    "lc = PolyCollection(polys,\n",
    "                    facecolor='white', closed=False)\n",
    "\n",
    "ax.add_collection3d(lc)\n",
    "\n",
    "\n",
    "\n",
    "ax.bar3d(surv_activity['lon'], #x\n",
    "         surv_activity['lat'],  #y\n",
    "         np.zeros(len(surv_activity)), #z \n",
    "         \n",
    "         2, #dx\n",
    "         \n",
    "         surv_activity['spec'],#y\n",
    "         \n",
    "         surv_activity['total'], #z\n",
    "         \n",
    "         color= surv_activity['color'],edgecolor='black',linewidth=0.01)\n",
    "ax.set_zlim(0,180)\n",
    "\n",
    "ax.set_zlabel('Relative Specialisation in surveillance topics')\n",
    "ax.set_ylabel('Bar depth represents relative specialisation in \\n AI surveillance topics')\n",
    "\n",
    "\n",
    "scatter1_proxy = matplotlib.lines.Line2D([0],[0], linestyle=\"none\", c='red', marker = 's',linewidth=5)\n",
    "scatter2_proxy = matplotlib.lines.Line2D([0],[0], linestyle=\"none\", c='orange', marker = 's',linewidth=5)\n",
    "scatter3_proxy = matplotlib.lines.Line2D([0],[0], linestyle=\"none\", c='lightgreen', marker = 's',linewidth=5)\n",
    "ax.legend([scatter1_proxy, scatter2_proxy,scatter3_proxy], ['Not free', 'Partially free','Free'], numpoints = 1,bbox_to_anchor=(0.9,0.7),\n",
    "         title='Country classification')\n",
    "\n",
    "#plt.savefig(f'../reports/figures/paper_rev/{today_str}_facial_recognition.png')\n",
    "\n",
    "\n",
    "plt.savefig(f'../reports/figures/paper_rev/{today_str}_neurips_facial_recognition.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
