{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. GtR analysis\n",
    "\n",
    "Analysis of AI activity in UK research funding data.\n",
    "\n",
    "We will load a df with projects and another with IDs for AI projects.\n",
    "\n",
    "We want to do the following analyses on it:\n",
    "\n",
    "#### 1. Analysis of AI diffusion in the GtR data.\n",
    "\n",
    "Here, we have two research questions + a stretch goal\n",
    "\n",
    "##### Research questions\n",
    "\n",
    "1. What are the levels of diffusion of AI activity in different disciplines and industries?\n",
    "2. How has this evolved over time?\n",
    "3. What are its drivers?\n",
    "\n",
    "Our prior is that there will be strong concentration of activity in digital industries and (perhaps) an underrepresentation in public service oriented sectors. We will explore the determinants of activity. Do sectors with a stronger propensity to generate database outputs also see faster growth in AI activity?\n",
    "\n",
    "#### 2. Analysis of concentration\n",
    "\n",
    "##### Research questions\n",
    "\n",
    "1. What are the levels of spatial concentration in AI research funding and how have they concentrated over time?\n",
    "2. What are the levels of organisational concentration? (here it would be nice to have the company level information - ask Alex.\n",
    "3. Do we see organisations/teams that started early moving across application domains?\n",
    "\n",
    "3 may require loading the link table and the person data\n",
    "\n",
    "#### 3. Analysis of purpose\n",
    "\n",
    "##### Research questions\n",
    "\n",
    "1. What are the levels of activity in SDGs and how have they evolved over time?\n",
    "2. How do they compare to the population of activity overall?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_preamble.ipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GtR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GtR data\n",
    "gtr = pd.read_csv('../data/external/14_6_2019_gtr_cleaned.csv',compression='zip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We remove irrelevant columns\n",
    "gtr = gtr.loc[:,['Unnamed' not in x for x in gtr.columns]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse the lists \n",
    "\n",
    "lad_vars = [x for x in gtr.columns if 'lad_' in x]\n",
    "\n",
    "for ladvar in lad_vars:\n",
    "    \n",
    "    gtr[ladvar] = [literal_eval(x) for x in gtr[ladvar]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AI IDs\n",
    "with open('../data/external/ai_ids.p','rb') as infile:\n",
    "    ai_ids = pickle.load(infile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr['ai'] = [x in ai_ids for x in gtr['project_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem: We only identified AI projects for grants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will train an AI classifier on the GtR data and use it to predict the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "def flatten(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lq_df(df):\n",
    "    '''\n",
    "    Takes a df with cells = activity in col in row and returns a df with cells = lq\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    area_activity = df.sum(axis=0)\n",
    "    area_shares = area_activity/area_activity.sum()\n",
    "    \n",
    "    lqs = df.apply(lambda x: (x/x.sum())/area_shares, axis=1)\n",
    "    return(lqs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ML imports\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore',UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def flatten_list(my_list):\n",
    "    '''\n",
    "    Flattens a list\n",
    "    '''\n",
    "    \n",
    "    return([x for el in my_list for x in el])\n",
    "\n",
    "\n",
    "def dummies_from_list(list_of_categories):\n",
    "    '''\n",
    "    This function takes a list of categories and returns a df where every column is a dummie for each unique variable\n",
    "    in the category. Admittedly, the function could be nicer.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    #We concatenate a bunch of series whose indices are the names of the variables.\n",
    "    #We could have done something similar by creating DFs with one row\n",
    "    \n",
    "    cats = [x for x in set(flatten_list(list_of_categories))]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for category in cats:\n",
    "    \n",
    "        var = [category in x for x in list_of_categories]\n",
    "\n",
    "        df[category] = var\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #dummy_df = pd.concat([pd.Series({v:1 for v in obs}) for obs in list_of_categories],axis=1).T.fillna(0)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load text_classifier.py\n",
    "# CLasses\n",
    "\n",
    "#One class for text classification based on text inputs\n",
    "\n",
    "class TextClassification():\n",
    "    '''\n",
    "    This class takes a corpus (could be a list of strings or a tokenised corpus) and a target (could be multiclass or single class).\n",
    "    \n",
    "    When it is initialised it vectorises the list of tokens using sklearn's count vectoriser.\n",
    "    \n",
    "    It has a grid search method that takes a list of models and parameters and trains the model.\n",
    "    \n",
    "    It returns the output of grid search for diagnosis\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus,target):\n",
    "        '''\n",
    "        \n",
    "        Initialise. The class will recognise if we are feeding it a list of strings or a list of\n",
    "        tokenised documents and vectorise accordingly. \n",
    "        \n",
    "        It will also recognise is this a multiclass or one class problem based on the dimensions of the target array\n",
    "        \n",
    "        Later on, it will use control flow to modify model parameters depending on the type of data we have\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Is this a multiclass classification problem or a single class classification problem?\n",
    "        if target.shape[1]>1:\n",
    "            self.mode = 'multiclass'\n",
    "            \n",
    "        else:\n",
    "            self.mode = 'single_class'\n",
    "    \n",
    "    \n",
    "        #Store the target\n",
    "        self.Y = target\n",
    "    \n",
    "        #Did we feed the model a bunch of strings or a list of tokenised docs? If the latter, we clean and tokenise.\n",
    "        \n",
    "        if type(corpus[0])==str:\n",
    "            #corpus = CleanTokenize(corpus).clean().bigram().tokenised\n",
    "            corpus = CleanTokenize(corpus).clean().tokenised\n",
    "            \n",
    "        #Turn every list of tokens into a string for count vectorising\n",
    "        corpus_string =  [' '.join(words) for words in corpus]\n",
    "        \n",
    "        \n",
    "        #And then we count vectorise in a hacky way.\n",
    "        count_vect = CountVectorizer(stop_words='english',min_df=5).fit(corpus_string)\n",
    "        \n",
    "        #Store the features\n",
    "        self.X = count_vect.transform(corpus_string)\n",
    "        \n",
    "        #Store the count vectoriser (we will use it later on for prediction on new data)\n",
    "        self.count_vect = count_vect\n",
    "        \n",
    "    def grid_search(self,models):\n",
    "        '''\n",
    "        The grid search method takes a list with models and their parameters and it does grid search crossvalidation.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load inputs and targets into the model\n",
    "        Y = self.Y\n",
    "        X = self.X\n",
    "        \n",
    "        if self.mode=='multiclass':\n",
    "            '''\n",
    "            If the model is multiclass then we need to add some prefixes to the model paramas\n",
    "            \n",
    "            '''\n",
    "        \n",
    "            for mod in models:\n",
    "                #Make ovr\n",
    "                mod[0] = OneVsRestClassifier(mod[0])\n",
    "                \n",
    "                #Add the estimator prefix\n",
    "                mod[1] = {'estimator__'+k:v for k,v in mod[1].items()}\n",
    "                \n",
    "        \n",
    "        #Container with results\n",
    "        results = []\n",
    "\n",
    "        #For each model, run the analysis.\n",
    "        for num,mod in enumerate(models):\n",
    "            print(num)\n",
    "\n",
    "            #Run the classifier\n",
    "            clf = GridSearchCV(mod[0],mod[1])\n",
    "\n",
    "            #Fit\n",
    "            clf.fit(X,Y)\n",
    "\n",
    "            #Append results\n",
    "            results.append(clf)\n",
    "        \n",
    "        self.results = results\n",
    "        return(self)\n",
    "\n",
    "    \n",
    "#Class to visualise the outputs of multilabel models.\n",
    "\n",
    "#I call it OrangeBrick after YellowBrick, the package for ML output visualisation \n",
    "#(which currently doesn't support multilabel classification)\n",
    "\n",
    "\n",
    "class OrangeBrick():\n",
    "    '''\n",
    "    This class takes a df with the true classes for a multilabel classification exercise and produces some charts visualising findings.\n",
    "    \n",
    "    The methods include:\n",
    "    \n",
    "        .confusion_stack: creates a stacked barchart with the confusion matrices stacked by category, sorting classes by performance\n",
    "        .prec_rec: creates a barchart showing each class precision and recall;\n",
    "        #Tobe done: Consider mixes between classes?\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,true_labels,predicted_labels,var_names):\n",
    "        '''\n",
    "        Initialise with a true labels, predicted labels and the variable names\n",
    "        '''\n",
    "         \n",
    "        self.true_labels = true_labels\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.var_names = var_names\n",
    "    \n",
    "    def make_metrics(self):\n",
    "        '''\n",
    "        Estimates performance metrics (for now just confusion charts by class and precision/recall scores for the 0.5 \n",
    "        decision rule.\n",
    "        \n",
    "        '''\n",
    "        #NB in a confusion matrix in SKlearn the X axis indicates the predicted class and the Y axis indicates the ground truth.\n",
    "        #This means that:\n",
    "            #cf[0,0]-> TN\n",
    "            #cf[1,1]-> TP\n",
    "            #cf[0,1]-> FN (prediction is false, groundtruth is true)\n",
    "            #cf[1,0]-> FP (prediction is true, ground truth is false)\n",
    "\n",
    "\n",
    "\n",
    "        #Predictions and true labels\n",
    "        true_labels = self.true_labels\n",
    "        pred_labels = self.predicted_labels\n",
    "\n",
    "        #Variable names\n",
    "        var_names = self.var_names\n",
    "\n",
    "        #Store confusion matrices\n",
    "        score_store = []\n",
    "\n",
    "\n",
    "        for num in np.arange(len(var_names)):\n",
    "\n",
    "            #This is the confusion matrix\n",
    "            cf = confusion_matrix(pred_labels[:,num],true_labels[:,num])\n",
    "\n",
    "            #This is a melted confusion matrix\n",
    "            melt_cf = pd.melt(pd.DataFrame(cf).reset_index(drop=False),id_vars='index')['value']\n",
    "            melt_cf.index = ['true_negative','false_positive','false_negative','true_positive']\n",
    "            melt_cf.name = var_names[num]\n",
    "            \n",
    "            #Order variables to separate failed vs correct predictions\n",
    "            melt_cf = melt_cf.loc[['true_positive','true_negative','false_positive','false_negative']]\n",
    "\n",
    "            #We are also interested in precision and recall\n",
    "            prec = cf[1,1]/(cf[1,1]+cf[1,0])\n",
    "            rec = cf[1,1]/(cf[1,1]+cf[0,1])\n",
    "\n",
    "            prec_rec = pd.Series([prec,rec],index=['precision','recall'])\n",
    "            prec_rec.name = var_names[num]\n",
    "            score_store.append([melt_cf,prec_rec])\n",
    "    \n",
    "        self.score_store = score_store\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def confusion_chart(self,ax):\n",
    "        '''\n",
    "        Plot the confusion charts\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Visualise confusion matrix outputs\n",
    "        cf_df = pd.concat([x[0] for x in self.score_store],1)\n",
    "\n",
    "        #This ranks categories by the error rates\n",
    "        failure_rate = cf_df.apply(lambda x: x/x.sum(),axis=0).loc[['false' in x for x in cf_df.index]].sum().sort_values(\n",
    "            ascending=False).index\n",
    "\n",
    "        \n",
    "        #Plot and add labels\n",
    "        cf_df.T.loc[failure_rate,:].plot.bar(stacked=True,ax=ax,width=0.8,cmap='Accent')\n",
    "\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Stacked confusion matrix for disease areas',size=16)\n",
    "    \n",
    "    \n",
    "    def prec_rec_chart(self,ax):\n",
    "        '''\n",
    "        \n",
    "        Plot a precision-recall chart\n",
    "        \n",
    "        '''\n",
    "    \n",
    "\n",
    "        #Again, we sort them here to assess model performance in different disease areas\n",
    "        prec_rec = pd.concat([x[1] for x in self.score_store],1).T.sort_values('precision')\n",
    "        prec_rec.plot.bar(ax=ax)\n",
    "\n",
    "        #Add legend and title\n",
    "        ax.legend(bbox_to_anchor=(1.01,1))\n",
    "        #ax.set_title('Precision and Recall by disease area',size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_grants = gtr.loc[gtr['grant_category']=='Research Grant'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the corpus\n",
    "corpus = list(gtr_grants['abstract'])\n",
    "\n",
    "#We use a utility function to create a df for a one vs rest classification\n",
    "target = pd.get_dummies(gtr_grants['ai'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run grid search with these model parameters\n",
    "my_models = [\n",
    "    [RandomForestClassifier(),\n",
    "     {'class_weight':['balanced',None],'min_samples_leaf':[1,5]}],\n",
    "    \n",
    "    [LogisticRegression(),\n",
    "     {'class_weight':['balanced',None],'penalty':['l1','l2'],\n",
    "      'C':[0.1,1,100]}]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict groups\n",
    "\n",
    "#Initialise the TextClassification class\n",
    "gtr_t = TextClassification(corpus,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_t.grid_search(my_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check scores and best estimators\n",
    "for res in gtr_t.results:\n",
    "    print(res.best_score_)\n",
    "    print(res.best_estimator_)\n",
    "    \n",
    "    #This is the best estimator\n",
    "best_est = gtr_t.results[1].best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_diag = OrangeBrick(true_labels=np.array(target),\n",
    "                      predicted_labels=best_est.predict(gtr_t.X),\n",
    "                      var_names=target.columns).make_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=2,figsize=(10,7.5))\n",
    "\n",
    "gtr_diag.confusion_chart(ax=ax[0])\n",
    "gtr_diag.prec_rec_chart(ax=ax[1])\n",
    "\n",
    "#fig.suptitle('Model evaluation for GTR disciplines',y=1.01,size=16)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_mask = pd.DataFrame(best_est.predict_proba(gtr_t.X)>0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_all = pd.concat([gtr_grants,gtr.loc[gtr['grant_category']!='Research Grant']],axis=0)\n",
    "\n",
    "#Extract the lead lad - there is only one per project\n",
    "gtr_all['lead_lad_value']= [x[0] if len(x)>0 else np.nan for x in gtr_all['lead_lad_name']]\n",
    "\n",
    "abstract_all_transformed = gtr_t.count_vect.transform(gtr_all['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_all['ai_mod'] = pd.DataFrame(best_est.predict_proba(abstract_all_transformed))[1]>0.6\n",
    "\n",
    "gtr_all = gtr_all.loc[(gtr_all['year']>2006)&(gtr_all['year']<2019)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does this look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funder distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(gtr_all['ai_mod'],gtr_all['funder'],normalize=0).T.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(gtr_all['year'],gtr_all['ai_mod'],normalize=0)[True].rolling(window=4).mean().dropna().plot(title='Share of total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(gtr_all['sel_disc'],gtr_all['ai_mod'],normalize=1).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discipline evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_ai = gtr_all.loc[gtr_all['ai_mod']==True]\n",
    "\n",
    "disc_sorted = gtr_ai['sel_disc'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(gtr_ai['year'],gtr_ai['sel_disc'],normalize=0)[disc_sorted[::-1]].rolling(window=3).mean().dropna().plot.bar(stacked=True,\n",
    "                                                                                                                        width=0.9,cmap='Accent',\n",
    "                                                                                                                       edgecolor='lightgrey',\n",
    "                                                                                                                             title='Discipline shares')\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(gtr_all['sel_industry'],gtr_all['ai_mod'],normalize=1).plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sorted = gtr_ai['sel_industry'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(gtr_ai['year'],gtr_ai['sel_industry'],normalize=0)[ind_sorted[:8]].rolling(window=3).mean().dropna().plot.bar(stacked=True,\n",
    "                                                                                                                        width=0.9,cmap='Accent_r',\n",
    "                                                                                                                       edgecolor='lightgrey',\n",
    "                                                                                                                             title='Discipline shares')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(gtr_all['sel_sdg'],gtr_all['ai_mod'],normalize=1).plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdg_sorted = gtr_ai['sel_sdg'].value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pd.crosstab(gtr_ai['year'],gtr_ai['sel_sdg'],normalize=0)[sdg_sorted[:8]].rolling(window=3).mean().dropna().plot.bar(stacked=True,\n",
    "                                                                                                                        width=0.9,cmap='Accent_r',\n",
    "                                                                                                                       edgecolor='lightgrey',\n",
    "                                                                                                                             title='Discipline shares')\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gtr_ai.loc[gtr_ai['sel_sdg']=='sdg_13_climate_action'].sort_values('sdg_13_climate_action',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SDG classification is underperforming compared with the others. Its training corpus is 'furthest away' from the GtR data. We will have to review this carefully and decide what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of organisations / places\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the lead lad - there is only one per project\n",
    "gtr_all['lead_lad_value']= [x[0] if len(x)>0 else np.nan for x in gtr_all['lead_lad_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crosstab\n",
    "ai_counts = pd.crosstab(gtr_all['lead_lad_value'],gtr_all['ai_mod']).sort_values(True,ascending=False)\n",
    "ai_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100*pd.crosstab(gtr_all['lead_lad_value'],gtr_all['ai_mod'],normalize=0).loc[ai_counts.index[:20]].sort_values(True,ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the level of concentration? Is it increasing over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In what industries do different locations specialise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_industry_counts = gtr_ai.groupby(['lead_lad_value','sel_industry'])['ai_mod'].sum().reset_index(\n",
    "    drop=False).pivot_table(index='lead_lad_value',columns='sel_industry',values='ai_mod').fillna(0).loc[ai_counts.index[:20],ind_sorted[:7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_industry_spec = create_lq_df(city_industry_counts).apply(lambda x: pd.qcut(x,q=np.arange(0,1.1,0.2),labels=False,duplicates='drop'),axis=1)\n",
    "\n",
    "sns.heatmap(city_industry_spec,cmap='Oranges',edgecolor='lightgray',linewidth=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are they evolving over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_year_counts = gtr_ai.groupby(['lead_lad_value','year'])['ai_mod'].sum().reset_index(\n",
    "    drop=False).pivot_table(index='lead_lad_value',columns='year',values='ai_mod').fillna(0)\n",
    "\n",
    "#city_year_counts.T.plot.bar(stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add industries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_industries = pd.read_csv('../data/processed/6_8_2019_gtr_organisations_industries_2.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_all_industries = pd.merge(gtr_all,projects_industries,left_on='project_id',right_on='project_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_all_industries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtr_all_industries.to_csv(f'../data/processed/{today_str}_gtr_processed.csv',compression='zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
